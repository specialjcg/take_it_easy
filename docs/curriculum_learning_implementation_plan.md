# Curriculum Learning - Plan d'Impl√©mentation Complet

## Contexte

Apr√®s l'√©chec de Gold GNN (127.74 pts, -11.66 pts vs baseline 139.40), nous passons au **Curriculum Learning avec Beam Search** pour am√©liorer l'IA de fa√ßon significative.

**Objectif:** +10-15 pts ‚Üí 149-154 pts (20-30 jours d'impl√©mentation)

---

## Phase 1: G√©n√©ration de Donn√©es Expertes ‚úÖ EN COURS

### Outils Cr√©√©s

1. **`src/bin/expert_data_generator.rs`** ‚úÖ COMPL√âT√â
   - G√©n√®re des parties avec beam search (configurable: 100, 500, 1000)
   - Sauvegarde au format JSON: plateau_state, tile_played, position_played, turn, score_after
   - Chaque partie g√©n√®re 19 exemples d'entra√Ænement (un par coup)

2. **`curriculum_learning.sh`** ‚úÖ COMPL√âT√â
   - Script orchestrant les 3 phases de g√©n√©ration de donn√©es
   - D√©tection automatique des fichiers d√©j√† g√©n√©r√©s (√©vite reg√©n√©ration)

### G√©n√©ration de Donn√©es

**Phase 1: Donn√©es Faciles (Beam 100)** üîÑ EN COURS
```bash
cargo run --release --bin expert_data_generator -- \
  -g 50 -b 100 \
  -o expert_data/phase1_beam100.json \
  -s 2025
```
- 50 parties √ó Beam 100
- Score attendu: ~150 pts (vs 139 baseline, vs 175 optimal)
- Dur√©e: ~30 minutes
- Exemples g√©n√©r√©s: 950 (50 √ó 19)

**Phase 2: Donn√©es Moyennes (Beam 500)** ‚è≥ √Ä FAIRE
```bash
cargo run --release --bin expert_data_generator -- \
  -g 100 -b 500 \
  -o expert_data/phase2_beam500.json \
  -s 2026
```
- 100 parties √ó Beam 500
- Score attendu: ~165 pts
- Dur√©e: ~3-4 heures
- Exemples g√©n√©r√©s: 1900 (100 √ó 19)

**Phase 3: Donn√©es Difficiles (Beam 1000)** ‚è≥ √Ä FAIRE
```bash
cargo run --release --bin expert_data_generator -- \
  -g 200 -b 1000 \
  -o expert_data/phase3_beam1000.json \
  -s 2027
```
- 200 parties √ó Beam 1000
- Score attendu: ~175 pts (quasi-optimal)
- Dur√©e: ~12-16 heures
- Exemples g√©n√©r√©s: 3800 (200 √ó 19)

**Total: 6650 exemples d'entra√Ænement d'expert**

---

## Phase 2: Impl√©mentation de l'Entra√Ænement Supervis√© ‚è≥ √Ä FAIRE

### Modification Requise: Nouveau Mode d'Entra√Ænement

Cr√©er `src/neural/training/supervised_trainer.rs`:

```rust
pub struct SupervisedTrainer {
    policy_net: PolicyNet,
    value_net: ValueNet,
    optimizer_policy: Optimizer,
    optimizer_value: Optimizer,
}

impl SupervisedTrainer {
    /// Charge les donn√©es d'entra√Ænement depuis un fichier JSON
    pub fn load_expert_data(path: &str) -> Vec<TrainingExample> {
        let json = std::fs::read_to_string(path)?;
        serde_json::from_str(&json)?
    }

    /// Entra√Æne PolicyNet √† pr√©dire les coups experts
    pub fn train_policy_epoch(&mut self, examples: &[TrainingExample]) -> f32 {
        let mut total_loss = 0.0;

        for batch in examples.chunks(32) {
            // 1. Convert plateau_state to tensor
            let states = self.batch_states_to_tensor(batch);

            // 2. Forward pass
            let predictions = self.policy_net.forward(&states, true);

            // 3. Target: one-hot sur position_played
            let targets = self.batch_positions_to_one_hot(batch);

            // 4. Loss: CrossEntropy
            let loss = predictions.cross_entropy_for_logits(&targets);

            // 5. Backward + update
            self.optimizer_policy.zero_grad();
            loss.backward();
            self.optimizer_policy.step();

            total_loss += loss.double_value(&[]);
        }

        total_loss / (examples.len() as f32 / 32.0)
    }

    /// Entra√Æne ValueNet √† pr√©dire le score final normalis√©
    pub fn train_value_epoch(&mut self, examples: &[TrainingExample]) -> f32 {
        let mut total_loss = 0.0;

        for batch in examples.chunks(32) {
            let states = self.batch_states_to_tensor(batch);
            let predictions = self.value_net.forward(&states, true);

            // Target: score_after normalis√© [-1, 1]
            // 0 pts ‚Üí -1, 175 pts (optimal) ‚Üí +1
            let targets = self.batch_scores_to_normalized(batch);

            // Loss: MSE
            let loss = (predictions - targets).pow(2).mean(Kind::Float);

            self.optimizer_value.zero_grad();
            loss.backward();
            self.optimizer_value.step();

            total_loss += loss.double_value(&[]);
        }

        total_loss / (examples.len() as f32 / 32.0)
    }
}
```

### Modification du main.rs

Ajouter option CLI:
```rust
#[arg(long)]
expert_data_path: Option<String>,
```

Dans la boucle d'entra√Ænement:
```rust
if let Some(expert_path) = args.expert_data_path {
    // MODE SUPERVIS√â
    let examples = SupervisedTrainer::load_expert_data(&expert_path)?;

    for epoch in 0..50 {
        let policy_loss = trainer.train_policy_epoch(&examples);
        let value_loss = trainer.train_value_epoch(&examples);

        println!("Epoch {}/50 - Policy Loss: {:.4}, Value Loss: {:.4}",
                 epoch+1, policy_loss, value_loss);

        if epoch % 10 == 0 {
            // Sauvegarder checkpoint
            manager.save_weights()?;

            // √âvaluation interm√©diaire
            let score = evaluate_model()?;
            println!("  Evaluation Score: {:.2} pts", score);
        }
    }
} else {
    // MODE SELF-PLAY (code actuel)
    // ...
}
```

---

## Phase 3: Curriculum Learning - Entra√Ænement Progressif ‚è≥ √Ä FAIRE

### Phase 1: Entra√Ænement sur Donn√©es Faciles

```bash
cargo run --release --bin take_it_easy -- \
  --mode training \
  --expert-data-path expert_data/phase1_beam100.json \
  --nn-architecture cnn \
  --epochs 50
```

**Attendu apr√®s Phase 1:**
- Score: 142-145 pts (+3-6 vs baseline)
- PolicyNet apprend les coups "bons" (pas optimaux)
- ValueNet pr√©dit scores ~150 pts

### Phase 2: Fine-tuning sur Donn√©es Moyennes

```bash
cargo run --release --bin take_it_easy -- \
  --mode training \
  --expert-data-path expert_data/phase2_beam500.json \
  --nn-architecture cnn \
  --epochs 30 \
  --load-weights model_weights  # Reprend Phase 1
```

**Attendu apr√®s Phase 2:**
- Score: 145-150 pts (+6-11 vs baseline)
- PolicyNet apprend les coups "tr√®s bons"
- ValueNet pr√©dit scores ~165 pts

### Phase 3: Fine-tuning sur Donn√©es Difficiles

```bash
cargo run --release --bin take_it_easy -- \
  --mode training \
  --expert-data-path expert_data/phase3_beam1000.json \
  --nn-architecture cnn \
  --epochs 20 \
  --load-weights model_weights  # Reprend Phase 2
```

**Attendu apr√®s Phase 3:**
- Score: **149-154 pts** (+10-15 vs baseline) üéØ OBJECTIF
- PolicyNet apprend les coups quasi-optimaux
- ValueNet pr√©dit scores ~175 pts

---

## Phase 4: √âvaluation Finale ‚è≥ √Ä FAIRE

```bash
# Benchmark sur 100 parties pour statistiques robustes
cargo run --release --bin compare_mcts -- \
  -g 100 \
  -s 150 \
  --nn-architecture cnn

# Analyse de l'√©cart avec l'optimal
cargo run --release --bin optimal_solver
```

**M√©triques de Succ√®s:**
- Score moyen ‚â• 149 pts (objectif minimum: +10 vs baseline)
- √âcart vs optimal < 15% (vs 20.5% actuellement)
- Victoires vs baseline ‚â• 70%

---

## Timeline Estim√©e

| Phase | T√¢che | Dur√©e | Statut |
|-------|-------|-------|--------|
| 1.1 | Cr√©er expert_data_generator.rs | 2h | ‚úÖ COMPL√âT√â |
| 1.2 | Cr√©er curriculum_learning.sh | 30min | ‚úÖ COMPL√âT√â |
| 1.3 | G√©n√©rer Phase 1 data (Beam 100) | 30min | üîÑ EN COURS |
| 1.4 | G√©n√©rer Phase 2 data (Beam 500) | 4h | ‚è≥ √Ä FAIRE |
| 1.5 | G√©n√©rer Phase 3 data (Beam 1000) | 16h | ‚è≥ √Ä FAIRE |
| 2.1 | Impl√©menter SupervisedTrainer | 4h | ‚è≥ √Ä FAIRE |
| 2.2 | Modifier main.rs pour CLI | 1h | ‚è≥ √Ä FAIRE |
| 2.3 | Tests unitaires | 2h | ‚è≥ √Ä FAIRE |
| 3.1 | Entra√Ænement Phase 1 | 1h | ‚è≥ √Ä FAIRE |
| 3.2 | Entra√Ænement Phase 2 | 1h | ‚è≥ √Ä FAIRE |
| 3.3 | Entra√Ænement Phase 3 | 1h | ‚è≥ √Ä FAIRE |
| 4.1 | Benchmark final | 2h | ‚è≥ √Ä FAIRE |
| 4.2 | Documentation r√©sultats | 1h | ‚è≥ √Ä FAIRE |

**Total: ~36 heures (~5 jours ouvr√©s)**

---

## Risques et Mitigation

### Risque 1: Overfitting sur Donn√©es Expertes
**Sympt√¥me:** Score d'entra√Ænement √©lev√© mais score de test faible
**Mitigation:**
- Split train/val 80/20
- Early stopping bas√© sur validation score
- Dropout 0.3 pendant entra√Ænement

### Risque 2: Beam Search Trop Lent
**Sympt√¥me:** G√©n√©ration Phase 3 prend > 24h
**Mitigation:**
- Parall√©liser g√©n√©ration (multi-threading)
- R√©duire nombre de parties (200 ‚Üí 150)
- Accepter Beam 800 au lieu de 1000

### Risque 3: Am√©lioration Insuffisante
**Sympt√¥me:** Score final < 145 pts (+6 seulement)
**Mitigation:**
- Ajouter Phase 4 avec mix MCTS + Beam (hybride)
- Augmenter capacit√© CNN (plus de filtres)
- Entra√Æner plus longtemps (100 epochs au lieu de 50)

---

## Prochaines √âtapes Imm√©diates

1. ‚úÖ Attendre fin de g√©n√©ration Phase 1 (~30min)
2. ‚è≥ V√©rifier qualit√© donn√©es (inspecter phase1_beam100.json)
3. ‚è≥ Impl√©menter SupervisedTrainer (4h)
4. ‚è≥ Tester entra√Ænement Phase 1 (1h)
5. ‚è≥ Si succ√®s: lancer g√©n√©ration Phase 2+3 en parall√®le (overnight)

---

## Comparaison avec Autres Approches

| Approche | Gain Estim√© | Dur√©e | Complexit√© |
|----------|-------------|-------|------------|
| Gold GNN | ‚ùå -11.66 pts | 12h | Moyenne |
| Pattern Rollouts V3 | ‚ùå -51.28 pts | 2h | Faible |
| **Curriculum Learning** | **+10-15 pts** üéØ | **5j** | **√âlev√©e** |
| Expert Data Simple | +8-12 pts | 3j | Moyenne |
| Hybrid Training | +5-8 pts | 2j | Faible |

**Curriculum Learning est l'approche la plus prometteuse mais aussi la plus complexe.**

---

## R√©f√©rences

- `docs/beam_search_learning_improvement.md` - 4 approches avec beam search
- `docs/optimality_gap_analysis.md` - IA √† 79.5% de l'optimal
- `src/bin/expert_data_generator.rs` - G√©n√©rateur de donn√©es
- `curriculum_learning.sh` - Script d'orchestration
