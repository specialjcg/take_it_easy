# Plan d'Am√©lioration des Performances - Take It Easy AI
**Date:** 2025-12-29
**Contexte:** Apr√®s impl√©mentation UCT MCTS et AlphaGo Zero self-play
**Performance actuelle:** 146.92 pts (+97.9% vs baseline 74.22 pts)

---

## üìä √âtat Actuel (Baseline pour prochaine session)

### Performances Mesur√©es
```
Batch MCTS (original):        74.22 ¬± 23.68 pts
UCT MCTS (breakthrough):      149.36 ¬± 23.25 pts  (+101.2%)
UCT Self-Play (converged):    146.92 ¬± 25.42 pts  (+97.9%)
```

### Configuration Actuelle
```bash
# Self-play training settings (alphago_zero_trainer)
--iterations 10
--games-per-iter 20              # 380 examples/iteration
--mcts-simulations 150
--epochs-per-iter 10
--learning-rate 0.01
--batch-size 32
--convergence-threshold 2.0      # Trop restrictif!
```

### M√©triques de Training
```
Iteration 1: policy_loss=2.9444, value_loss=3.9966, score=148.29
Iteration 2: policy_loss=2.9444, value_loss=1.5045, score=146.92
Status: CONVERGED (am√©lioration -1.37 < 2.0 seuil)
```

### Probl√®me Identifi√©: CIRCULAR LEARNING
```
Policy Uniforme (loss=2.9444 = ln(19))
    ‚Üì
UCT utilise priors uniformes
    ‚Üì
G√©n√®re donn√©es uniformes
    ‚Üì
Training ‚Üí Policy Uniforme
    ‚Üì
BOUCLE!
```

---

## üéØ Plan d'Action Prioritis√©

### PHASE 1: Quick Wins (Impact Imm√©diat) ‚≠ê‚≠ê‚≠ê
**Objectif:** 155-170 pts
**Temps estim√©:** 1-2 jours
**Complexit√©:** Moyenne

#### 1.1 Impl√©menter Dirichlet Noise (PRIORIT√â #1)
**Impact attendu:** +15-30 pts
**Pourquoi:** Brise la boucle circulaire en for√ßant l'exploration

**Impl√©mentation:**
```rust
// Dans alphago_zero_trainer.rs, fonction generate_self_play_games()
// Ligne ~248, AVANT l'appel MCTS:

// Add Dirichlet noise to root policy (AlphaGo Zero technique)
let epsilon = 0.25;  // Mix ratio: 75% policy + 25% noise
let alpha = 0.3;     // Dirichlet concentration (lower = more uniform)

// Generate Dirichlet noise
use rand_distr::{Dirichlet, Distribution};
let dirichlet = Dirichlet::new_with_size(alpha, legal_moves.len()).unwrap();
let noise: Vec<f64> = dirichlet.sample(&mut rng);

// Apply noise to policy BEFORE MCTS
// Modifier mcts_find_best_position_for_tile_uct() pour accepter noisy_policy
```

**Fichier √† modifier:**
- `src/bin/alphago_zero_trainer.rs` (ligne ~248)
- `src/mcts/algorithm.rs` (ajouter param√®tre optional `exploration_noise`)

**Test de validation:**
```bash
# Apr√®s impl√©mentation, lancer:
./target/release/alphago_zero_trainer \
  --games-per-iter 50 \
  --iterations 10 \
  --convergence-threshold 5.0

# V√©rifier: policy_loss devrait diminuer (< 2.90)
```

---

#### 1.2 Temperature-Based Sampling
**Impact attendu:** +5-15 pts
**Pourquoi:** Cr√©e de la diversit√© dans les donn√©es d'entra√Ænement

**Impl√©mentation:**
```rust
// Dans alphago_zero_trainer.rs, ligne ~263
// Au lieu de: plateau.tiles[mcts_result.best_position] = chosen_tile;

// Use temperature-based sampling
let temperature = if iteration < 10 { 1.0 } else { 0.5 };
let selected_position = sample_position_with_temperature(
    &visit_counts,
    temperature
);

// Ajouter fonction:
fn sample_position_with_temperature(
    visit_counts: &HashMap<usize, usize>,
    temperature: f64,
) -> usize {
    // visits_temp = visits^(1/œÑ)
    // P(a) = visits_temp(a) / sum(visits_temp)
    // Sample from this distribution
}
```

---

#### 1.3 Augmenter Volume de Training
**Impact attendu:** +10-20 pts
**Pourquoi:** Plus de donn√©es = meilleur apprentissage

**Commande imm√©diate (SANS changement de code):**
```bash
./target/release/alphago_zero_trainer \
  --games-per-iter 100 \
  --convergence-threshold 10.0 \
  --iterations 50 \
  --epochs-per-iter 15 \
  --mcts-simulations 200

# Temps: ~8-12h sur CPU
# R√©sultat attendu: 155-165 pts
```

---

### PHASE 2: Optimisation Qualit√© (Impact Moyen) ‚≠ê‚≠ê
**Objectif:** 165-180 pts
**Temps estim√©:** 2-3 jours
**Complexit√©:** Moyenne-√âlev√©e

#### 2.1 Augmenter Rollout Count
**Fichier:** `src/mcts/algorithm.rs` ligne 1547
```rust
// Actuel:
let rollout_count = 5;

// Nouveau:
let rollout_count = 15;  // 3x plus de rollouts = meilleures estimations
```

**Trade-off:** 3x plus lent, mais meilleures valeurs

---

#### 2.2 Am√©liorer Value Normalization
**Fichier:** `src/mcts/algorithm.rs` ligne 1562
```rust
// Actuel (assume max=200):
let normalized_value = ((rollout_value / 200.0).clamp(0.0, 1.0) * 2.0) - 1.0;

// Nouveau (adaptatif bas√© sur performance actuelle ~147):
let normalized_value = ((rollout_value - 80.0) / 70.0).clamp(-1.0, 1.0);
// Centre √† 80, range ¬±70 pour couvrir [10, 150]
```

---

#### 2.3 Learning Rate Schedule
**Fichier:** `src/bin/alphago_zero_trainer.rs`
```rust
// Ajouter dans la boucle d'it√©ration (ligne ~143):
let current_lr = if iteration < 10 {
    0.01  // D√©but: learning rapide
} else if iteration < 30 {
    0.005  // Milieu: stabilisation
} else {
    0.001  // Fin: fine-tuning
};

// Update optimizer learning rate
manager.set_learning_rate(current_lr);
```

---

### PHASE 3: Architecture & Algorithmes (Long Terme) ‚≠ê
**Objectif:** 180-200+ pts
**Temps estim√©:** 1-2 semaines
**Complexit√©:** √âlev√©e

#### 3.1 Essayer Architecture GNN
```bash
# L'architecture GNN existe d√©j√† dans le code!
./target/release/alphago_zero_trainer \
  --nn-architecture GNN \
  --games-per-iter 100 \
  --iterations 50

# GNN peut capturer mieux les relations spatiales
```

#### 3.2 R√©seau Plus Profond
**Fichier:** `src/neural/policy_value_net.rs`
- Ajouter ResNet blocks
- Passer de 3 conv layers √† 5-7
- Ajouter batch normalization

#### 3.3 Experience Replay Buffer
- Garder les N meilleures games en m√©moire
- R√©entra√Æner sur mix de nouvelles + anciennes donn√©es
- √âvite l'oubli catastrophique

#### 3.4 Progressive Widening
**Note:** Code existe d√©j√† dans `src/mcts/progressive_widening.rs`!
- √Ä int√©grer avec UCT
- Peut am√©liorer early-game exploration

---

## üöÄ Plan d'Ex√©cution Recommand√©

### Session 1 (2-3 heures): Quick Test
```bash
# 1. Test imm√©diat SANS code change
cargo build --release

# 2. Lancer training √©tendu
./target/release/alphago_zero_trainer \
  --games-per-iter 100 \
  --convergence-threshold 10.0 \
  --iterations 50 \
  --mcts-simulations 200 \
  --epochs-per-iter 15 \
  --output training_history_extended.csv

# 3. Laisser tourner overnight
# 4. Analyser r√©sultats le lendemain
```

### Session 2 (4-6 heures): Dirichlet Noise
```bash
# 1. Impl√©menter Dirichlet noise (1.1)
# 2. Impl√©menter temperature sampling (1.2)
# 3. Tester sur 20 iterations
# 4. Comparer avec baseline

# Commandes de test:
cargo build --release
./target/release/alphago_zero_trainer \
  --games-per-iter 100 \
  --iterations 20 \
  --output training_history_with_noise.csv
```

### Session 3 (2-4 heures): Fine-Tuning
```bash
# 1. Impl√©menter rollout count increase (2.1)
# 2. Am√©liorer value normalization (2.2)
# 3. Ajouter learning rate schedule (2.3)
# 4. Long training run (50-100 iterations)
```

---

## üìà M√©triques de Succ√®s

### Objectifs par Phase
```
Phase 1 R√©ussie si:
  - policy_loss < 2.80 (commence √† apprendre!)
  - value_loss < 1.00
  - benchmark_score > 160 pts

Phase 2 R√©ussie si:
  - policy_loss < 2.50
  - value_loss < 0.70
  - benchmark_score > 170 pts

Phase 3 R√©ussie si:
  - policy_loss < 2.00
  - value_loss < 0.50
  - benchmark_score > 185 pts
```

### Commandes de Validation
```bash
# Benchmark current model
MODEL_PATH=model_weights/cnn ./target/release/compare_batch_vs_uct

# V√©rifier distribution policy
./target/release/test_uct_distribution

# Analyser training history
cat training_history.csv
# Regarder √©volution policy_loss et value_loss
```

---

## üìÅ Fichiers Cl√©s √† Modifier

### Priorit√© HAUTE (Phase 1)
```
src/bin/alphago_zero_trainer.rs
  - Ligne 248: Ajouter Dirichlet noise
  - Ligne 263: Temperature sampling
  - Ligne 33-62: Augmenter default params
```

### Priorit√© MOYENNE (Phase 2)
```
src/mcts/algorithm.rs
  - Ligne 1547: Rollout count
  - Ligne 1562: Value normalization

src/bin/alphago_zero_trainer.rs
  - Ligne 143: Learning rate schedule
```

### Priorit√© BASSE (Phase 3)
```
src/neural/policy_value_net.rs
  - Architecture r√©seau

src/mcts/progressive_widening.rs
  - Int√©gration avec UCT
```

---

## üîç Debug & Monitoring

### V√©rifier l'Apprentissage
```bash
# Si policy_loss reste √† 2.9444:
# ‚Üí Dirichlet noise pas assez fort (augmenter epsilon)
# ‚Üí Ou temperature trop basse

# Si value_loss diverge (augmente):
# ‚Üí Learning rate trop √©lev√©
# ‚Üí Ou value normalization incorrecte

# Si benchmark stagne:
# ‚Üí Pas assez de diversit√© dans les donn√©es
# ‚Üí Augmenter games_per_iter
```

### Logs √† Surveiller
```bash
tail -f selfplay_uct.log

# Chercher:
# - "policy_loss=" doit diminuer au fil des iterations
# - "value_loss=" doit diminuer et stabiliser
# - "Score:" doit augmenter progressivement
```

---

## üíæ Sauvegarder les R√©sultats

### Avant chaque exp√©rience
```bash
# Backup current model
cp -r model_weights/cnn model_weights/cnn_backup_$(date +%Y%m%d_%H%M)

# Sauvegarder training history
cp training_history.csv training_history_backup_$(date +%Y%m%d_%H%M).csv
```

### Apr√®s exp√©rience r√©ussie
```bash
# Tag git
git tag -a v0.2.0-dirichlet -m "UCT + Dirichlet noise: XXX pts"
git push origin v0.2.0-dirichlet

# Documenter r√©sultats
echo "Date: $(date)" >> docs/PERFORMANCE_LOG.md
echo "Config: ..." >> docs/PERFORMANCE_LOG.md
echo "Score: XXX pts" >> docs/PERFORMANCE_LOG.md
```

---

## üéì R√©f√©rences Techniques

### AlphaGo Zero Paper
- Dirichlet noise: Œ±=0.3, Œµ=0.25 (Section 4.1)
- Temperature: œÑ=1.0 pour 30 premiers coups, puis œÑ‚Üí0
- MCTS: 1600 simulations par coup (nous: 150-200)

### Code Existant √† R√©utiliser
```bash
# Progressive Widening (d√©j√† impl√©ment√©)
src/mcts/progressive_widening.rs

# GNN Architecture (d√©j√† disponible)
--nn-architecture GNN

# Hyperparameters adaptifs
src/mcts/hyperparameters.rs
```

---

## ‚ö†Ô∏è Pi√®ges √† √âviter

1. **Ne pas augmenter learning_rate au-dessus de 0.01**
   - Cause: value_loss divergence

2. **Ne pas utiliser batch_size trop petit (<16)**
   - Cause: training instable

3. **Ne pas skip le Dirichlet noise**
   - C'est LA cl√© pour briser la boucle circulaire

4. **Ne pas oublier de rebuild apr√®s changement de code**
   ```bash
   cargo build --release  # Toujours en --release!
   ```

5. **Ne pas interrompre training au milieu d'une iteration**
   - Risque de corrompre les weights
   - Utiliser Ctrl+C seulement entre iterations

---

## üìû Quick Reference Commands

```bash
# Build
cargo build --release

# Training standard (rapide, 2h)
./target/release/alphago_zero_trainer \
  --games-per-iter 50 --iterations 20

# Training long (overnight, 8-12h)
./target/release/alphago_zero_trainer \
  --games-per-iter 100 --iterations 50 \
  --convergence-threshold 10.0

# Benchmark current model
./target/release/compare_batch_vs_uct

# V√©rifier policy distribution
./target/release/test_uct_distribution

# Monitor training
tail -f training_history.csv
```

---

## üéØ Objectif Final

**Target Performance:** 180-200+ pts
**Actuel:** 146.92 pts
**Gap:** +33-53 pts (+22-36%)

**Timeline R√©aliste:**
- Phase 1 (Quick Wins): +15-25 pts ‚Üí **162-172 pts**
- Phase 2 (Optimisation): +10-15 pts ‚Üí **172-187 pts**
- Phase 3 (Architecture): +5-15 pts ‚Üí **180-200+ pts**

---

**Date de cr√©ation:** 2025-12-29
**Auteur:** Jean-Charles GOULEAU
**Baseline:** UCT Self-Play 146.92 pts (commit 1846218)
**Prochaine session:** Commencer par Phase 1.1 (Dirichlet Noise)
