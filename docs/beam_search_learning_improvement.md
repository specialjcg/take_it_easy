# Comment le Beam Search Peut AmÃ©liorer l'Apprentissage

## ğŸ¯ ProblÃ¨me Actuel

L'IA s'entraÃ®ne par **self-play** : elle joue contre elle-mÃªme avec MCTS et apprend de ses propres parties.

**Limitation** : L'IA n'atteint que 79.5% de l'optimal â†’ elle apprend de donnÃ©es sous-optimales, ce qui crÃ©e un **plafond de performance**.

## ğŸ’¡ Solutions avec Beam Search

### 1. ğŸ… Expert Data Generation (Solution RecommandÃ©e)

**Concept** : GÃ©nÃ©rer des donnÃ©es d'entraÃ®nement de haute qualitÃ© en utilisant le beam search comme "expert".

**Pipeline** :
```
1. Jouer N parties avec MCTS (exploration rapide)
2. Pour chaque partie, rejouer avec beam search (beam width 1000)
3. Extraire les meilleurs coups du beam search
4. EntraÃ®ner le PolicyNet sur ces coups optimaux
```

**Avantages** :
- âœ… DonnÃ©es de meilleure qualitÃ© (jusqu'Ã  175 pts au lieu de 139)
- âœ… Apprentissage supervisÃ© au lieu de reinforcement learning pur
- âœ… Convergence plus rapide
- âœ… Plafond de performance plus Ã©levÃ©

**InconvÃ©nients** :
- âŒ CoÃ»t computationnel Ã©levÃ© (beam search lent)
- âŒ NÃ©cessite beaucoup de stockage (sauvegarder les trajectoires)

### 2. ğŸ“ Curriculum Learning

**Concept** : Commencer avec des objectifs simples et augmenter progressivement la difficultÃ©.

**Phases** :
```
Phase 1 (100 parties) : Beam width 100 (rapide, ~160 pts)
  â†’ PolicyNet apprend les bases

Phase 2 (100 parties) : Beam width 500 (moyen, ~170 pts)
  â†’ PolicyNet apprend Ã  optimiser

Phase 3 (200 parties) : Beam width 1000 (lent, ~175 pts)
  â†’ PolicyNet apprend l'excellence
```

**Avantages** :
- âœ… Progression graduelle (Ã©vite l'overfitting)
- âœ… Meilleur Ã©quilibre vitesse/qualitÃ©
- âœ… L'IA apprend Ã  gÃ©nÃ©raliser

**InconvÃ©nients** :
- âŒ Long Ã  entraÃ®ner (3 phases)
- âŒ Complexe Ã  implÃ©menter

### 3. ğŸ”„ Hybrid Training: MCTS + Beam Replay

**Concept** : MÃ©langer donnÃ©es MCTS (exploration) et beam search (exploitation).

**Mix de donnÃ©es** :
- 70% MCTS self-play (exploration, variÃ©tÃ©)
- 30% Beam search optimal (exploitation, qualitÃ©)

**Workflow** :
```
1. GÃ©nÃ©rer 100 parties MCTS (rapide)
2. SÃ©lectionner les 30 meilleures parties
3. Rejouer ces 30 avec beam search
4. EntraÃ®ner sur: 70 MCTS + 30 Beam
5. RÃ©pÃ©ter
```

**Avantages** :
- âœ… Ã‰quilibre exploration/exploitation
- âœ… CoÃ»t modÃ©rÃ© (beam sur 30% seulement)
- âœ… Ã‰vite le sur-apprentissage

**InconvÃ©nients** :
- âŒ ComplexitÃ© d'implÃ©mentation
- âŒ HyperparamÃ¨tres Ã  tuner (ratio MCTS/Beam)

### 4. ğŸ“Š Value Net Training avec Scores Beam

**Concept** : Utiliser le beam search pour obtenir des **labels de score prÃ©cis**.

**ProblÃ¨me actuel** :
- ValueNet apprend le score final de la partie
- Mais ces scores ne reflÃ¨tent pas le **vrai potentiel** (optimal)

**Solution** :
```
Pour chaque position de jeu:
  1. Calculer le score final rÃ©el: s_real
  2. Calculer le score optimal (beam): s_optimal
  3. EntraÃ®ner ValueNet Ã  prÃ©dire: s_optimal

â†’ ValueNet apprend le "vrai" potentiel, pas juste le score MCTS
```

**Avantages** :
- âœ… ValueNet plus prÃ©cis
- âœ… Meilleure Ã©valuation des positions
- âœ… MCTS plus efficace (moins de simulations nÃ©cessaires)

**InconvÃ©nients** :
- âŒ Beam search trÃ¨s coÃ»teux (pour chaque position !)
- âŒ Faisable seulement en offline

## ğŸ“ˆ Gain EstimÃ© par Approche

| Approche | ComplexitÃ© | CoÃ»t Calcul | Gain EstimÃ© | Temps EntraÃ®nement |
|----------|------------|-------------|-------------|-------------------|
| **Expert Data** | Moyenne | Ã‰levÃ© | **+8-12 pts** | 48h (500 parties) |
| **Curriculum** | Ã‰levÃ©e | TrÃ¨s Ã©levÃ© | **+10-15 pts** | 72h (3 phases) |
| **Hybrid 70/30** | Ã‰levÃ©e | Moyen | **+5-8 pts** | 36h (400 parties) |
| **ValueNet Optimal** | Faible | TrÃ¨s Ã©levÃ© | **+3-5 pts** | 24h (offline) |

## ğŸš€ Plan d'ImplÃ©mentation RecommandÃ©

### Phase 1 : Expert Data Generation (Prioritaire)

**Objectif** : Atteindre 145-150 pts

**Steps** :
1. GÃ©nÃ©rer 200 parties avec MCTS (self-play actuel)
2. SÃ©lectionner les 100 meilleures parties (score > 140)
3. Rejouer ces 100 parties avec beam search (width 1000)
4. Extraire les placements optimaux comme labels
5. EntraÃ®ner Gold GNN (256-256-128-64) sur ces donnÃ©es

**Estimation** :
- GÃ©nÃ©ration MCTS : 6h (200 parties Ã— 150 sims)
- Beam search replay : 12h (100 parties Ã— beam 1000)
- EntraÃ®nement : 8h (Gold GNN)
- **Total : ~26h**

### Phase 2 : Gold GNN avec Plus de DonnÃ©es (ParallÃ¨le)

**Objectif** : AmÃ©liorer la capacitÃ© du rÃ©seau

**Configuration Gold GNN** :
```rust
Architecture: [256, 256, 128, 64]  // vs Silver [128, 128, 64]
Dropout: 0.2
Learning rate: 0.0005 (plus faible pour stabilitÃ©)
Batch size: 64
Parties: 500 (vs 200 actuellement)
```

**Gain estimÃ©** : +3-5 pts (grÃ¢ce Ã  la capacitÃ© accrue)

### Phase 3 : Combine Les Deux (Optimal)

**Expert Data + Gold GNN** â†’ Gain total estimÃ© : **+10-15 pts**

**Score final attendu** : 139.40 + 10-15 = **149-154 pts** âœ…

## ğŸ› ï¸ ImplÃ©mentation Technique

### 1. Module `beam_data_generator.rs`

```rust
pub struct BeamDataGenerator {
    beam_width: usize,
}

impl BeamDataGenerator {
    /// GÃ©nÃ¨re des donnÃ©es optimales Ã  partir d'une partie MCTS
    pub fn generate_expert_data(
        &self,
        game_history: &[GameState]
    ) -> Vec<(Position, Tile, f32)> {
        // Pour chaque Ã©tat de jeu:
        // 1. Lancer beam search pour trouver le meilleur coup
        // 2. Retourner (position, tuile, score_optimal)
    }
}
```

### 2. Module `curriculum_trainer.rs`

```rust
pub struct CurriculumConfig {
    phase1_games: usize,
    phase1_beam: usize,
    phase2_games: usize,
    phase2_beam: usize,
    // ...
}

pub fn train_with_curriculum(config: CurriculumConfig) {
    // Phase 1 : Beam faible
    // Phase 2 : Beam moyen
    // Phase 3 : Beam fort
}
```

### 3. Integration dans `trainer.rs`

```rust
pub struct TrainingConfig {
    // ... existing fields
    use_beam_guidance: bool,
    beam_width: usize,
    beam_data_ratio: f64,  // 0.3 pour 30% beam, 70% MCTS
}
```

## ğŸ“Š MÃ©triques de Suivi

Pour mesurer l'amÃ©lioration :

1. **Gap d'optimalitÃ©** : Comparer score IA vs beam search
2. **Taux d'apprentissage** : Mesurer la vitesse de convergence
3. **Variance** : VÃ©rifier la stabilitÃ© des performances
4. **Overfitting** : Tester sur donnÃ©es jamais vues

## âš ï¸ Risques et Mitigation

| Risque | Impact | ProbabilitÃ© | Mitigation |
|--------|--------|-------------|------------|
| Overfitting sur beam data | Ã‰levÃ© | Moyenne | Mix 70% MCTS + 30% Beam |
| CoÃ»t calcul prohibitif | Moyen | Ã‰levÃ©e | Beam seulement sur best games |
| Pas d'amÃ©lioration | Ã‰levÃ© | Faible | Benchmark Ã  chaque phase |
| RÃ©gression vs baseline | Critique | Faible | Toujours garder baseline CNN |

## ğŸ¯ Recommandation Finale

### Option Conservatrice : Accepter 139.40 pts
- âœ… Objectifs dÃ©passÃ©s
- âœ… Pas de risque
- âœ… Code production-ready

### Option Ambitieuse : Expert Data + Gold GNN
- â­ Gain estimÃ© : +10-15 pts â†’ **149-154 pts**
- â±ï¸ Temps : ~26h d'entraÃ®nement
- ğŸ’° CoÃ»t : Ã‰levÃ© mais rÃ©alisable
- ğŸ“ˆ ROI : Excellent si objectif 145+ pts important

### Approche Hybride : Gold GNN seul d'abord
- ğŸ¯ Gain estimÃ© : +3-5 pts â†’ **142-144 pts**
- â±ï¸ Temps : ~12h d'entraÃ®nement
- ğŸ’° CoÃ»t : ModÃ©rÃ©
- ğŸ“ˆ ROI : Bon compromis

**Ma recommandation** : **Approche Hybride** (Gold GNN seul)
- Lancer entraÃ®nement Gold GNN (256-256-128-64) sur 500 parties
- Si rÃ©sultats prometteurs (143+ pts), investir dans Expert Data
- Si Ã©chec, accepter 139.40 pts comme optimal

---

*Document rÃ©digÃ© le 2025-10-26*
*BasÃ© sur l'analyse du gap d'optimalitÃ© (20.5% gap, beam search 174.8 pts)*
