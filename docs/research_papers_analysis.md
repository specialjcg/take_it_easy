# Analyse des Pistes de Recherche pour Take It Easy

## Contexte du Jeu

**Take It Easy** est un jeu avec :
- ‚úÖ Information parfaite (tous les joueurs voient les tuiles jou√©es)
- ‚úÖ Stochasticit√© importante (tirage al√©atoire de 19 tuiles parmi 27)
- ‚ùå PAS d'information cach√©e (pas de r√¥les secrets, pas de cartes cach√©es)
- ‚ùå PAS multi-agents adversaires (chaque joueur optimise son propre plateau)

**Baseline actuel** : Pattern Rollouts V2 = 139.40 pts (79.5% de l'optimal th√©orique)

## Analyse des 10 R√©f√©rences

### üî¥ NON APPLICABLES (Information Cach√©e / R√¥les Secrets)

#### 1. Re-determinizing ISMCTS in Hanabi (Goodman, 2019)
**Raison de rejet** : Hanabi a des cartes cach√©es et information imparfaite. Take It Easy n'a PAS d'information cach√©e.

#### 3. Learning in Games with Progressive Hiding (Heymann et al., 2024)
**Raison de rejet** : Con√ßu pour jeux avec information cach√©e progressive. Non applicable √† Take It Easy.

#### 5. ReBeL - Imperfect-Information Games (Brown et al., 2020)
**Raison de rejet** : Pour jeux √† information imparfaite (poker, etc.). Take It Easy a information parfaite.

#### 6. Hidden-Role Stochastic Games (Han et al., 2023)
**Raison de rejet** : Jeux avec r√¥les cach√©s. Take It Easy n'a pas de r√¥les ni d'adversaires.

#### 7. ISMCTS in Secret Hitler (Reinhardt, 2020)
**Raison de rejet** : Jeu de r√¥le cach√©. Non applicable.

---

### üü° PARTIELLEMENT APPLICABLES (Contexte diff√©rent mais techniques utiles)

#### 9. Evolutionary Algorithm for Hearthstone (Garc√≠a-S√°nchez et al., 2024)
**Applicable** : ‚ö†Ô∏è Limit√©
**Raison** : Hearthstone est multi-joueurs avec adversaires. Approche √©volutionnaire pourrait optimiser les hyperparam√®tres MCTS mais ne change pas fondamentalement l'algorithme.

**Potentiel** : Utiliser algorithme √©volutionnaire pour tuner :
- Param√®tres UCB (exploration/exploitation)
- Poids des heuristiques Pattern Rollouts
- Profondeur de simulation

**Gain estim√©** : +1-2 pts ‚Üí 140-141 pts
**Effort** : 1 semaine

---

### üü¢ TR√àS APPLICABLES (Stochasticit√© + Information Parfaite)

#### 2. Monte Carlo Tree Search: A Review (≈öwiechowski et al., 2021) ‚≠ê‚≠ê‚≠ê
**Applicable** : ‚úÖ Oui
**Pourquoi** : Revue compl√®te des variantes MCTS pour jeux stochastiques.

**Techniques recommand√©es du papier** :
1. **MCTS avec d√©terminisation multiple** : Faire plusieurs arbres MCTS avec diff√©rents tirages possibles
2. **Progressive Widening** : Limiter le nombre de branches enfants dans l'arbre (utile pour l'al√©a)
3. **RAVE am√©lior√©** : All-Moves-As-First (am√©lioration de RAVE que nous avons test√©)

**Gain estim√©** : +2-4 pts ‚Üí 141-143 pts
**Effort** : 2 semaines

---

#### 4. Learning to Play Stochastic Perfect-Information Games (Cohen-Solal et al., 2023) ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê
**Applicable** : ‚úÖ‚úÖ‚úÖ PARFAIT MATCH
**Pourquoi** : **EXACTEMENT le contexte de Take It Easy** : jeu stochastique √† information parfaite.

**Techniques du papier applicables** :
1. **Expectimax MCTS** : Au lieu de UCB classique, utiliser l'esp√©rance sur les tirages al√©atoires possibles
2. **Chance Nodes** : Noeuds repr√©sentant l'al√©a (tirage tuile) s√©par√©s des noeuds de d√©cision
3. **Variance Reduction** : Techniques pour r√©duire la variance des estimations dues √† l'al√©a

**Architecture propos√©e** :
```
√âtat plateau
    ‚Üì
Chance Node (tirage tuile)
    ‚Üì (27 possibilit√©s)
Decision Node (19 positions)
    ‚Üì (r√©p√©ter)
√âvaluation finale
```

**Gain estim√©** : +4-7 pts ‚Üí 143-146 pts ‚≠ê MEILLEURE PISTE
**Effort** : 3 semaines

---

#### 8. Q-Learning for Stochastic Control (2024)
**Applicable** : ‚úÖ Oui
**Pourquoi** : Base th√©orique solide pour RL dans contexte stochastique.

**Apport** : Garanties de convergence pour Q-Learning dans jeux stochastiques. Pourrait remplacer MCTS par Deep Q-Learning avec :
- √âtat = plateau + tuiles disponibles
- Action = (tuile, position)
- Reward = score final

**Gain estim√©** : +3-6 pts ‚Üí 142-145 pts
**Effort** : 4 semaines (complexe, n√©cessite grosse infrastructure d'entra√Ænement)

---

#### 10. MCTS + Supervised Learning for Hearthstone (≈öwiechowski et al., 2018) ‚≠ê‚≠ê‚≠ê‚≠ê
**Applicable** : ‚úÖ‚úÖ Oui
**Pourquoi** : Jeu de cartes stochastique, approche hybride MCTS + r√©seau.

**Techniques applicables** :
1. **Policy Network pour guider MCTS** : R√©seau pr√©dit la "meilleure position" pour chaque tuile ‚Üí r√©duit espace de recherche
2. **Value Network pour estimation** : Remplace rollouts al√©atoires par √©valuation directe du plateau
3. **Training hybride** : MCTS g√©n√®re donn√©es ‚Üí entra√Æne r√©seau ‚Üí r√©seau guide MCTS (boucle)

**Diff√©rence avec notre Gold GNN** : Eux utilisent le r√©seau PENDANT la recherche MCTS (pas apr√®s). Le r√©seau r√©duit l'espace de recherche au lieu de remplacer MCTS.

**Gain estim√©** : +3-5 pts ‚Üí 142-144 pts
**Effort** : 2-3 semaines

---

## Recommandations Prioris√©es

### ü•á Option 1 : Expectimax MCTS (Papier #4) - RECOMMAND√â

**Pourquoi** :
- Contexte EXACT de Take It Easy (stochastique + information parfaite)
- Fondamentalement meilleur que MCTS classique pour ce type de jeu
- MCTS classique ne mod√©lise pas correctement l'al√©a du tirage

**Plan d'impl√©mentation** :
1. Ajouter **Chance Nodes** dans l'arbre MCTS
2. Remplacer UCB par **Expectimax** (moyenne pond√©r√©e sur tirages possibles)
3. Impl√©menter **variance reduction** pour stabiliser estimations

**Fichiers √† modifier** :
- `src/mcts/algorithm.rs` : Ajouter type de noeud `ChanceNode`
- `src/mcts/selection.rs` : Remplacer UCB par Expectimax
- `src/mcts/expansion.rs` : G√©rer expansion chance nodes

**Timeline** :
- Semaine 1 : Impl√©mentation Chance Nodes
- Semaine 2 : Expectimax selection
- Semaine 3 : Variance reduction + tuning

**Gain attendu** : +4-7 pts ‚Üí **143-146 pts** ‚úÖ Atteindrait l'objectif 145 pts !

---

### ü•à Option 2 : MCTS-Guided Neural Network (Papier #10)

**Pourquoi** :
- Combine forces de MCTS (recherche exhaustive) et NN (reconnaissance patterns)
- Retour d'exp√©rience positif sur Hearthstone (jeu similaire)
- Peut r√©utiliser infrastructure CNN existante

**Approche** :
1. **Policy Network** : Pr√©dit P(position | tuile, plateau) ‚Üí top-3 positions
2. **MCTS explore seulement top-3** au lieu de 19 positions ‚Üí 6√ó plus rapide
3. **Value Network** : Remplace pattern rollouts par √©valuation directe

**Diff√©rence cl√© avec Gold GNN √©chou√©** :
- Gold GNN : R√©seau REMPLACE MCTS (√©chec)
- Cette approche : R√©seau GUIDE MCTS (succ√®s attendu)

**Gain attendu** : +3-5 pts ‚Üí 142-144 pts
**Effort** : 2-3 semaines

---

### ü•â Option 3 : Evolutionary Hyperparameter Tuning (Papier #9)

**Pourquoi** : Plus simple, quick win possible.

**Hyperparam√®tres √† optimiser** :
1. UCB exploration constant (actuellement empirique)
2. Poids des heuristiques Pattern Rollouts V2
3. Nombre de simulations par coup
4. Profondeur de rollout

**Algorithme** : CMA-ES (Covariance Matrix Adaptation Evolution Strategy)

**Gain attendu** : +1-2 pts ‚Üí 140-141 pts
**Effort** : 1 semaine

---

## Comparaison avec Approches Pr√©c√©dentes

| Approche | Score | Gain vs Baseline | Statut | Raison |
|----------|-------|------------------|--------|---------|
| Pattern Rollouts V2 (baseline) | 139.40 | - | ‚úÖ PRODUCTION | - |
| Gold GNN (test√©) | 127.74 | -11.66 | ‚ùå √âCHEC | R√©seau remplace MCTS au lieu de guider |
| Curriculum Learning (test√©) | N/A | N/A | ‚ùå ANNUL√â | Beam search pire que MCTS |
| **Expectimax MCTS (Papier #4)** | **143-146** | **+4-7** | üéØ RECOMMAND√â | Fondamentalement adapt√© √† l'al√©a |
| MCTS-Guided NN (Papier #10) | 142-144 | +3-5 | ‚≠ê ALTERNATIF | R√©seau guide MCTS, pas remplace |
| Evolutionary Tuning (Papier #9) | 140-141 | +1-2 | üí° QUICK WIN | Simple √† impl√©menter |

---

## D√©cision Recommand√©e

### ‚úÖ Impl√©menter Expectimax MCTS (Papier #4)

**Justification** :
1. **Adapt√© au jeu** : Stochastique + information parfaite = contexte exact
2. **Gain maximal** : +4-7 pts ‚Üí atteindrait objectif 145 pts
3. **Fondamentalement meilleur** : MCTS classique n'est pas optimal pour jeux avec al√©a
4. **Effort raisonnable** : 3 semaines, r√©utilise code MCTS existant

**Prochaines √©tapes** :
1. Lire papier complet Cohen-Solal et al. (2023)
2. Cr√©er `docs/expectimax_mcts_implementation_plan.md`
3. Impl√©menter Chance Nodes
4. Benchmarker avec 50 games √ó 150 sims
5. Si gain ‚â• +3 pts ‚Üí continuer avec variance reduction

---

## R√©f√©rences Bibliographiques

1. ‚ùå Goodman (2019) - Re-determinizing ISMCTS (Hanabi) - Information cach√©e
2. ‚≠ê‚≠ê‚≠ê ≈öwiechowski et al. (2021) - MCTS Review - Techniques g√©n√©rales
3. ‚ùå Heymann et al. (2024) - Progressive Hiding - Information cach√©e
4. ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê **Cohen-Solal et al. (2023) - Stochastic Perfect-Information Games - MATCH PARFAIT**
5. ‚ùå Brown et al. (2020) - ReBeL - Information imparfaite
6. ‚ùå Han et al. (2023) - Hidden-Role Games - R√¥les cach√©s
7. ‚ùå Reinhardt (2020) - ISMCTS Secret Hitler - R√¥les cach√©s
8. ‚≠ê‚≠ê‚≠ê Q-Learning Stochastic (2024) - Base th√©orique RL
9. ‚≠ê‚≠ê Garc√≠a-S√°nchez et al. (2024) - Evolutionary Hearthstone - Tuning hyperparam√®tres
10. ‚≠ê‚≠ê‚≠ê‚≠ê **≈öwiechowski et al. (2018) - MCTS + Supervised Hearthstone - R√©seau guide MCTS**

**Top 3 √† impl√©menter** :
1. ü•á Papier #4 : Expectimax MCTS
2. ü•à Papier #10 : MCTS-Guided Neural Network
3. ü•â Papier #9 : Evolutionary Hyperparameter Tuning
