# Repository Guidelines

## Project Structure & Module Organization
The Rust backend lives in `src/`, with domain modules like `game/`, `mcts/`, `neural/`, and `servers/` separated for gameplay, search, model inference, and service interfaces. gRPC bindings generated by `build.rs` land in `src/generated/`; avoid editing them directly and update `.proto` files in `protos/` instead. End-to-end and regression tests sit under `tests/`, while reusable automation lives in `scripts/`. The SolidJS client, assets, and Vite config reside in `frontend/`, and neural checkpoints are stored in `model_weights/` alongside the generated `game_data_*` tensors.

## Build, Test, and Development Commands
- `make dev` (or `npm run dev`): launches backend plus SolidJS frontend using `dev_start.sh`.
- `cargo run -- --mode multiplayer`: runs the backend service only; swap the flag for other launch modes defined in `src/main.rs`.
- `make build`: performs a release build and triggers the frontend production bundle.
- `cargo run --bin compare_mcts -- --games 200 --simulations 150`: benchmark pure MCTS versus neural-guided MCTS.
- `run_all_tests.sh`: executes the Rust unit and integration tests sequentially; inspect `lib_tests.log` and `integration_tests.log` if failures occur.

## Coding Style & Naming Conventions
Format Rust code with `cargo fmt` before committing; follow Rust 2021 defaults (4-space indentation, module-level `snake_case`). Prefer `PascalCase` for types and components, `camelCase` for local variables, and `SCREAMING_SNAKE_CASE` for constants. Keep module boundaries aligned with domain folders (e.g., AI helpers stay under `neural/`) and document non-obvious behaviour with concise inline comments. Frontend components should use PascalCase filenames inside `frontend/src`.

## Testing Guidelines
Use `cargo test` locally for fast iterations; target specific scenarios with `cargo test --test lib_integration_test`. Integration suites rely on the pretrained weights shipped in `model_weights/`, so avoid mutating those files in-place‚Äîstage replacements under the same names. Record new regression cases by mirroring the naming pattern in `tests/ui_reactivity_regression_test.rs`. Capture coverage reports by running `run_all_tests.sh` and reviewing artifacts under `coverage/`.

## Commit & Pull Request Guidelines
Match the Conventional Commit style visible in history (`feat(scope): message`, `chore: ...`). Keep commit bodies focused on motivation and include follow-up instructions when manual steps are required. PRs should summarize backend and frontend impacts separately, link to any tracking issues, and attach screenshots or terminal logs when the UI or CLI output changes. Mention data or model updates explicitly so reviewers can verify large artifacts.

## Model & Generated Assets
Regenerate protobuf stubs with `cargo build` after editing files in `protos/`; clean old artifacts via `cargo clean` if you encounter stale compile errors. For AI experiments, prefer adding notebooks or scripts under `docs/` or `scripts/` instead of committing raw experimentation outputs to keep the repo lean. Respect the `.gitignore` rules added for game data and log files, and store new sizeable assets inside `model_weights/`.
# ===============================================================
# ü¶Ä OPENAI CODEX PROMPTS ‚Äî RUST EXPERT SUITE
# ===============================================================
# Place this file at the root of your Rust project or in ~/.config/openai/
# Use with:
#   openai api completions.create -m code-davinci-002 -p "$(grep -A1000 '### [PROMPT NAME]' codex_rust_prompts.txt)"
# Or simply copy the prompt section you want and append your code.
# ===============================================================


### RUST-AUDIT
You are a senior Rust expert specialized in performance, safety, and software architecture.
Task: Audit this Rust project.
Evaluate:
1. Code hygiene (dead code, warnings, unsafe, error handling)
2. Performance (profiling, algorithmic efficiency, memory usage)
3. Maintainability (refactoring, documentation, testing)
4. Production readiness (logging, monitoring, containerization)
   Output:
- Executive summary (‚â§5 lines)
- Action plan with phases: Critical / Performance / Quality / Production
- Concrete tasks, estimated time, expected impact
- Recommended cargo commands (cargo clippy, cargo bench, cargo test, etc.)

#### ‚úÖ Executive Summary
- Production paths still rely on `Tensor::load` panics in `src/main.rs`, risking crashes whenever weights or datasets are missing.
- MCTS search clones `Plateau`/`Deck` every simulation, creating avoidable allocation churn under heavy workloads.
- Observability for training and the new `compare_mcts` benchmark remains minimal, hindering long-running monitoring.
- Multiplayer gRPC flows lack automated regression tests; coverage focuses on local logic only.

#### üõ†Ô∏è Action Plan
- **Critical:** Harden model/dataset loading, propagate domain errors, and validate configuration before serving traffic.
- **Performance:** Profile cloning hotspots inside MCTS loops and introduce shared buffers or borrow-driven updates.
- **Quality:** Add regression coverage for pure-vs-neural comparisons and document expected datasets.
- **Production:** Add health/status endpoints plus log rotation to sustain multiplayer deployments.

#### üìå Concrete Tasks
| Task | Estimate | Impact |
| --- | --- | --- |
| Build fallible weight/dataset loader with typed errors (`src/neural/manager.rs`, `src/main.rs`) | 0.5d | Prevents startup panics and surfaces actionable diagnostics. |
| Swap per-simulation clones for preallocated board buffers in `mcts::algorithm::mcts_find_best_position_for_tile_with_nn` | 1.5d | Cuts allocation overhead and improves simulations per second in self-play. |
| Add regression tests covering `compare_mcts` outputs and pure-versus-neural score deltas (`tests/mcts_comparison.rs`) | 0.75d | Guards against unintended changes to evaluation logic. |
| Add structured tracing + Prometheus metrics to `servers::GrpcServer` startup | 1d | Enables production monitoring and alerting. |

#### üß™ Recommended Commands
- `cargo fmt && cargo clippy -- -D warnings`
- `cargo test --all`
- `cargo run --bin compare_mcts -- --games 200 --simulations 150`
- `cargo bench -- mcts` (after introducing a Criterion harness)
- `cargo audit`


### RUST-ARCHITECTURE
You are a Rust software architect.
Task: Describe the overall architecture of this project.
Explain:
- Application type (CLI, Web API, Library, etc.)
- Structure of src/ (main.rs, lib.rs, modules)
- Data flow between layers
- Key dependencies and business domain
  Output:
- Architecture overview
- File tree structure
- Data flow diagram (text)
- Components and their responsibilities

#### Architecture Overview
- Hybrid backend combining a gRPC multiplayer service, static web hosting, offline self-play training, and analytics binaries (e.g., `compare_mcts`).
- Core game logic lives in pure modules (`src/game`, `src/scoring`, `src/strategy`), orchestrated by service layers plus MCTS/NN integration.
- The neural subsystem (`src/neural`) wraps libtorch models and exposes a `NeuralManager` facade reused across modes.
- `src/training` handles asynchronous self-play pipelines with optional WebSocket feedback.

#### File Tree Structure
```text
src/
  main.rs                  # CLI entrypoint selecting multiplayer or training modes
  bin/
    compare_mcts.rs        # Pure vs neural MCTS benchmarking utility
    inspect_pt.rs          # Dataset inspection helper
  game/                    # Immutable game state, deck, tile, and simulation helpers
  mcts/                    # Tree search algorithm, node snapshots, result structs
  neural/                  # Policy/value networks, tensor conversion, managers
  services/                # Session/game management, gRPC adapters, orchestration
  servers/                 # gRPC server bootstrap + static web server
  training/                # Self-play sessions, evaluators, WebSocket interface
  strategy/                # Heuristic boosters layered onto MCTS
  data/, scoring/, utils/  # Persistence, scoring rules, shared utilities
tests/                     # Integration and regression suites
frontend/                  # SolidJS client served alongside backend
```

#### Data Flow (text)
- Player/Web clients ‚Üí `servers::WebUiServer` (static assets + WebSocket) ‚Üí `services::session_service` for lobby/session lifecycle.
- gRPC calls (`takeiteasygame.v1`) ‚Üí `services::game_service::mcts_integration` ‚Üí `services::game_manager` ‚Üí pure `game::*` logic.
- AI decision path: `game_manager` ‚Üí `mcts::algorithm` ‚Üí `neural::manager` (policy/value nets) ‚Üî `strategy::*` heuristics ‚Üí updated session state.
- Training mode: `training::session` streams self-play data ‚Üí `neural::training` optimisers ‚Üí persists into `game_data_*.pt` and `model_weights/`.
- Analytics: `compare_mcts` replays sampled games using pure and neural-guided MCTS to produce score deltas.

#### Components & Responsibilities
- `NeuralManager` (`src/neural/manager.rs`): encapsulates policy/value networks, optimisers, and VS wrappers.
- `GrpcServer` (`src/servers/grpc.rs`): configures tonic server, optional gRPC-Web layer, and CORS middleware (`SimpleCors`).
- `SessionManager` (`src/services/session_manager.rs`): tracks multiplayer sessions, player queues, and AI participation.
- `MCTSResult` (`src/mcts/mcts_result.rs`) & `strategy::*`: combine neural priors with handcrafted boosts for move ranking.


### RUST-CONTEXT
You are a Rust technical and business analyst.
Task: Explain the project‚Äôs business and technical context.
Include:
- Problem being solved
- Target users
- Main use cases
- Technical constraints
- Integrations (DBs, APIs, services)
- Deployment
  Output:
- üéØ Business Context
- ‚öôÔ∏è Technical Context
- üîó Integrations
- üöÄ Deployment

#### üéØ Business Context
- Deliver an AI-assisted version of the *Take It Easy* board game that supports both casual multiplayer sessions and solo play against strong AI.
- Target users include board-game enthusiasts, competitive players seeking practice, and developers experimenting with neural-enhanced MCTS.
- Primary value proposition is seamless online play with adaptive AI opponents and rapid iteration on model improvements.

#### ‚öôÔ∏è Technical Context
- Rust 2021 async backend built on `tokio`, `tonic`, and `axum`, coupled with libtorch via `tch` for neural inference/training.
- Game logic isolated in pure modules enabling deterministic simulations for MCTS rollouts and dataset generation.
- Execution modes: multiplayer service, headless self-play training, and analytics binaries (e.g., `compare_mcts`) sharing a common neural infrastructure.
- Relies on PyTorch `.pt` assets under `game_data_*.pt` and `model_weights/`, with CUDA acceleration when available.

#### üîó Integrations
- gRPC API (generated into `src/generated/`) consumed by the SolidJS frontend and potential third-party clients.
- WebSocket telemetry channel (port 9000) streaming training metrics to the UI.
- File-based datasets/checkpoints; no database layer, but significant disk I/O to `.pt` tensors and CSV logs.

#### üöÄ Deployment
- Development: `make dev` launches backend + frontend; `cargo run -- --mode multiplayer` serves backend only.
- Production packaging remains ad hoc (no Docker/Helm); requires libtorch setup and built frontend assets under `frontend/dist`.
- Long-running deployments should provision GPU drivers for training modes and mount persistent storage for `model_weights/`.
- Observability limited to flexi_logger output‚Äîadd structured tracing and metrics before public hosting.


### RUST-QUALITY
You are a Rust quality engineer.
Task: Evaluate the quality of this Rust project based on 2025 standards.
Criteria:
- Extensibility, maintainability, readability, testability
- Complexity, unsafe usage, clone overhead
- CI/CD, documentation, monitoring
  Output:
- Global score /100
- Qualitative and quantitative analysis
- Quick wins (<1h)
- Priority improvements
- Recommended cargo commands

#### Global Score
- 68 / 100

#### Qualitative & Quantitative Analysis
- Strengths: Modular boundaries (`src/game`, `src/mcts`, `src/neural`, `src/services`) remain clean; async server bootstrap follows idiomatic tokio patterns; comments document complex areas.
- Risks: Dozens of `unwrap`/`expect` calls in production paths (`src/main.rs`, `src/training/session.rs`) expose runtime panics; limited unit coverage for core AI logic.
- Performance hotspots stem from cloning large boards/tensors within MCTS and running high-simulation benchmarks without reuse of buffers.
- CI/automation story unclear‚Äîno visible workflow enforcing `cargo fmt`, `clippy`, or dataset integrity checks.

#### Quick Wins (<1h)
- Add `cargo audit`/`cargo-deny` to `run_all_tests.sh` and gate merges on `cargo clippy -- -D warnings`.
- Guard tensor/dataset loads with `Path::exists` or fallible helpers to avoid panics while assets synchronise.
- Document required environment variables (libtorch path, CUDA flags) in `README.md` and `AGENTS.md`.

#### Priority Improvements
- Introduce integration tests for gRPC endpoints using `tonic::transport::Channel` within `tokio::test`.
- Refactor the MCTS simulation loop to reuse buffers or apply arena allocation; add Criterion benchmarks for regression tracking.
- Create a configuration layer (serde-driven `config/`) decoupling CLI flags from runtime constants (learning rates, dataset paths).
- Establish CI/CD pipelines (GitHub Actions) running `cargo fmt`, `clippy`, `test`, `npm test`, and publishing release artifacts.

#### Recommended Commands
- `cargo fmt`
- `cargo clippy --all-targets -- -D warnings`
- `cargo test --all -- --nocapture`
- `cargo bench --bench mcts_bench` (once Criterion benchmarks exist)
- `cargo audit`


### RUST-TDD
You are a Rust TDD expert.
Task: Evaluate and improve the project's Test-Driven Development practice.
Include:
- RED ‚Üí GREEN ‚Üí REFACTOR analysis
- Test coverage and structure
- Missing tests and suggested modules
- Recommended commands: cargo test, cargo tarpaulin, cargo bench
  Output:
 - TDD score /100
 - Current test map
 - Plan to strengthen TDD discipline

#### TDD Score
- 55 / 100

#### Current Test Map
- `tests/lib_integration_test.rs`: public API metadata and error-type assertions.
- `tests/ui_reactivity_regression_test.rs`: UI interaction regression harness (partially skipped; requires frontend context).
- Analytics currently validated manually via `cargo run --bin compare_mcts`; no automated assertions yet.

#### Strengthening Plan (RED ‚Üí GREEN ‚Üí REFACTOR)
1. **RED:** Author failing cases around `services::game_manager::apply_player_move` (illegal move, stale waiting list) with deterministic decks.
2. **GREEN:** Implement targeted fixes (error propagation, waiting list updates) and add invariants for `current_tile`.
3. **REFACTOR:** Extract shared fixtures (deck builders, plateau layouts) into `tests/common/mod.rs` to remove duplication.
4. Extend async coverage with `tokio::test` for gRPC/WebSocket flows and add Criterion benchmarks gated by `cargo bench`.

#### Recommended Commands
- `cargo test --lib`
- `cargo test --test lib_integration_test -- --nocapture`
- `cargo tarpaulin --out Html`
- `cargo bench --bench mcts_bench`


### RUST-INTEGRATION
You are a Rust integration & E2E testing expert.
Task: Assess the integration and end-to-end test strategy.
Include:
- Unit, integration, and performance testing
- Tools: tokio-test, testcontainers, wiremock, assert_cmd, tempfile, criterion
  Output:
 - Coverage summary
 - Recommended test directory structure
 - Example test setup
 - CI/CD test workflow commands

#### Coverage Summary
- Integration focus centres on library metadata; multiplayer gRPC endpoints lack automated verification.
- No end-to-end harness starts `GrpcServer` and `WebUiServer` together; smoke tests remain manual.
- Performance regression tracking is absent‚Äîself-play simulations run without baseline measurements beyond the `compare_mcts` binary.

#### Recommended Test Directory Structure
```text
tests/
  integration/
    multiplayer_grpc.rs         # tonic client against in-process server
    websocket_training.rs       # validates training telemetry stream
  performance/
    mcts_bench.rs               # Criterion benchmarks for the search loop
  analytics/
    compare_mcts.rs             # asserts on score differentials and deltas
```

#### Example Test Setup
```rust
#[tokio::test(flavor = "multi_thread")]
async fn grpc_join_session_happy_path() {
    let server = spawn_grpc_server().await;
    let mut client = GameServiceClient::connect(server.uri()).await.unwrap();
    let request = tonic::Request::new(JoinSessionRequest { code: "ABCD".into() });
    let response = client.join_session(request).await.unwrap();
    assert_eq!(response.get_ref().status, ResponseStatus::Joined);
}
```

#### CI/CD Test Workflow Commands
- `cargo fmt --check`
- `cargo clippy --workspace -- -D warnings`
- `cargo test --all -- --nocapture`
- `cargo bench --bench mcts_bench`
- `cargo run --bin compare_mcts -- --games 100 --simulations 150`


### RUST-DOCUMENTATION
You are a Rust technical writer.
Task: Generate or improve documentation for this Rust project.
Include:
- Modules, structs, functions (with examples)
- Architecture summary
- Developer setup (cargo commands, environment)
  Output:
 - üìñ Technical Docs
 - üèóÔ∏è Architecture Docs
 - üõ†Ô∏è DevOps Setup
 - üöÄ Improvement suggestions

#### üìñ Technical Docs
- `src/game`: pure gameplay primitives (`create_deck`, `get_legal_moves`, `simulate_game`) for deterministic simulations.
- `src/mcts`: `algorithm::mcts_find_best_position_for_tile_with_nn` combines board state, policy/value nets, and heuristics into an `MCTSResult`.
- `src/neural`: `NeuralManager` orchestrates libtorch policy/value networks; `tensor_conversion` encodes boards into tensors.
- `src/training::session`: `train_and_evaluate` & `train_and_evaluate_offline` drive self-play loops and persist `.pt` datasets.

#### üèóÔ∏è Architecture Docs
- CLI (`src/main.rs`) accepts `--mode` (`multiplayer`, `training`) and routes into the relevant subsystems.
- Multiplayer flow: CLI ‚Üí `servers::WebUiServer` (static host) & `servers::GrpcServer` ‚Üí `services::game_service` ‚Üí `services::game_manager`.
- Training flow: CLI ‚Üí `training::session` ‚Üí `neural::training` optimisers; artifacts stored under `game_data_*` & `model_weights/`.
- Analytics flow: `cargo run --bin compare_mcts` to benchmark pure vs neural-guided MCTS.

#### üõ†Ô∏è DevOps Setup
- Prerequisites: Rust 1.70+, Node 18+, libtorch (CPU ou CUDA). Export `LD_LIBRARY_PATH` to the libtorch `lib` directory.
- Development: `make dev` (full stack), `cargo run -- --mode multiplayer`, `npm run dev` within `frontend/` for hot reload.
- Dataset preparation: generate via `cargo run -- --mode training --offline-training`; inspect with `cargo run --bin inspect_pt`; compare strategies with `cargo run --bin compare_mcts`.

#### üöÄ Improvement Suggestions
- Add `docs/architecture.md` diagramming game flow, MCTS, and neural pipelines.
- Publish a gRPC API contract in Markdown to onboard external client developers quickly.
- Provide troubleshooting steps for libtorch setup (CUDA mismatches, missing libs).
- Include observability guidance once metrics/logging enhancements land.


### RUST-MIKADO
You are a Rust expert applying the Mikado Method for safe refactoring.
Process:
1. Attempt the goal directly
2. Note blockers as prerequisites
3. Roll back to a stable state
4. Resolve from leaves to root
   Rust-specific:
- Run `cargo check` after each leaf refactor
- Maintain borrow checker safety
  Output:
 - üéØ Main goal
 - üå≥ Mikado graph (tree with leaf ‚≠ê markers)
 - üöÄ Next actionable step
 - üí° cargo commands per step

#### üéØ Main Goal
- Make neural weight and dataset loading resilient and observable without panicking in production modes.

#### üå≥ Mikado Graph
- Replace panic-prone tensor loading with typed error handling
  - ‚≠ê Introduce a `ModelAssets` helper in `src/neural/manager.rs` returning `Result<ModelAssets, ModelLoadError>`
    - ‚≠ê Extend `TakeItEasyError` with a model-loading variant carrying the source `tch::TchError`
    - ‚≠ê Move path fallback logic from `src/main.rs` into the helper with configurable search order
  - Update `NeuralManager::with_config` to consume the helper and bubble errors upward
- Add integration coverage for missing-weight scenarios
  - ‚≠ê Create temporary fixture directories via `tempfile`
  - ‚≠ê Assert multiplayer startup fails gracefully with descriptive errors

#### üöÄ Next Actionable Step
- Sketch the `ModelAssets` helper in `src/neural/manager.rs` returning explicit `ModelLoadError`.

#### üí° Cargo Commands per Step
- `cargo check`
- `cargo test --lib`
- `cargo test --test multiplayer_startup`


### RUST-FUNCTIONAL
You are a Rust functional programming engineer.
Task: Refactor code to apply functional programming principles.
Focus:
- Immutability
- Pure functions
- Composition and combinators (map, and_then)
- Idiomatic Result/Option usage
  Output:
 - Functional patterns applied
 - Before/After example
 - Cargo validation commands

#### Functional Patterns Applied
- Promote immutable transformations by deriving next state via iterator pipelines rather than mutating buffers in place.
- Replace imperative guard clauses with combinators (`then_some`, `ok_or_else`, `map`) to surface domain errors succinctly.
- Compose tile selection and deck replacement through `Option`/`Result` chaining, eliminating temporary mutable variables.

#### Before / After
```rust
// Before (src/services/game_manager.rs)
pub fn start_new_turn(mut game_state: TakeItEasyGameState) -> Result<TakeItEasyGameState, String> {
    if game_state.current_turn >= game_state.total_turns {
        return Err("GAME_ALREADY_FINISHED".into());
    }
    let valid_tiles: Vec<Tile> = game_state
        .deck
        .tiles
        .iter()
        .cloned()
        .filter(|tile| *tile != Tile(0, 0, 0))
        .collect();
    if valid_tiles.is_empty() {
        return Err("NO_TILES_REMAINING".into());
    }
    let chosen_tile = valid_tiles[rand::rng().random_range(0..valid_tiles.len())];
    game_state.deck = replace_tile_in_deck(&game_state.deck, &chosen_tile);
    // ...
}
```

```rust
// After (proposed)
pub fn start_new_turn(game_state: TakeItEasyGameState) -> Result<TakeItEasyGameState, String> {
    (game_state.current_turn < game_state.total_turns)
        .then_some(game_state)
        .ok_or_else(|| "GAME_ALREADY_FINISHED".to_string())
        .and_then(|state| {
            state
                .deck
                .tiles
                .iter()
                .cloned()
                .filter(|tile| *tile != Tile(0, 0, 0))
                .collect::<Vec<_>>()
                .split_last()
                .map(|(_, pool)| (state, pool))
                .ok_or_else(|| "NO_TILES_REMAINING".to_string())
        })
        .map(|(mut state, pool)| {
            let idx = rand::rng().random_range(0..pool.len());
            let chosen = pool[idx];
            let deck = replace_tile_in_deck(&state.deck, &chosen);
            let waiting = state.player_plateaus.keys().cloned().collect();
            TakeItEasyGameState {
                deck,
                current_tile: Some(chosen),
                waiting_for_players: waiting,
                ..state
            }
        })
}
```

#### Cargo Validation Commands
- `cargo check --lib`
- `cargo test --test lib_integration_test`
- `cargo fmt`


### RUST-DEDUP
You are a Rust refactoring engineer.
Task: Detect code duplication and redundant logic across the entire project.
Suggest consolidation through utility functions, traits, or shared modules.
Output:
- File / Redundancy Detected / Suggested Refactor / Expected Impact
- Validation commands: cargo check, cargo clippy, cargo test

| File(s) | Redundancy Detected | Suggested Refactor | Expected Impact |
| --- | --- | --- | --- |
| `src/services/game_manager.rs` vs `src/game/simulate_game.rs` | Tile filtering/drawing logic re-implemented with similar rules | Extract a reusable `draw_valid_tile` helper within `src/game` and reuse across modules | Keeps gameplay invariants consistent and eases future rule tweaks |
| `src/bin/compare_mcts.rs` vs `src/training/session.rs` | Duplicate tile sampling and plateau-reset loops | Move shared helpers into `src/game/draw.rs` to keep benchmarking and training aligned | Ensures analytics and training evolve together |
| `src/bin/compare_mcts.rs` Stats helper vs future reporting needs | Ad-hoc stats computation repeated in benchmarks/tests | Promote `Stats` helper into `src/utils` for reuse across tests and analytics | Simplifies adding assertions around score deltas |

**Validation Commands:** `cargo check`, `cargo clippy`, `cargo test`


### RUST-CARGO-CHECK
You are a Rust compilation safety guard.
Task: After each refactor or new implementation, ensure compilation success.
Commands:
- cargo check  (compilation validation)
- cargo clippy (lint)
- cargo test --no-run (ensure tests compile)
  Output:
- Files affected
- Errors and warnings
- Fix recommendations

#### Files Affected
- Focus on core modules when implementing recommendations: `src/main.rs`, `src/neural/manager.rs`, `src/services/game_manager.rs`, and analytics binaries.

#### Errors & Warnings to Monitor
- `cargo clippy` will flag clone-heavy patterns (`clippy::map_clone`, `clippy::too_many_arguments`) in `mcts::algorithm` once linting is enforced.
- `cargo audit` currently reports `rand = 0.9.0-beta.1` as pre-release; plan to upgrade to the forthcoming stable cut.
- Building without libtorch available fails at link time‚Äîdocument `LD_LIBRARY_PATH` setup or add feature-gated stubs for CI.

#### Fix Recommendations
- Introduce feature flags to compile the multiplayer server without libtorch for lightweight CI runs.
- Address lint findings before adding `#[allow]` attributes; favour refactoring over suppression.
- Automate `cargo fmt && cargo check && cargo test --no-run` in CI to surface issues early.

#### Suggested Commands
- `cargo fmt --check`
- `cargo check --all-targets`
- `cargo clippy --workspace --all-features`
- `cargo test --no-run`
