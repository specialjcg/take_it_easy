//! Lightweight Transformer stack used to experiment with learned priors for the MCTS agent.
//!
//! The module exposes:
//! - [`TransformerModel`], a compact encoder with policy/value heads and serialization helpers.
//! - [`training`] utilities for supervised fine-tuning on datasets generated by self-play.
//! - [`mcts_integration`] helpers that plug Transformer predictions back into search.
//! Supporting submodules (attention, optimization, profiling) are intentionally lightweight
//! and tuned for CPU-friendly experimentation.
pub mod attention;
pub mod evaluation;
pub mod game_state;
pub mod hybrid_policy;
pub mod mcts_integration;
pub mod model;
mod model_test;
pub mod optimization;
pub mod profiling;
pub mod training;

use attention::{KeyTensor, QueryTensor, ValueTensor};
use optimization::{OptimizedConfig, OptimizedTransformer};
use profiling::TransformerProfiler;
use std::result::Result;
use tch::nn::{self};
use tch::{Device, Kind, Tensor};

pub type TchResult<T> = Result<T, tch::TchError>;

// Architecture CNN standard (format [1,5,47,1])
const NUM_LAYERS: i64 = 2;
const EMBEDDING_DIM: i64 = 64;
const NUM_HEADS: i64 = 2;
const FF_DIM: i64 = 128;
const POLICY_OUTPUTS: i64 = 19;

#[derive(Debug)]
pub enum TransformerError {
    AttentionError(attention::AttentionError),
    DimensionError(String),
    DeviceError(String),
    OptimizationError(String),
    ProfilingError(String),
    TensorConversionError(String),
}

impl From<attention::AttentionError> for TransformerError {
    fn from(err: attention::AttentionError) -> Self {
        TransformerError::AttentionError(err)
    }
}

impl std::fmt::Display for TransformerError {
    fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {
        match self {
            TransformerError::AttentionError(e) => write!(f, "Attention error: {}", e),
            TransformerError::DimensionError(msg) => write!(f, "Dimension error: {}", msg),
            TransformerError::DeviceError(msg) => write!(f, "Device error: {}", msg),
            TransformerError::OptimizationError(msg) => write!(f, "Optimization error: {}", msg),
            TransformerError::ProfilingError(msg) => write!(f, "Profiling error: {}", msg),
            TransformerError::TensorConversionError(msg) => {
                write!(f, "Tensor conversion error: {}", msg)
            }
        }
    }
}

// Structure déplacée au niveau du module
#[derive(Debug, Default)]
pub struct PerformanceMetrics {
    pub inference_time_ms: f32,
    pub memory_usage_mb: f64,
    pub throughput: f32,
}

#[derive(Debug, Clone)]
pub struct TransformerConfig {
    num_layers: i64,
    embedding_dim: i64,
    num_heads: i64,
    ff_dim: i64,
    dropout_rate: Option<f64>,
}

impl Default for TransformerConfig {
    fn default() -> Self {
        Self {
            num_layers: NUM_LAYERS,
            embedding_dim: EMBEDDING_DIM,
            num_heads: NUM_HEADS,
            ff_dim: FF_DIM,
            dropout_rate: None,
        }
    }
}

impl TransformerConfig {
    pub fn new(
        embedding_dim: i64,
        num_heads: i64,
        num_layers: i64,
    ) -> Result<Self, TransformerError> {
        if embedding_dim <= 0 || num_heads <= 0 || num_layers <= 0 {
            return Err(TransformerError::DimensionError(
                "Dimensions must be positive".into(),
            ));
        }
        if embedding_dim % num_heads != 0 {
            return Err(TransformerError::DimensionError(
                "Embedding dimension must be divisible by number of heads".into(),
            ));
        }

        Ok(Self {
            num_layers,
            embedding_dim,
            num_heads,
            ff_dim: embedding_dim * 4,
            dropout_rate: None,
        })
    }

    pub fn with_dropout(mut self, rate: f64) -> Result<Self, TransformerError> {
        if !(0.0..=1.0).contains(&rate) {
            return Err(TransformerError::DimensionError(
                "Dropout rate must be between 0 and 1".into(),
            ));
        }
        self.dropout_rate = Some(rate);
        Ok(self)
    }

    pub fn embedding_dim(&self) -> i64 {
        self.embedding_dim
    }
}

// #[derive(Debug)] // SUPPRIMÉ car AttentionLayer ne supporte pas Debug
pub struct TransformerLayer {
    pub attention: attention::AttentionLayer,
    pub ff1: nn::Linear,
    pub ff2: nn::Linear,
    pub ff_dim: i64,
}

impl TransformerLayer {
    pub fn new<'a>(
        config: &TransformerConfig,
        path: &nn::Path<'a>,
    ) -> Result<Self, TransformerError> {
        let attention_config =
            attention::AttentionConfig::new(config.embedding_dim, config.num_heads)?;
        let attention = attention::AttentionLayer::new(attention_config, &(path / "attention"));
        let ff1 = nn::linear(
            path / "ff1",
            config.embedding_dim,
            config.ff_dim,
            Default::default(),
        );
        let ff2 = nn::linear(
            path / "ff2",
            config.ff_dim,
            config.embedding_dim,
            Default::default(),
        );
        Ok(Self {
            attention,
            ff1,
            ff2,
            ff_dim: config.ff_dim,
        })
    }
}

impl Clone for TransformerLayer {
    fn clone(&self) -> Self {
        Self {
            attention: attention::AttentionLayer {
                config: self.attention.config.clone(),
                linear_q: nn::Linear {
                    ws: self.attention.linear_q.ws.shallow_clone(),
                    bs: self
                        .attention
                        .linear_q
                        .bs
                        .as_ref()
                        .map(|t| t.shallow_clone()),
                },
                linear_k: nn::Linear {
                    ws: self.attention.linear_k.ws.shallow_clone(),
                    bs: self
                        .attention
                        .linear_k
                        .bs
                        .as_ref()
                        .map(|t| t.shallow_clone()),
                },
                linear_v: nn::Linear {
                    ws: self.attention.linear_v.ws.shallow_clone(),
                    bs: self
                        .attention
                        .linear_v
                        .bs
                        .as_ref()
                        .map(|t| t.shallow_clone()),
                },
                linear_out: nn::Linear {
                    ws: self.attention.linear_out.ws.shallow_clone(),
                    bs: self
                        .attention
                        .linear_out
                        .bs
                        .as_ref()
                        .map(|t| t.shallow_clone()),
                },
            },
            ff1: nn::Linear {
                ws: self.ff1.ws.shallow_clone(),
                bs: self.ff1.bs.as_ref().map(|t| t.shallow_clone()),
            },
            ff2: nn::Linear {
                ws: self.ff2.ws.shallow_clone(),
                bs: self.ff2.bs.as_ref().map(|t| t.shallow_clone()),
            },
            ff_dim: self.ff_dim,
        }
    }
}

// #[derive(Debug)] // SUPPRIMÉ car les champs ne supportent pas Debug/Default
pub struct TransformerModel {
    config: TransformerConfig,
    layers: Vec<TransformerLayer>,
    policy_head: nn::Linear,
    value_head: nn::Linear,
    boost_head: nn::Linear, // Nouvelle tête pour prédire les boosts (19 positions)
}

// SAFETY: TransformerModel is Send + Sync in evaluation mode (immutable inference)
// All tch::Tensor objects are internally Arc-based and thread-safe for read operations
// We ensure no mutable access during parallel inference
unsafe impl Send for TransformerModel {}
unsafe impl Sync for TransformerModel {}

impl TransformerModel {
    pub fn new<'a>(
        config: TransformerConfig,
        path: &nn::Path<'a>,
    ) -> Result<Self, TransformerError> {
        let mut layers = Vec::with_capacity(config.num_layers as usize);
        for i in 0..config.num_layers {
            layers.push(TransformerLayer::new(
                &config,
                &(path / format!("layer_{}", i)),
            )?);
        }
        let policy_head = nn::linear(
            path / "policy_head",
            config.embedding_dim,
            POLICY_OUTPUTS,
            Default::default(),
        );
        let value_head = nn::linear(
            path / "value_head",
            config.embedding_dim,
            1,
            Default::default(),
        );
        let boost_head = nn::linear(
            path / "boost_head",
            config.embedding_dim,
            POLICY_OUTPUTS, // 19 positions, probabilité de boost pour chaque
            Default::default(),
        );
        Ok(Self {
            config,
            layers,
            policy_head,
            value_head,
            boost_head,
        })
    }

    fn encode(&self, input: &Tensor) -> Result<Tensor, TransformerError> {
        let mut x = match input.size().as_slice() {
            [_seq, _dim] => input.unsqueeze(0), // [seq, dim] -> [1, seq, dim]
            [_batch, _seq, _dim] => input.shallow_clone(),
            _ => {
                return Err(TransformerError::AttentionError(
                    attention::AttentionError::new(
                        attention::AttentionErrorKind::ShapeMismatch,
                        "All inputs must be 2D or 3D tensors [batch, seq, dim] or [seq, dim]",
                    ),
                ))
            }
        };
        for layer in &self.layers {
            let attended = layer.attention.forward(
                attention::QueryTensor(x.shallow_clone()),
                attention::KeyTensor(x.shallow_clone()),
                attention::ValueTensor(x.shallow_clone()),
            )?;
            let normalized = attended.layer_norm::<Tensor>(
                &[self.config.embedding_dim],
                None,
                None,
                1e-5,
                false,
            );
            let ff_output = normalized.apply(&layer.ff1).gelu("none").apply(&layer.ff2);
            x = ff_output;
        }
        Ok(x)
    }

    pub fn forward(&self, input: &Tensor) -> Result<Tensor, TransformerError> {
        self.encode(input)
    }

    pub fn infer(&self, input: &Tensor) -> Result<(Tensor, Tensor), TransformerError> {
        let encoded = self.encode(input)?;
        let pooled = encoded.mean_dim(&[1i64][..], false, Kind::Float);
        let policy_logits = pooled.apply(&self.policy_head);
        let value = pooled.apply(&self.value_head).tanh();
        Ok((policy_logits, value))
    }

    /// Inférence complète incluant la prédiction de boost
    pub fn infer_with_boost(&self, input: &Tensor) -> Result<(Tensor, Tensor, Tensor), TransformerError> {
        let encoded = self.encode(input)?;
        let pooled = encoded.mean_dim(&[1i64][..], false, Kind::Float);
        let policy_logits = pooled.apply(&self.policy_head);
        let value = pooled.apply(&self.value_head).tanh();
        let boost_logits = pooled.apply(&self.boost_head); // Logits de probabilité de boost
        Ok((policy_logits, value, boost_logits))
    }

    pub fn get_attention_weights(&self, input: &Tensor) -> Result<Vec<Tensor>, TransformerError> {
        let mut attention_weights = Vec::with_capacity(self.config.num_layers as usize);
        let mut current = input.shallow_clone();

        for layer in &self.layers {
            // Obtenir les poids d'attention pour chaque couche
            let layer_weights = layer.attention.get_attention_weights(
                QueryTensor(current.shallow_clone()),
                KeyTensor(current.shallow_clone()),
                ValueTensor(current.shallow_clone()),
            )?;

            // Forward pass pour la prochaine couche
            current = layer.attention.forward(
                QueryTensor(current.shallow_clone()),
                KeyTensor(current.shallow_clone()),
                ValueTensor(current.shallow_clone()),
            )?;

            attention_weights.push(layer_weights);
        }

        Ok(attention_weights)
    }

    pub fn config(&self) -> &TransformerConfig {
        &self.config
    }

    pub fn with_optimizations(
        self,
        config: OptimizedConfig,
    ) -> Result<OptimizedTransformer, TransformerError> {
        OptimizedTransformer::new(self, config)
    }

    pub fn with_profiling(self) -> (Self, TransformerProfiler) {
        (self, TransformerProfiler::new())
    }

    pub fn layers(&self) -> &[TransformerLayer] {
        &self.layers
    }

    pub fn predict(&self, input: &Tensor) -> (Tensor, Tensor) {
        self.infer(input).expect("Transformer forward failed")
    }

    // Implémentation des fonctionnalités de monitoring
    pub fn get_performance_metrics(
        &self,
        batch_size: i64,
    ) -> Result<PerformanceMetrics, TransformerError> {
        let input = Tensor::rand(&[batch_size, 4, 64], (Kind::Float, Device::Cpu));

        let start = std::time::Instant::now();
        let _ = self
            .forward(&input)
            .map_err(|e| TransformerError::OptimizationError(e.to_string()))?;
        let duration = start.elapsed();

        Ok(PerformanceMetrics {
            inference_time_ms: duration.as_secs_f32() * 1000.0,
            memory_usage_mb: self.estimate_memory_usage(),
            throughput: batch_size as f32 / duration.as_secs_f32(),
        })
    }

    fn estimate_memory_usage(&self) -> f64 {
        let config = self.config();
        let params_per_layer = config.embedding_dim * config.ff_dim * 2
            + config.embedding_dim * config.embedding_dim * 4;
        let total_params = params_per_layer * config.num_layers;

        // Conversion en MB (approximation: 4 bytes par paramètre)
        (total_params as f64 * 4.0) / (1024.0 * 1024.0)
    }

    pub fn save_model(&self, path: &str) -> Result<(), Box<dyn std::error::Error>> {
        std::fs::create_dir_all(path)?;
        for (i, layer) in self.layers.iter().enumerate() {
            layer
                .ff1
                .ws
                .save(&format!("{}/layer{}_ff1_ws.ot", path, i))?;
            if let Some(ref bs) = layer.ff1.bs {
                bs.save(&format!("{}/layer{}_ff1_bs.ot", path, i))?;
            }
            layer
                .ff2
                .ws
                .save(&format!("{}/layer{}_ff2_ws.ot", path, i))?;
            if let Some(ref bs) = layer.ff2.bs {
                bs.save(&format!("{}/layer{}_ff2_bs.ot", path, i))?;
            }
        }
        self.policy_head
            .ws
            .save(&format!("{}/policy_head_ws.ot", path))?;
        if let Some(ref bs) = self.policy_head.bs {
            bs.save(&format!("{}/policy_head_bs.ot", path))?;
        }
        self.value_head
            .ws
            .save(&format!("{}/value_head_ws.ot", path))?;
        if let Some(ref bs) = self.value_head.bs {
            bs.save(&format!("{}/value_head_bs.ot", path))?;
        }
        self.boost_head
            .ws
            .save(&format!("{}/boost_head_ws.ot", path))?;
        if let Some(ref bs) = self.boost_head.bs {
            bs.save(&format!("{}/boost_head_bs.ot", path))?;
        }
        Ok(())
    }

    pub fn load_model(&mut self, path: &str) -> Result<(), Box<dyn std::error::Error>> {
        for (i, layer) in self.layers.iter_mut().enumerate() {
            layer.ff1.ws = layer.ff1.ws.detach();
            layer.ff1.ws.copy_(&tch::Tensor::load(&format!(
                "{}/layer{}_ff1_ws.ot",
                path, i
            ))?);
            if let Some(ref mut bs) = layer.ff1.bs {
                *bs = bs.detach();
                bs.copy_(&tch::Tensor::load(&format!(
                    "{}/layer{}_ff1_bs.ot",
                    path, i
                ))?);
            }
            layer.ff2.ws = layer.ff2.ws.detach();
            layer.ff2.ws.copy_(&tch::Tensor::load(&format!(
                "{}/layer{}_ff2_ws.ot",
                path, i
            ))?);
            if let Some(ref mut bs) = layer.ff2.bs {
                *bs = bs.detach();
                bs.copy_(&tch::Tensor::load(&format!(
                    "{}/layer{}_ff2_bs.ot",
                    path, i
                ))?);
            }
        }
        self.policy_head.ws = self.policy_head.ws.detach();
        self.policy_head
            .ws
            .copy_(&tch::Tensor::load(&format!("{}/policy_head_ws.ot", path))?);
        if let Some(ref mut bs) = self.policy_head.bs {
            *bs = bs.detach();
            bs.copy_(&tch::Tensor::load(&format!("{}/policy_head_bs.ot", path))?);
        }
        self.value_head.ws = self.value_head.ws.detach();
        self.value_head
            .ws
            .copy_(&tch::Tensor::load(&format!("{}/value_head_ws.ot", path))?);
        if let Some(ref mut bs) = self.value_head.bs {
            *bs = bs.detach();
            bs.copy_(&tch::Tensor::load(&format!("{}/value_head_bs.ot", path))?);
        }
        // Charger la tête de boost si elle existe (rétrocompatibilité)
        let boost_ws_path = format!("{}/boost_head_ws.ot", path);
        if std::path::Path::new(&boost_ws_path).exists() {
            self.boost_head.ws = self.boost_head.ws.detach();
            self.boost_head.ws.copy_(&tch::Tensor::load(&boost_ws_path)?);
            if let Some(ref mut bs) = self.boost_head.bs {
                *bs = bs.detach();
                bs.copy_(&tch::Tensor::load(&format!("{}/boost_head_bs.ot", path))?);
            }
        }
        Ok(())
    }
}

impl Clone for TransformerModel {
    fn clone(&self) -> Self {
        Self {
            config: self.config.clone(),
            layers: self.layers.iter().map(|layer| layer.clone()).collect(),
            policy_head: nn::Linear {
                ws: self.policy_head.ws.shallow_clone(),
                bs: self.policy_head.bs.as_ref().map(|t| t.shallow_clone()),
            },
            value_head: nn::Linear {
                ws: self.value_head.ws.shallow_clone(),
                bs: self.value_head.bs.as_ref().map(|t| t.shallow_clone()),
            },
            boost_head: nn::Linear {
                ws: self.boost_head.ws.shallow_clone(),
                bs: self.boost_head.bs.as_ref().map(|t| t.shallow_clone()),
            },
        }
    }
}

// Tests d'intégration
#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    fn test_transformer_config() {
        assert!(TransformerConfig::new(64, 2, 2).is_ok());
        assert!(TransformerConfig::new(-1, 2, 2).is_err());
        assert!(TransformerConfig::new(63, 2, 2).is_err());
    }

    #[test]
    fn test_transformer_forward() {
        let config = TransformerConfig::new(64, 2, 2).unwrap();
        let vs = nn::VarStore::new(tch::Device::Cpu);
        let model = TransformerModel::new(config, &vs.root()).unwrap();

        let input = Tensor::rand(&[1, 4, 64], (Kind::Float, Device::Cpu));
        let output = model.forward(&input);

        assert!(output.is_ok());
        if let Ok(output) = output {
            assert_eq!(output.size(), input.size());
        }
    }

    #[test]
    fn test_transformer_dropout() {
        let config = TransformerConfig::new(64, 2, 2)
            .unwrap()
            .with_dropout(0.1)
            .unwrap();
        let vs = nn::VarStore::new(tch::Device::Cpu);
        let model = TransformerModel::new(config, &vs.root()).unwrap();

        let input = Tensor::rand(&[1, 4, 64], (Kind::Float, Device::Cpu));
        assert!(model.forward(&input).is_ok());
    }

    #[test]
    fn test_optimized_transformer() {
        let vs = nn::VarStore::new(tch::Device::Cpu);
        let model = TransformerModel::new(TransformerConfig::default(), &vs.root()).unwrap();
        let optimized = model.with_optimizations(OptimizedConfig::default());
        assert!(optimized.is_ok());
    }

    // Test temporairement désactivé - problème d'import du macro profile_section dans les tests
    #[test]
    #[ignore]
    fn test_transformer_profiling() {
        let vs = nn::VarStore::new(tch::Device::Cpu);
        let model = TransformerModel::new(TransformerConfig::default(), &vs.root()).unwrap();
        let (_model, _profiler) = model.with_profiling();
        // crate::profile_section!(&mut profiler, "test_forward", 0.0, {
        //     let input = Tensor::rand(&[1, 4, 64], (Kind::Float, Device::Cpu));
        //     model.forward(&input).unwrap()
        // });
        // let stats = profiler.get_operation_stats("test_forward");
        // assert!(stats.is_some());
    }

    #[test]
    fn test_performance_metrics() {
        let vs = nn::VarStore::new(tch::Device::Cpu);
        let model = TransformerModel::new(TransformerConfig::default(), &vs.root()).unwrap();
        let metrics = model.get_performance_metrics(32);
        assert!(metrics.is_ok());
        if let Ok(metrics) = metrics {
            assert!(metrics.inference_time_ms > 0.0);
            assert!(metrics.memory_usage_mb > 0.0);
            assert!(metrics.throughput > 0.0);
        }
    }

    #[test]
    fn test_model_saving_loading() {
        let config = TransformerConfig::new(64, 2, 2).unwrap();
        let vs = nn::VarStore::new(tch::Device::Cpu);
        let model = TransformerModel::new(config.clone(), &vs.root()).unwrap();

        // Sauvegarder le modèle
        let save_path = "test_model";
        model.save_model(save_path).unwrap();

        // Charger le modèle
        let vs2 = nn::VarStore::new(tch::Device::Cpu);
        let mut loaded_model = TransformerModel::new(config, &vs2.root()).unwrap();
        loaded_model.load_model(save_path).unwrap();

        // Vérifier que la sortie est identique après chargement
        let input = Tensor::rand(&[1, 4, 64], (Kind::Float, Device::Cpu));
        let original_output = model.forward(&input).unwrap();
        let loaded_output = loaded_model.forward(&input).unwrap();
        assert!(original_output.allclose(&loaded_output, 1e-5, 1e-8, false));
    }
}
