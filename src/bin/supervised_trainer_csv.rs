//! CSV-based Supervised Trainer for GNN
//!
//! Trains neural networks on pure MCTS expert data generated by generate_supervised_dataset
//! Reads CSV format: game_id,turn,plateau_0-18,tile_0-2,position,final_score

use clap::Parser;
use csv::ReaderBuilder;
use flexi_logger::Logger;
use rand::prelude::*;
use rand::seq::SliceRandom;
use std::error::Error;
use std::fs::File;
use tch::{Device, Tensor};

use take_it_easy::neural::manager::NNArchitecture;
use take_it_easy::neural::{NeuralConfig, NeuralManager};

#[derive(Parser, Debug)]
#[command(
    name = "supervised-trainer-csv",
    about = "Train neural networks from CSV expert data"
)]
struct Args {
    /// CSV file with expert data
    #[arg(short, long)]
    data: String,

    /// Number of training epochs
    #[arg(short, long, default_value_t = 50)]
    epochs: usize,

    /// Batch size for training
    #[arg(short, long, default_value_t = 64)]
    batch_size: usize,

    /// Learning rate for policy network
    #[arg(long, default_value_t = 0.001)]
    policy_lr: f64,

    /// Learning rate for value network
    #[arg(long, default_value_t = 0.0001)]
    value_lr: f64,

    /// Neural network architecture (CNN or GNN)
    #[arg(long, default_value = "GNN")]
    nn_architecture: String,

    /// Validation split (0.0-1.0)
    #[arg(long, default_value_t = 0.1)]
    validation_split: f64,

    /// Random seed for shuffling
    #[arg(long, default_value_t = 42)]
    seed: u64,
}

#[derive(Debug, Clone)]
struct TrainingExample {
    plateau_state: Vec<i32>,  // 19 encoded tiles
    tile: (i32, i32, i32),    // Current tile
    position: usize,          // Target position (policy label)
    final_score: i32,         // Final game score (value label)
}

fn main() -> Result<(), Box<dyn Error>> {
    Logger::try_with_env_or_str("info")?
        .format(flexi_logger::colored_default_format)
        .start()?;

    let args = Args::parse();

    log::info!("üéì CSV Supervised Trainer");
    log::info!("Architecture: {}", args.nn_architecture);
    log::info!("Data file: {}", args.data);
    log::info!("Epochs: {}", args.epochs);
    log::info!("Batch size: {}", args.batch_size);
    log::info!("Policy LR: {}", args.policy_lr);
    log::info!("Value LR: {}", args.value_lr);

    // Parse architecture
    let nn_arch = match args.nn_architecture.to_uppercase().as_str() {
        "CNN" => NNArchitecture::Cnn,
        "GNN" => NNArchitecture::Gnn,
        _ => return Err(format!("Invalid architecture: {}", args.nn_architecture).into()),
    };

    // Load training data from CSV
    log::info!("\nüìÇ Loading training data from CSV...");
    let examples = load_csv_data(&args.data)?;
    log::info!("‚úÖ Loaded {} training examples", examples.len());

    // Calculate statistics
    let scores: Vec<i32> = examples.iter().map(|e| e.final_score).collect();
    let avg_score = scores.iter().sum::<i32>() as f64 / scores.len() as f64;
    let min_score = *scores.iter().min().unwrap();
    let max_score = *scores.iter().max().unwrap();
    log::info!("üìä Score statistics: avg={:.1}, range=[{}, {}]", avg_score, min_score, max_score);

    // Shuffle and split data
    let mut rng = rand::rngs::StdRng::seed_from_u64(args.seed);
    let mut shuffled = examples.clone();
    shuffled.shuffle(&mut rng);

    let split_idx = ((1.0 - args.validation_split) * shuffled.len() as f64) as usize;
    let (train_data, val_data) = shuffled.split_at(split_idx);
    log::info!("Split: {} training, {} validation examples", train_data.len(), val_data.len());

    // Initialize neural network
    log::info!("\nüß† Initializing {} neural network...", args.nn_architecture);
    let input_channels = 8; // GNN uses 8 channels (no position ID needed)

    let neural_config = NeuralConfig {
        input_dim: (input_channels, 5, 5),
        nn_architecture: nn_arch,
        policy_lr: args.policy_lr,
        value_lr: args.value_lr,
        ..Default::default()
    };
    let mut manager = NeuralManager::with_config(neural_config)?;
    log::info!("‚úÖ Neural network initialized");

    // Training loop
    log::info!("\nüèãÔ∏è Starting training...");
    let device = Device::Cpu;
    let mut best_val_loss = f64::INFINITY;
    let patience = 10;
    let mut epochs_without_improvement = 0;

    for epoch in 0..args.epochs {
        // Training
        let (train_policy_loss, train_value_loss) = train_epoch(
            train_data,
            &mut manager,
            args.batch_size,
            device,
        )?;

        // Validation
        let (val_policy_loss, val_value_loss) = validate_epoch(
            val_data,
            &manager,
            args.batch_size,
            device,
        )?;

        let total_val_loss = val_policy_loss + val_value_loss;

        // Log progress
        if (epoch + 1) % 5 == 0 || epoch == 0 || epoch == args.epochs - 1 {
            log::info!(
                "Epoch {:3}/{} | Train: policy={:.4}, value={:.4} | Val: policy={:.4}, value={:.4}",
                epoch + 1,
                args.epochs,
                train_policy_loss,
                train_value_loss,
                val_policy_loss,
                val_value_loss
            );
        }

        // Early stopping
        if total_val_loss < best_val_loss {
            best_val_loss = total_val_loss;
            epochs_without_improvement = 0;

            // Save best model
            manager.save_models()?;
            if (epoch + 1) % 10 == 0 {
                log::info!("üíæ Saved improved model (val_loss={:.4})", best_val_loss);
            }
        } else {
            epochs_without_improvement += 1;
            if epochs_without_improvement >= patience {
                log::info!(
                    "‚ö†Ô∏è Early stopping at epoch {} (no improvement for {} epochs)",
                    epoch + 1,
                    patience
                );
                break;
            }
        }
    }

    log::info!("\nüéâ Training Complete!");
    log::info!("Best validation loss: {:.4}", best_val_loss);
    log::info!("Model weights saved to default location");

    Ok(())
}

fn load_csv_data(path: &str) -> Result<Vec<TrainingExample>, Box<dyn Error>> {
    let file = File::open(path)?;
    let mut reader = ReaderBuilder::new()
        .has_headers(true)
        .from_reader(file);

    let mut examples = Vec::new();

    for result in reader.records() {
        let record = result?;

        // Parse plateau state (columns 2-20: plateau_0 to plateau_18)
        let mut plateau_state = Vec::with_capacity(19);
        for i in 2..21 {
            let encoded: i32 = record[i].parse()?;
            plateau_state.push(encoded);
        }

        // Parse tile (columns 21-23: tile_0, tile_1, tile_2)
        let tile = (
            record[21].parse()?,
            record[22].parse()?,
            record[23].parse()?,
        );

        // Parse position and final_score
        let position: usize = record[24].parse()?;
        let final_score: i32 = record[25].parse()?;

        examples.push(TrainingExample {
            plateau_state,
            tile,
            position,
            final_score,
        });
    }

    Ok(examples)
}

fn train_epoch(
    examples: &[TrainingExample],
    manager: &mut NeuralManager,
    batch_size: usize,
    device: Device,
) -> Result<(f64, f64), Box<dyn Error>> {
    let mut total_policy_loss = 0.0;
    let mut total_value_loss = 0.0;
    let mut num_batches = 0;

    for batch in examples.chunks(batch_size) {
        let (state_tensors, policy_targets, value_targets) = prepare_batch(batch, device)?;

        // Train policy network
        let policy_net = manager.policy_net();
        let policy_pred = policy_net.forward(&state_tensors, true);
        let policy_loss = policy_pred.cross_entropy_for_logits(&policy_targets);

        let policy_opt = manager.policy_optimizer_mut();
        policy_opt.backward_step(&policy_loss);
        total_policy_loss += f64::try_from(&policy_loss)?;

        // Train value network
        let value_net = manager.value_net();
        let value_pred = value_net.forward(&state_tensors, true);
        let value_loss = value_pred.mse_loss(&value_targets, tch::Reduction::Mean);

        let value_opt = manager.value_optimizer_mut();
        value_opt.backward_step(&value_loss);
        total_value_loss += f64::try_from(&value_loss)?;

        num_batches += 1;
    }

    Ok((
        total_policy_loss / num_batches as f64,
        total_value_loss / num_batches as f64,
    ))
}

fn validate_epoch(
    examples: &[TrainingExample],
    manager: &NeuralManager,
    batch_size: usize,
    device: Device,
) -> Result<(f64, f64), Box<dyn Error>> {
    let mut total_policy_loss = 0.0;
    let mut total_value_loss = 0.0;
    let mut num_batches = 0;

    tch::no_grad(|| {
        for batch in examples.chunks(batch_size) {
            let (state_tensors, policy_targets, value_targets) = prepare_batch(batch, device)?;

            // Validate policy
            let policy_net = manager.policy_net();
            let policy_pred = policy_net.forward(&state_tensors, false);
            let policy_loss = policy_pred.cross_entropy_for_logits(&policy_targets);
            total_policy_loss += f64::try_from(&policy_loss)?;

            // Validate value
            let value_net = manager.value_net();
            let value_pred = value_net.forward(&state_tensors, false);
            let value_loss = value_pred.mse_loss(&value_targets, tch::Reduction::Mean);
            total_value_loss += f64::try_from(&value_loss)?;

            num_batches += 1;
        }
        Ok((
            total_policy_loss / num_batches as f64,
            total_value_loss / num_batches as f64,
        ))
    })
}

fn prepare_batch(
    examples: &[TrainingExample],
    device: Device,
) -> Result<(Tensor, Tensor, Tensor), Box<dyn Error>> {
    let batch_size = examples.len();
    let input_channels = 8i64;

    // Prepare tensors
    let mut states = vec![0.0f32; batch_size * 8 * 5 * 5];
    let mut policy_targets = vec![0i64; batch_size];
    let mut value_targets = vec![0.0f32; batch_size];

    for (i, example) in examples.iter().enumerate() {
        // Encode state
        let state = encode_state(&example.plateau_state, &example.tile);
        let offset = i * 8 * 5 * 5;
        states[offset..offset + 8 * 5 * 5].copy_from_slice(&state);

        // Policy target: position
        policy_targets[i] = example.position as i64;

        // Value target: normalize final_score to [0, 1] range
        // Scores typically range from 0-180, normalize to approximately [0, 1]
        value_targets[i] = (example.final_score as f32) / 180.0;
    }

    let state_tensor = Tensor::from_slice(&states)
        .view([batch_size as i64, input_channels, 5, 5])
        .to_device(device);

    let policy_tensor = Tensor::from_slice(&policy_targets).to_device(device);

    let value_tensor = Tensor::from_slice(&value_targets)
        .view([batch_size as i64, 1])
        .to_device(device);

    Ok((state_tensor, policy_tensor, value_tensor))
}

fn encode_state(plateau: &[i32], tile: &(i32, i32, i32)) -> Vec<f32> {
    // 8 channels: 3 for placed tiles, 1 for empty mask, 3 for current tile, 1 for turn progress
    let mut state = vec![0.0f32; 8 * 5 * 5];

    let num_placed = plateau.iter().filter(|&&x| x != 0).count();
    let turn_progress = num_placed as f32 / 19.0;

    for (pos, &encoded) in plateau.iter().enumerate() {
        let grid_idx = pos; // Direct mapping for 5x5 (19 positions)

        if encoded == 0 {
            // Empty cell
            state[3 * 25 + grid_idx] = 1.0;
        } else {
            // Decode: encoded = v1*100 + v2*10 + v3
            let v1 = (encoded / 100) as f32 / 9.0;
            let v2 = ((encoded % 100) / 10) as f32 / 9.0;
            let v3 = (encoded % 10) as f32 / 9.0;

            state[grid_idx] = v1;
            state[25 + grid_idx] = v2;
            state[2 * 25 + grid_idx] = v3;
        }

        // Current tile (broadcast to all positions)
        state[4 * 25 + grid_idx] = tile.0 as f32 / 9.0;
        state[5 * 25 + grid_idx] = tile.1 as f32 / 9.0;
        state[6 * 25 + grid_idx] = tile.2 as f32 / 9.0;

        // Turn progress
        state[7 * 25 + grid_idx] = turn_progress;
    }

    state
}
