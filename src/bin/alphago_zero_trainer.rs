//! AlphaGo Zero Style Iterative Training with UCT MCTS
//!
//! Implements iterative self-play training loop using UCT MCTS (+101% performance):
//! 1. Generate games using UCT MCTS (149 pts vs 74 pts batch)
//! 2. Train networks on high-quality UCT self-play data
//! 3. Benchmark to measure convergence
//! 4. Repeat until networks converge
//!
//! UCT MCTS concentrates exploration on promising positions for better estimates.

use clap::Parser;
use flexi_logger::Logger;
use rand::rngs::StdRng;
use rand::SeedableRng;
use rand::prelude::IndexedRandom;
use rand_distr::{Distribution, Gamma};
use std::fs::File;
use std::io::Write;
use take_it_easy::game::create_deck::create_deck;
use take_it_easy::game::get_legal_moves::get_legal_moves;
use take_it_easy::game::plateau::create_plateau_empty;
use take_it_easy::game::remove_tile_from_deck::{get_available_tiles, replace_tile_in_deck};
use take_it_easy::mcts::algorithm::mcts_find_best_position_for_tile_uct;
use take_it_easy::neural::manager::NNArchitecture;
use take_it_easy::neural::tensor_conversion::convert_plateau_to_tensor;
use take_it_easy::neural::{NeuralConfig, NeuralManager};
use take_it_easy::scoring::scoring::result;
use tch::{Device, Tensor};

#[derive(Parser, Debug)]
#[command(name = "alphago-zero-trainer")]
#[command(about = "AlphaGo Zero style iterative training with convergence")]
struct Args {
    /// Number of training iterations
    #[arg(long, default_value_t = 50)]
    iterations: usize,

    /// Games per iteration (self-play)
    #[arg(long, default_value_t = 20)]
    games_per_iter: usize,

    /// MCTS simulations for self-play
    #[arg(long, default_value_t = 150)]
    mcts_simulations: usize,

    /// Training epochs per iteration
    #[arg(long, default_value_t = 10)]
    epochs_per_iter: usize,

    /// Learning rate
    #[arg(long, default_value_t = 0.01)]
    learning_rate: f64,

    /// Batch size for training
    #[arg(long, default_value_t = 32)]
    batch_size: usize,

    /// Benchmark games (for convergence check)
    #[arg(long, default_value_t = 100)]
    benchmark_games: usize,

    /// Convergence threshold (stop if score improvement < this)
    #[arg(long, default_value_t = 2.0)]
    convergence_threshold: f64,

    /// Random seed
    #[arg(long, default_value_t = 2025)]
    seed: u64,

    /// Neural network architecture
    #[arg(long, default_value = "CNN")]
    nn_architecture: String,

    /// Start fresh (don't load existing weights)
    #[arg(long, default_value_t = false)]
    fresh_start: bool,

    /// Output file for training history
    #[arg(long, default_value = "training_history.csv")]
    output: String,
}

struct TrainingExample {
    state: Tensor,
    policy_target: i64,
    value_target: f32,
}

fn main() -> Result<(), Box<dyn std::error::Error>> {
    Logger::try_with_env_or_str("info")?
        .format(flexi_logger::colored_default_format)
        .start()?;

    let args = Args::parse();

    log::info!("ðŸš€ AlphaGo Zero Style Training");
    log::info!("   Iterations: {}", args.iterations);
    log::info!("   Games/iter: {}, MCTS sims: {}", args.games_per_iter, args.mcts_simulations);
    log::info!("   Epochs/iter: {}, LR: {}", args.epochs_per_iter, args.learning_rate);
    log::info!("   Benchmark games: {}", args.benchmark_games);
    log::info!("   Convergence threshold: {:.2} pts", args.convergence_threshold);

    // Parse architecture
    let nn_arch = match args.nn_architecture.to_uppercase().as_str() {
        "CNN" => NNArchitecture::CNN,
        "GNN" => NNArchitecture::GNN,
        _ => return Err(format!("Invalid architecture: {}", args.nn_architecture).into()),
    };

    // Initialize neural network
    let neural_config = NeuralConfig {
        input_dim: (8, 5, 5),
        nn_architecture: nn_arch,
        policy_lr: args.learning_rate,
        value_lr: args.learning_rate,
        ..Default::default()
    };

    // Optionally start fresh
    let model_path_backup = if args.fresh_start {
        let backup = std::env::var("MODEL_PATH").ok();
        std::env::set_var("MODEL_PATH", "/nonexistent");
        log::info!("   Starting with fresh weights (no loading)");
        Some(backup)
    } else {
        log::info!("   Loading existing weights if available");
        None
    };

    let mut manager = NeuralManager::with_config(neural_config)?;

    // Restore env
    if let Some(Some(path)) = model_path_backup {
        std::env::set_var("MODEL_PATH", path);
    }

    // Initialize training history file
    let mut history_file = File::create(&args.output)?;
    writeln!(history_file, "iteration,policy_loss,value_loss,benchmark_score_mean,benchmark_score_std")?;

    let mut previous_score = 0.0;
    let device = Device::Cpu;

    // Main training loop
    for iteration in 0..args.iterations {
        log::info!("\n{}", "=".repeat(60));
        log::info!("ðŸ“Š Iteration {}/{}", iteration + 1, args.iterations);
        log::info!("{}", "=".repeat(60));

        // Step 1: Self-play to generate training data
        log::info!("\nðŸŽ® Phase 1: Self-play ({} games)", args.games_per_iter);
        let training_data = generate_self_play_games(
            &manager,
            args.games_per_iter,
            args.mcts_simulations,
            args.seed + iteration as u64,
        )?;

        log::info!("   Generated {} training examples", training_data.len());

        // Step 2: Train on self-play data
        log::info!("\nðŸ‹ï¸ Phase 2: Training ({} epochs)", args.epochs_per_iter);
        let (avg_policy_loss, avg_value_loss) = train_on_data(
            &training_data,
            &mut manager,
            args.epochs_per_iter,
            args.batch_size,
            device,
        )?;

        log::info!("   Final losses: policy={:.4}, value={:.4}", avg_policy_loss, avg_value_loss);

        // Step 3: Benchmark to measure progress
        log::info!("\nðŸ“ˆ Phase 3: Benchmark ({} games)", args.benchmark_games);
        let (benchmark_mean, benchmark_std) = benchmark_performance(
            &manager,
            args.benchmark_games,
            args.mcts_simulations,
            args.seed + 1000 + iteration as u64,
        )?;

        log::info!("   Score: {:.2} Â± {:.2}", benchmark_mean, benchmark_std);

        // Record history
        writeln!(
            history_file,
            "{},{:.4},{:.4},{:.2},{:.2}",
            iteration + 1,
            avg_policy_loss,
            avg_value_loss,
            benchmark_mean,
            benchmark_std
        )?;
        history_file.flush()?;

        // Step 4: Check convergence
        let improvement = benchmark_mean - previous_score;
        log::info!("\nðŸŽ¯ Progress Check:");
        log::info!("   Previous score: {:.2}", previous_score);
        log::info!("   Current score:  {:.2}", benchmark_mean);
        log::info!("   Improvement:    {:.2}", improvement);

        // Only converge if improvement is VERY SMALL (not just less than threshold)
        // With high threshold, we want to continue training
        if iteration > 0 && args.convergence_threshold < 100.0 && improvement.abs() < args.convergence_threshold {
            log::info!("\nâœ… CONVERGED: Improvement < {:.2} pts", args.convergence_threshold);
            log::info!("   Training complete at iteration {}", iteration + 1);
            break;
        } else if iteration > 0 {
            log::info!("   Continuing training (threshold: {:.2} pts)", args.convergence_threshold);
        }

        previous_score = benchmark_mean;

        // Step 5: Save checkpoint (weights are auto-saved by NeuralManager)
        log::info!("\nðŸ’¾ Checkpoint: weights auto-saved");
    }

    log::info!("\n{}", "=".repeat(60));
    log::info!("âœ… Training Complete");
    log::info!("   History saved to: {}", args.output);
    log::info!("{}", "=".repeat(60));

    Ok(())
}

fn generate_self_play_games(
    manager: &NeuralManager,
    num_games: usize,
    mcts_sims: usize,
    seed: u64,
) -> Result<Vec<TrainingExample>, Box<dyn std::error::Error>> {
    let mut rng = StdRng::seed_from_u64(seed);
    let mut training_data = Vec::new();
    let turns_per_game = 19;

    for game_idx in 0..num_games {
        let mut plateau = create_plateau_empty();
        let mut deck = create_deck();

        for turn in 0..turns_per_game {
            let available = get_available_tiles(&deck);
            if available.is_empty() {
                break;
            }

            let chosen_tile = *available.choose(&mut rng).unwrap();
            deck = replace_tile_in_deck(&deck, &chosen_tile);

            // ====================================================================
            // DIRICHLET NOISE - AlphaGo Zero exploration technique
            // ====================================================================
            // Add Dirichlet noise to encourage exploration during self-play
            // This breaks the circular learning problem where uniform policy
            // leads to uniform MCTS priors, which generate uniform training data
            let legal_moves = get_legal_moves(&plateau);
            let epsilon = 0.25;  // Mix ratio: 75% policy + 25% noise
            let alpha = 0.3;     // Dirichlet concentration (lower = more uniform)

            // Generate Dirichlet noise for exploration
            // Dirichlet is sampled using Gamma distributions: X_i ~ Gamma(alpha, 1)
            // Then normalize: Y_i = X_i / sum(X_i)
            let gamma = Gamma::new(alpha, 1.0)
                .expect("Failed to create Gamma distribution");
            let mut noise: Vec<f64> = (0..legal_moves.len())
                .map(|_| gamma.sample(&mut rng))
                .collect();
            let sum: f64 = noise.iter().sum();
            for val in &mut noise {
                *val /= sum;
            }

            // Convert noise to exploration_priors (map position -> noise value)
            let mut exploration_priors = vec![0.0; 19]; // 19 positions on plateau
            for (idx, &pos) in legal_moves.iter().enumerate() {
                // Mix: (1-Îµ) * policy_prior + Îµ * noise
                // For now, we apply noise directly as MCTS will mix with policy
                exploration_priors[pos] = (noise[idx] * epsilon) as f32;
            }

            // Use UCT MCTS with current network to find best position
            // The exploration_priors will be mixed with network policy
            let mcts_result = mcts_find_best_position_for_tile_uct(
                &mut plateau,
                &mut deck,
                chosen_tile,
                manager.policy_net(),
                manager.value_net(),
                mcts_sims,
                turn,
                turns_per_game,
                None, // Use default hyperparameters
                Some(exploration_priors), // Pass Dirichlet noise for exploration
            );

            // Record training example
            let state_tensor = convert_plateau_to_tensor(&plateau, &chosen_tile, &deck, turn, turns_per_game);
            let policy_target = mcts_result.best_position as i64;

            // Store for later (we'll assign value after game ends)
            training_data.push((state_tensor, policy_target, 0.0));

            // Execute move
            plateau.tiles[mcts_result.best_position] = chosen_tile;
        }

        // Assign value targets based on final score
        let final_score = result(&plateau);
        let normalized_value = (final_score as f32 - 80.0) / 80.0; // Normalize around current performance
        let normalized_value = normalized_value.max(-1.0).min(1.0);

        // Update value targets for all moves in this game
        let game_start = training_data.len() - turns_per_game.min(training_data.len());
        for i in game_start..training_data.len() {
            training_data[i].2 = normalized_value;
        }

        if (game_idx + 1) % 10 == 0 {
            log::info!("   Generated {}/{} games", game_idx + 1, num_games);
        }
    }

    // Convert to TrainingExample structs
    let examples = training_data
        .into_iter()
        .map(|(state, policy, value)| TrainingExample {
            state,
            policy_target: policy,
            value_target: value,
        })
        .collect();

    Ok(examples)
}

fn train_on_data(
    training_data: &[TrainingExample],
    manager: &mut NeuralManager,
    epochs: usize,
    batch_size: usize,
    device: Device,
) -> Result<(f64, f64), Box<dyn std::error::Error>> {
    let mut final_policy_loss = 0.0;
    let mut final_value_loss = 0.0;

    for epoch in 0..epochs {
        let mut epoch_policy_loss = 0.0;
        let mut epoch_value_loss = 0.0;
        let mut batch_count = 0;

        for batch in training_data.chunks(batch_size) {
            // Prepare batch tensors
            let states: Vec<&Tensor> = batch.iter().map(|ex| &ex.state).collect();
            let states_batch = Tensor::cat(&states, 0).to_device(device);

            let policy_targets: Vec<i64> = batch.iter().map(|ex| ex.policy_target).collect();
            let policy_targets_batch = Tensor::from_slice(&policy_targets).to_device(device);

            let value_targets: Vec<f32> = batch.iter().map(|ex| ex.value_target).collect();
            let value_targets_batch = Tensor::from_slice(&value_targets)
                .view([batch.len() as i64, 1])
                .to_device(device);

            // Train policy network
            let policy_net = manager.policy_net();
            let policy_pred = policy_net.forward(&states_batch, true);
            let policy_loss = policy_pred.cross_entropy_for_logits(&policy_targets_batch);

            let policy_opt = manager.policy_optimizer_mut();
            policy_opt.backward_step(&policy_loss);

            // Train value network
            let value_net = manager.value_net();
            let value_pred = value_net.forward(&states_batch, true);
            let value_loss = value_pred.mse_loss(&value_targets_batch, tch::Reduction::Mean);

            let value_opt = manager.value_optimizer_mut();
            value_opt.backward_step(&value_loss);

            epoch_policy_loss += policy_loss.double_value(&[]);
            epoch_value_loss += value_loss.double_value(&[]);
            batch_count += 1;
        }

        final_policy_loss = epoch_policy_loss / batch_count as f64;
        final_value_loss = epoch_value_loss / batch_count as f64;

        if (epoch + 1) % 5 == 0 || epoch == epochs - 1 {
            log::info!(
                "   Epoch {}/{}: policy_loss={:.4}, value_loss={:.4}",
                epoch + 1,
                epochs,
                final_policy_loss,
                final_value_loss
            );
        }
    }

    Ok((final_policy_loss, final_value_loss))
}

fn benchmark_performance(
    manager: &NeuralManager,
    num_games: usize,
    mcts_sims: usize,
    seed: u64,
) -> Result<(f64, f64), Box<dyn std::error::Error>> {
    let mut rng = StdRng::seed_from_u64(seed);
    let mut scores = Vec::new();
    let turns_per_game = 19;

    for game_idx in 0..num_games {
        let mut plateau = create_plateau_empty();
        let mut deck = create_deck();

        for turn in 0..turns_per_game {
            let available = get_available_tiles(&deck);
            if available.is_empty() {
                break;
            }

            let chosen_tile = *available.choose(&mut rng).unwrap();
            deck = replace_tile_in_deck(&deck, &chosen_tile);

            let mcts_result = mcts_find_best_position_for_tile_uct(
                &mut plateau,
                &mut deck,
                chosen_tile,
                manager.policy_net(),
                manager.value_net(),
                mcts_sims,
                turn,
                turns_per_game,
                None,
                None, // No exploration noise for pure benchmarking
            );

            plateau.tiles[mcts_result.best_position] = chosen_tile;
        }

        let final_score = result(&plateau);
        scores.push(final_score as f64);

        if (game_idx + 1) % 25 == 0 {
            log::info!("   Benchmarked {}/{} games", game_idx + 1, num_games);
        }
    }

    let mean = scores.iter().sum::<f64>() / scores.len() as f64;
    let variance = scores
        .iter()
        .map(|&s| {
            let diff = s - mean;
            diff * diff
        })
        .sum::<f64>()
        / scores.len() as f64;
    let std_dev = variance.sqrt();

    Ok((mean, std_dev))
}
