//! Supervised Trainer - Learns from pre-generated quality data
//!
//! Loads training data generated by alphago_zero_trainer --save-data-path
//! and performs supervised learning to improve the network.

use clap::Parser;
use flexi_logger::Logger;
use std::error::Error;
use tch::{Device, Kind, Reduction, Tensor};

use rand::prelude::*;
use take_it_easy::data::load_data::load_game_data_with_arch;
use take_it_easy::game::create_deck::create_deck;
use take_it_easy::game::plateau::create_plateau_empty;
use take_it_easy::game::remove_tile_from_deck::get_available_tiles;
use take_it_easy::mcts::algorithm::mcts_find_best_position_for_tile_uct;
use take_it_easy::neural::manager::NNArchitecture;
use take_it_easy::neural::{NeuralConfig, NeuralManager};
use take_it_easy::scoring::scoring::result;

#[derive(Parser, Debug)]
#[command(name = "supervised-quality-trainer")]
#[command(about = "Train neural network from quality game data")]
struct Args {
    /// Path to training data (without arch suffix, e.g. 'quality_data_baseline_0')
    #[arg(long)]
    data_path: String,

    /// Number of training epochs
    #[arg(long, default_value_t = 50)]
    epochs: usize,

    /// Batch size
    #[arg(long, default_value_t = 64)]
    batch_size: usize,

    /// Learning rate
    #[arg(long, default_value_t = 0.001)]
    learning_rate: f64,

    /// Neural network architecture (CNN or GNN)
    #[arg(long, default_value = "CNN")]
    nn_architecture: String,

    /// Number of benchmark games after training
    #[arg(long, default_value_t = 100)]
    benchmark_games: usize,
}

fn main() -> Result<(), Box<dyn Error>> {
    Logger::try_with_env_or_str("info")?
        .format(flexi_logger::colored_default_format)
        .start()?;

    let args = Args::parse();

    log::info!("ðŸ“š Supervised Learning from Quality Data");
    log::info!("   Data: {}", args.data_path);
    log::info!("   Epochs: {}, LR: {}", args.epochs, args.learning_rate);

    // Parse architecture
    let nn_arch = match args.nn_architecture.to_uppercase().as_str() {
        "CNN" => NNArchitecture::Cnn,
        "GNN" => NNArchitecture::Gnn,
        _ => return Err(format!("Invalid architecture: {}", args.nn_architecture).into()),
    };

    // Load training data
    log::info!("\nðŸ“‚ Loading training data...");
    let training_data = load_game_data_with_arch(&args.data_path, nn_arch);

    if training_data.is_empty() {
        return Err("No training data loaded. Check file paths.".into());
    }

    log::info!("âœ… Loaded {} training examples", training_data.len());

    // Initialize neural network
    let device = Device::cuda_if_available();
    let neural_config = NeuralConfig {
        input_dim: (9, 5, 5),
        nn_architecture: nn_arch,
        policy_lr: args.learning_rate,
        value_lr: args.learning_rate,
        ..Default::default()
    };

    let mut manager = NeuralManager::with_config(neural_config)?;

    // Benchmark before training
    log::info!(
        "\nðŸ“Š Benchmark BEFORE training ({} games)...",
        args.benchmark_games
    );
    let score_before = benchmark(&manager, args.benchmark_games);
    log::info!("   Score: {:.2} pts", score_before);

    // Train
    log::info!("\nðŸ‹ï¸ Training for {} epochs...", args.epochs);
    train_supervised(
        &mut manager,
        &training_data,
        args.epochs,
        args.batch_size,
        device,
    )?;

    // Benchmark after training
    log::info!(
        "\nðŸ“Š Benchmark AFTER training ({} games)...",
        args.benchmark_games
    );
    let score_after = benchmark(&manager, args.benchmark_games);
    log::info!("   Score: {:.2} pts", score_after);

    // Summary
    log::info!("\n{}", "=".repeat(60));
    log::info!("âœ… Training Complete");
    log::info!("   Before: {:.2} pts", score_before);
    log::info!("   After:  {:.2} pts", score_after);
    log::info!(
        "   Change: {:+.2} pts ({:+.1}%)",
        score_after - score_before,
        ((score_after - score_before) / score_before) * 100.0
    );
    log::info!("{}", "=".repeat(60));

    Ok(())
}

fn train_supervised(
    manager: &mut NeuralManager,
    data: &[take_it_easy::mcts::mcts_result::MCTSResult],
    epochs: usize,
    batch_size: usize,
    device: Device,
) -> Result<(), Box<dyn Error>> {
    let num_examples = data.len();

    for epoch in 0..epochs {
        let mut total_policy_loss = 0.0;
        let mut total_value_loss = 0.0;
        let mut num_batches = 0;

        // Shuffle indices
        let mut indices: Vec<usize> = (0..num_examples).collect();
        indices.shuffle(&mut rand::thread_rng());

        // Process batches
        for batch_start in (0..num_examples).step_by(batch_size) {
            let batch_end = (batch_start + batch_size).min(num_examples);
            let batch_indices = &indices[batch_start..batch_end];

            // Prepare batch
            let states: Vec<Tensor> = batch_indices
                .iter()
                .map(|&i| data[i].board_tensor.shallow_clone())
                .collect();
            let states_batch = Tensor::stack(&states, 0).to(device);

            let policy_targets: Vec<Tensor> = batch_indices
                .iter()
                .map(|&i| data[i].policy_distribution.shallow_clone())
                .collect();
            let policy_targets_batch = Tensor::stack(&policy_targets, 0).to(device);

            let value_targets: Vec<f32> = batch_indices
                .iter()
                .map(|&i| data[i].subscore as f32)
                .collect();
            let value_targets_batch = Tensor::from_slice(&value_targets)
                .to_kind(Kind::Float)
                .to(device);

            // Forward pass and train policy network
            let policy_net = manager.policy_net();
            let policy_pred_logits = policy_net.forward(&states_batch, true);
            let policy_pred_probs = policy_pred_logits.log_softmax(-1, Kind::Float);

            // KL divergence loss for policy
            let policy_loss = -(policy_targets_batch * policy_pred_probs)
                .sum_dim_intlist([-1].as_slice(), false, Kind::Float)
                .mean(Kind::Float);

            let policy_opt = manager.policy_optimizer_mut();
            policy_opt.backward_step(&policy_loss);

            // Forward pass and train value network
            let value_net = manager.value_net();
            let value_pred = value_net.forward(&states_batch, true);
            let value_targets_reshaped = value_targets_batch.view([-1, 1]);
            let value_loss = value_pred.mse_loss(&value_targets_reshaped, Reduction::Mean);

            let value_opt = manager.value_optimizer_mut();
            value_opt.backward_step(&value_loss);

            total_policy_loss += f64::try_from(policy_loss)?;
            total_value_loss += f64::try_from(value_loss)?;
            num_batches += 1;
        }

        let avg_policy_loss = total_policy_loss / num_batches as f64;
        let avg_value_loss = total_value_loss / num_batches as f64;

        if (epoch + 1) % 10 == 0 || epoch == 0 {
            log::info!(
                "   Epoch {}/{}: policy_loss={:.4}, value_loss={:.4}",
                epoch + 1,
                epochs,
                avg_policy_loss,
                avg_value_loss
            );
        }
    }

    Ok(())
}

fn benchmark(manager: &NeuralManager, num_games: usize) -> f64 {
    let mut scores = Vec::new();

    for _ in 0..num_games {
        let mut plateau = create_plateau_empty();
        let mut deck = create_deck();

        for turn in 0..19 {
            let available = get_available_tiles(&deck);
            if available.is_empty() {
                break;
            }

            let tile = *available.choose(&mut rand::thread_rng()).unwrap();

            let mcts_result = mcts_find_best_position_for_tile_uct(
                &mut plateau,
                &mut deck,
                tile,
                manager.policy_net(),
                manager.value_net(),
                150,
                turn,
                19,
                None,
                None,
            );

            plateau.tiles[mcts_result.best_position] = tile;
        }

        scores.push(result(&plateau) as f64);
    }

    scores.iter().sum::<f64>() / scores.len() as f64
}
