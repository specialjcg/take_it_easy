//! Supervised Trainer with Curriculum Learning
//!
//! Trains neural networks on expert data generated by expert_data_generator
//! Supports curriculum learning with progressive difficulty phases

use clap::Parser;
use flexi_logger::Logger;
use serde::{Deserialize, Serialize};
use std::collections::HashMap;
use std::fs::File;
use std::io::BufReader;
use tch::{nn, Device, Tensor};

use take_it_easy::neural::manager::NNArchitecture;
use take_it_easy::neural::{NeuralConfig, NeuralManager};

#[derive(Parser, Debug)]
#[command(
    name = "supervised-trainer",
    about = "Train neural networks with curriculum learning"
)]
struct Args {
    /// JSON file(s) with expert data (comma-separated for curriculum)
    #[arg(short, long)]
    data: String,

    /// Number of training epochs per phase
    #[arg(short, long, default_value_t = 50)]
    epochs: usize,

    /// Batch size for training
    #[arg(short, long, default_value_t = 32)]
    batch_size: usize,

    /// Learning rate
    #[arg(short, long, default_value_t = 0.001)]
    learning_rate: f64,

    /// Checkpoint directory
    #[arg(short, long, default_value = "checkpoints")]
    checkpoint_dir: String,

    /// Neural network architecture (CNN or GNN)
    #[arg(long, default_value = "CNN")]
    nn_architecture: String,

    /// Train policy network
    #[arg(long, default_value_t = true)]
    train_policy: bool,

    /// Train value network
    #[arg(long, default_value_t = true)]
    train_value: bool,

    /// Validation split (0.0-1.0)
    #[arg(long, default_value_t = 0.1)]
    validation_split: f64,
}

#[derive(Serialize, Deserialize, Debug, Clone)]
struct ExpertGame {
    game_id: usize,
    moves: Vec<ExpertMove>,
    final_score: i32,
    seed: u64,
}

#[derive(Serialize, Deserialize, Debug, Clone)]
struct ExpertMove {
    turn: usize,
    plateau_before: Vec<i32>,
    tile: TileData,
    best_position: usize,
    expected_value: f64,
    #[serde(skip_serializing_if = "Option::is_none")]
    policy_distribution: Option<HashMap<usize, f64>>,
}

#[derive(Serialize, Deserialize, Debug, Clone)]
struct TileData {
    value1: i32,
    value2: i32,
    value3: i32,
}

fn main() -> Result<(), Box<dyn std::error::Error>> {
    Logger::try_with_env_or_str("info")?
        .format(flexi_logger::colored_default_format)
        .start()?;

    let args = Args::parse();

    log::info!("üéì Supervised Trainer with Curriculum Learning");
    log::info!("Architecture: {}", args.nn_architecture);
    log::info!("Data files: {}", args.data);
    log::info!("Epochs per phase: {}", args.epochs);
    log::info!("Batch size: {}", args.batch_size);
    log::info!("Learning rate: {}", args.learning_rate);

    // Parse architecture
    let nn_arch = match args.nn_architecture.to_uppercase().as_str() {
        "CNN" => NNArchitecture::CNN,
        "GNN" => NNArchitecture::GNN,
        _ => return Err(format!("Invalid architecture: {}", args.nn_architecture).into()),
    };

    // Initialize neural network
    log::info!("Initializing neural network...");
    let neural_config = NeuralConfig {
        input_dim: (8, 5, 5),
        nn_architecture: nn_arch,
        ..Default::default()
    };
    let mut manager = NeuralManager::with_config(neural_config)?;
    log::info!("‚úÖ Neural network initialized");

    // Load curriculum phases
    let data_files: Vec<&str> = args.data.split(',').collect();
    log::info!("\nüìö Curriculum Learning Pipeline:");
    log::info!("Number of phases: {}", data_files.len());

    for (phase_idx, data_file) in data_files.iter().enumerate() {
        let phase_num = phase_idx + 1;
        log::info!("\n{}", "=".repeat(60));
        log::info!("üìñ Phase {}/{}: {}", phase_num, data_files.len(), data_file);
        log::info!("{}", "=".repeat(60));

        // Load expert data
        log::info!("Loading expert data from {}...", data_file);
        let expert_games = load_expert_data(data_file)?;
        let total_moves: usize = expert_games.iter().map(|g| g.moves.len()).sum();
        log::info!(
            "‚úÖ Loaded {} games with {} total training examples",
            expert_games.len(),
            total_moves
        );

        // Calculate average expert score
        let avg_score: f64 = expert_games
            .iter()
            .map(|g| g.final_score as f64)
            .sum::<f64>()
            / expert_games.len() as f64;
        log::info!("üìä Expert average score: {:.2} pts", avg_score);

        // Split into train/validation
        let split_idx = ((1.0 - args.validation_split) * expert_games.len() as f64) as usize;
        let (train_games, val_games) = expert_games.split_at(split_idx);
        log::info!(
            "Split: {} training games, {} validation games",
            train_games.len(),
            val_games.len()
        );

        // Train on this phase
        train_phase(&mut manager, train_games, val_games, &args, phase_num)?;

        // Save checkpoint after each phase
        let checkpoint_path = format!("{}/phase{}", args.checkpoint_dir, phase_num);
        std::fs::create_dir_all(&checkpoint_path)?;
        manager.save_models()?;
        log::info!(
            "üíæ Phase {} checkpoint saved to {}",
            phase_num,
            checkpoint_path
        );
    }

    log::info!("\nüéâ Training Complete!");
    log::info!(
        "Final weights saved to: {}/phase{}",
        args.checkpoint_dir,
        data_files.len()
    );

    Ok(())
}

fn load_expert_data(path: &str) -> Result<Vec<ExpertGame>, Box<dyn std::error::Error>> {
    let file = File::open(path)?;
    let reader = BufReader::new(file);
    let games: Vec<ExpertGame> = serde_json::from_reader(reader)?;
    Ok(games)
}

fn train_phase(
    _manager: &mut NeuralManager,
    _train_games: &[ExpertGame],
    _val_games: &[ExpertGame],
    args: &Args,
    phase_num: usize,
) -> Result<(), Box<dyn std::error::Error>> {
    log::info!(
        "\nüèãÔ∏è Training Phase {} for {} epochs...",
        phase_num,
        args.epochs
    );

    // TODO: Implement training with the current NeuralManager architecture
    // The current PolicyNet/ValueNet don't expose VarStore for optimizer creation
    log::warn!("‚ö†Ô∏è Training not yet implemented for current architecture");
    log::info!("Skipping training for phase {}", phase_num);
    return Ok(());

    /*
    let device = Device::cuda_if_available();
    let policy_net = manager.policy_net();
    let value_net = manager.value_net();

    // Create optimizers - NEEDS REFACTORING
    // let mut policy_opt = nn::Adam::default().build(&policy_net.vs, args.learning_rate)?;
    // let mut value_opt = nn::Adam::default().build(&value_net.vs, args.learning_rate)?;

    let train_moves: Vec<&ExpertMove> = train_games.iter().flat_map(|g| &g.moves).collect();
    let val_moves: Vec<&ExpertMove> = val_games.iter().flat_map(|g| &g.moves).collect();

    log::info!("Training samples: {}", train_moves.len());
    log::info!("Validation samples: {}", val_moves.len());

    let mut best_val_loss = f64::INFINITY;
    let mut epochs_without_improvement = 0;
    let patience = 10;

    for epoch in 0..args.epochs {
        // Training
        let (policy_loss, value_loss) = train_epoch(
            &train_moves,
            policy_net,
            value_net,
            &mut policy_opt,
            &mut value_opt,
            args.batch_size,
            device,
            args.train_policy,
            args.train_value,
        )?;

        // Validation
        let (val_policy_loss, val_value_loss) = validate_epoch(
            &val_moves,
            policy_net,
            value_net,
            args.batch_size,
            device,
            args.train_policy,
            args.train_value,
        )?;

        let total_val_loss = val_policy_loss + val_value_loss;

        // Log progress every 5 epochs
        if (epoch + 1) % 5 == 0 || epoch == 0 {
            log::info!(
                "Epoch {}/{} | Train: policy={:.4}, value={:.4} | Val: policy={:.4}, value={:.4}",
                epoch + 1,
                args.epochs,
                policy_loss,
                value_loss,
                val_policy_loss,
                val_value_loss
            );
        }

        // Early stopping
        if total_val_loss < best_val_loss {
            best_val_loss = total_val_loss;
            epochs_without_improvement = 0;
        } else {
            epochs_without_improvement += 1;
            if epochs_without_improvement >= patience {
                log::info!(
                    "‚ö†Ô∏è Early stopping at epoch {} (no improvement for {} epochs)",
                    epoch + 1,
                    patience
                );
                break;
            }
        }
    }

    log::info!("‚úÖ Phase {} training complete", phase_num);
    log::info!("Best validation loss: {:.4}", best_val_loss);

    Ok(())
    */
}

#[allow(dead_code)]
fn train_epoch(
    moves: &[&ExpertMove],
    policy_net: &take_it_easy::neural::policy_value_net::PolicyNet,
    value_net: &take_it_easy::neural::policy_value_net::ValueNet,
    policy_opt: &mut nn::Optimizer,
    value_opt: &mut nn::Optimizer,
    batch_size: usize,
    device: Device,
    train_policy: bool,
    train_value: bool,
) -> Result<(f64, f64), Box<dyn std::error::Error>> {
    let mut total_policy_loss = 0.0;
    let mut total_value_loss = 0.0;
    let mut num_batches = 0;

    for batch_moves in moves.chunks(batch_size) {
        // Prepare batch tensors
        let (state_tensors, policy_targets, value_targets) = prepare_batch(batch_moves, device)?;

        // Train policy network
        let policy_loss = if train_policy {
            let policy_pred = policy_net.forward(&state_tensors, true);
            let loss = policy_pred.cross_entropy_for_logits(&policy_targets);
            policy_opt.backward_step(&loss);
            loss.double_value(&[])
        } else {
            0.0
        };

        // Train value network
        let value_loss = if train_value {
            let value_pred = value_net.forward(&state_tensors, true);
            let loss = value_pred.mse_loss(&value_targets, tch::Reduction::Mean);
            value_opt.backward_step(&loss);
            loss.double_value(&[])
        } else {
            0.0
        };

        total_policy_loss += policy_loss;
        total_value_loss += value_loss;
        num_batches += 1;
    }

    Ok((
        total_policy_loss / num_batches as f64,
        total_value_loss / num_batches as f64,
    ))
}

#[allow(dead_code)]
fn validate_epoch(
    moves: &[&ExpertMove],
    policy_net: &take_it_easy::neural::policy_value_net::PolicyNet,
    value_net: &take_it_easy::neural::policy_value_net::ValueNet,
    batch_size: usize,
    device: Device,
    validate_policy: bool,
    validate_value: bool,
) -> Result<(f64, f64), Box<dyn std::error::Error>> {
    let mut total_policy_loss = 0.0;
    let mut total_value_loss = 0.0;
    let mut num_batches = 0;

    tch::no_grad(|| {
        for batch_moves in moves.chunks(batch_size) {
            let (state_tensors, policy_targets, value_targets) =
                prepare_batch(batch_moves, device)?;

            if validate_policy {
                let policy_pred = policy_net.forward(&state_tensors, false);
                let loss = policy_pred.cross_entropy_for_logits(&policy_targets);
                total_policy_loss += loss.double_value(&[]);
            }

            if validate_value {
                let value_pred = value_net.forward(&state_tensors, false);
                let loss = value_pred.mse_loss(&value_targets, tch::Reduction::Mean);
                total_value_loss += loss.double_value(&[]);
            }

            num_batches += 1;
        }
        Ok((
            total_policy_loss / num_batches as f64,
            total_value_loss / num_batches as f64,
        ))
    })
}

#[allow(dead_code)]
fn prepare_batch(
    moves: &[&ExpertMove],
    device: Device,
) -> Result<(Tensor, Tensor, Tensor), Box<dyn std::error::Error>> {
    let batch_size = moves.len();

    // Create state tensors (8 channels √ó 5 √ó 5)
    let mut states = vec![0.0f32; batch_size * 8 * 5 * 5];
    let mut policy_targets = vec![0i64; batch_size];
    let mut value_targets = vec![0.0f32; batch_size];

    for (i, expert_move) in moves.iter().enumerate() {
        // Encode plateau state into 8 channels
        let state = encode_state(&expert_move.plateau_before, &expert_move.tile);
        let offset = i * 8 * 5 * 5;
        states[offset..offset + 8 * 5 * 5].copy_from_slice(&state);

        // Policy target: best position
        policy_targets[i] = expert_move.best_position as i64;

        // Value target: normalized expected value
        value_targets[i] = (expert_move.expected_value / 180.0) as f32; // Normalize to ~[0,1]
    }

    let state_tensor = Tensor::from_slice(&states)
        .view([batch_size as i64, 8, 5, 5])
        .to_device(device);

    let policy_tensor = Tensor::from_slice(&policy_targets).to_device(device);

    let value_tensor = Tensor::from_slice(&value_targets)
        .view([batch_size as i64, 1])
        .to_device(device);

    Ok((state_tensor, policy_tensor, value_tensor))
}

#[allow(dead_code)]
fn encode_state(plateau_before: &[i32], tile: &TileData) -> Vec<f32> {
    // 8 channels encoding (same as game state encoding)
    let mut state = vec![0.0f32; 8 * 5 * 5];

    // Channel 0: Tile value1 presence
    // Channel 1: Tile value2 presence
    // Channel 2: Tile value3 presence
    // Channel 3: Empty cells mask
    // Channel 4: Current tile value1
    // Channel 5: Current tile value2
    // Channel 6: Current tile value3
    // Channel 7: Turn progress (all cells same value)

    let num_placed = plateau_before.iter().filter(|&&x| x != -1).count();
    let turn_progress = num_placed as f32 / 19.0;

    for (cell_idx, &encoded_tile) in plateau_before.iter().enumerate() {
        // Map linear index to 5√ó5 grid coordinates
        let row = cell_idx / 5;
        let col = cell_idx % 5;
        let grid_idx = row * 5 + col;

        if encoded_tile == -1 {
            // Empty cell
            state[3 * 25 + grid_idx] = 1.0;
        } else {
            // Decode tile values
            let v1 = (encoded_tile / 100) as f32 / 9.0; // Normalize to [0,1]
            let v2 = ((encoded_tile % 100) / 10) as f32 / 9.0;
            let v3 = (encoded_tile % 10) as f32 / 9.0;

            state[0 * 25 + grid_idx] = v1;
            state[1 * 25 + grid_idx] = v2;
            state[2 * 25 + grid_idx] = v3;
        }

        // Current tile (broadcast to all cells)
        state[4 * 25 + grid_idx] = tile.value1 as f32 / 9.0;
        state[5 * 25 + grid_idx] = tile.value2 as f32 / 9.0;
        state[6 * 25 + grid_idx] = tile.value3 as f32 / 9.0;

        // Turn progress
        state[7 * 25 + grid_idx] = turn_progress;
    }

    state
}
