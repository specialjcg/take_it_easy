# ðŸš€ START HERE - Prochaine Session Claude

**Date:** 2025-12-29
**Status:** UCT MCTS implÃ©mentÃ©, Self-Play convergÃ© Ã  146.92 pts

---

## âš¡ Action ImmÃ©diate (5 min setup)

### 1. VÃ©rifier l'Ã©tat actuel
```bash
cd /home/jcgouleau/IdeaProjects/RustProject/take_it_easy
git status
git log -1 --oneline
# Devrait montrer: 1846218 feat(mcts): implement UCT algorithm...
```

### 2. Lire le plan complet
```bash
cat docs/ROADMAP_PERFORMANCE_2025-12-29.md
```

### 3. Performance actuelle
```
âœ… Batch MCTS (baseline): 74.22 pts
âœ… UCT MCTS: 149.36 pts (+101%)
âœ… Self-Play: 146.92 pts (+97%)
ðŸŽ¯ Target: 180-200 pts
ðŸ“Š Gap: +33-53 pts Ã  combler
```

---

## ðŸŽ¯ Prochaine Ã‰tape RecommandÃ©e

### Option A: Test Rapide (2-3h) - RECOMMANDÃ‰
Lancer training Ã©tendu SANS changement de code:

```bash
cargo build --release

./target/release/alphago_zero_trainer \
  --games-per-iter 100 \
  --convergence-threshold 10.0 \
  --iterations 50 \
  --mcts-simulations 200 \
  --epochs-per-iter 15 \
  --output training_history_extended.csv

# Laisser tourner, rÃ©sultats attendus: 155-165 pts
```

### Option B: ImplÃ©menter Dirichlet Noise (4-6h) - MAX IMPACT
C'est la modification la plus impactante (+15-30 pts):

1. Ouvrir `src/bin/alphago_zero_trainer.rs`
2. Aller Ã  la ligne ~248 (fonction `generate_self_play_games`)
3. Ajouter Dirichlet noise AVANT l'appel MCTS (voir ROADMAP section 1.1)
4. Rebuild et tester

RÃ©sultat attendu: policy_loss < 2.80 (vs 2.9444 actuel)

---

## ðŸ“‹ Checklist PrioritÃ©s

### Phase 1: Quick Wins (START HERE)
- [ ] 1.1 Dirichlet Noise (PRIORITÃ‰ #1) â†’ +15-30 pts
- [ ] 1.2 Temperature Sampling â†’ +5-15 pts
- [ ] 1.3 Augmenter Volume Training â†’ +10-20 pts

### Phase 2: Optimisation
- [ ] 2.1 Rollout Count (5â†’15)
- [ ] 2.2 Value Normalization
- [ ] 2.3 Learning Rate Schedule

### Phase 3: Architecture
- [ ] 3.1 Tester GNN
- [ ] 3.2 ResNet Blocks
- [ ] 3.3 Experience Replay

---

## ðŸ“Š MÃ©triques Ã  Surveiller

```bash
# Pendant training:
tail -f training_history_extended.csv

# Chercher:
policy_loss: doit diminuer de 2.9444 â†’ 2.80 â†’ 2.50 â†’ 2.00
value_loss: doit diminuer de 1.50 â†’ 1.00 â†’ 0.70 â†’ 0.50
score: doit augmenter de 147 â†’ 160 â†’ 170 â†’ 180+
```

---

## ðŸ”§ Fichiers ClÃ©s

```
docs/ROADMAP_PERFORMANCE_2025-12-29.md  â† Plan complet dÃ©taillÃ©
src/bin/alphago_zero_trainer.rs         â† Main training loop
src/mcts/algorithm.rs                   â† UCT implementation
training_history.csv                    â† RÃ©sultats actuels
```

---

## ðŸ’¡ Contexte Rapide

**ProblÃ¨me identifiÃ©:** Circular Learning
- Policy uniforme â†’ UCT uniforme â†’ DonnÃ©es uniformes â†’ Policy uniforme

**Solution:** Dirichlet Noise (Phase 1.1)
- Force l'exploration mÃªme avec policy uniforme
- Technique clÃ© d'AlphaGo Zero

**Commit actuel:** 1846218
- UCT MCTS: +101% performance
- Self-play infrastructure complÃ¨te
- PrÃªt pour amÃ©lioration

---

## ðŸš¨ Quick Commands

```bash
# Build
cargo build --release

# Training rapide (2h)
./target/release/alphago_zero_trainer --games-per-iter 50 --iterations 20

# Training long (overnight)
./target/release/alphago_zero_trainer --games-per-iter 100 --iterations 50 --convergence-threshold 10.0

# Benchmark
./target/release/compare_batch_vs_uct
```

---

**Recommandation:** Commencer par Option A (test rapide) pour Ã©tablir nouvelle baseline, puis passer Ã  Option B (Dirichlet) pour maximiser gains.
