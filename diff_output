diff --git a/src/data/append_result.rs b/src/data/append_result.rs
index e69de29..5e0a05b 100644
--- a/src/data/append_result.rs
+++ b/src/data/append_result.rs
@@ -0,0 +1,23 @@
+use std::fs::OpenOptions;
+use std::io::{BufWriter, Write};
+use chrono::Utc;
+use futures_util::stream::SplitSink;
+use futures_util::StreamExt;
+use tokio::net::TcpListener;
+use tokio_tungstenite::tungstenite::Message;
+use tokio_tungstenite::{accept_async, WebSocketStream};
+
+pub fn append_to_results_file(file_path: &str, avg_score: f64) {
+    let timestamp = Utc::now().to_rfc3339();
+    let result_line = format!("{},{:.2}\n", timestamp, avg_score);
+
+    let file = OpenOptions::new()
+        .create(true)
+        .append(true)
+        .open(file_path)
+        .expect("Unable to open results file");
+    let mut writer = BufWriter::new(file);
+    writer
+        .write_all(result_line.as_bytes())
+        .expect("Unable to write to results file");
+}
diff --git a/src/data/load_data.rs b/src/data/load_data.rs
index e69de29..b9b6cc9 100644
--- a/src/data/load_data.rs
+++ b/src/data/load_data.rs
@@ -0,0 +1,52 @@
+use std::path::Path;
+use tch::Tensor;
+use crate::mcts::mcts_result::MCTSResult;
+
+pub fn load_game_data(file_path: &str) -> Vec<MCTSResult> {
+    // Paths for the .pt files
+    let states_path = format!("{}_states.pt", file_path);
+    let positions_path = format!("{}_positions.pt", file_path);
+    let subscores_path = format!("{}_subscores.pt", file_path);
+
+    // Check if all files exist
+    if !Path::new(&states_path).exists() {
+        println!(
+            "‚ö†Ô∏è  Warning: '{}' not found. Returning empty dataset.",
+            states_path
+        );
+        return Vec::new();
+    }
+    if !Path::new(&positions_path).exists() {
+        println!(
+            "‚ö†Ô∏è  Warning: '{}' not found. Returning empty dataset.",
+            positions_path
+        );
+        return Vec::new();
+    }
+    if !Path::new(&subscores_path).exists() {
+        println!(
+            "‚ö†Ô∏è  Warning: '{}' not found. Returning empty dataset.",
+            subscores_path
+        );
+        return Vec::new();
+    }
+
+    println!("üöÄ Loading game data from .pt files...");
+
+    // Load the saved tensors
+    let state_tensor = Tensor::load(states_path).expect("Failed to load states");
+    let position_tensor = Tensor::load(positions_path).expect("Failed to load positions");
+    let subscore_tensor = Tensor::load(subscores_path).expect("Failed to load subscores");
+
+    // Convert them back into MCTSResult objects
+    let mut data = Vec::new();
+    for i in 0..state_tensor.size()[0] {
+        data.push(MCTSResult {
+            board_tensor: state_tensor.get(i),
+            best_position: position_tensor.get(i).int64_value(&[]) as usize,
+            subscore: subscore_tensor.get(i).double_value(&[]),
+        });
+    }
+    println!("‚úÖ Loaded {} game records.", data.len());
+    data
+}
\ No newline at end of file
diff --git a/src/data/mod.rs b/src/data/mod.rs
index e69de29..85c648b 100644
--- a/src/data/mod.rs
+++ b/src/data/mod.rs
@@ -0,0 +1,3 @@
+pub mod load_data;
+pub mod save_data;
+pub mod append_result;
\ No newline at end of file
diff --git a/src/data/save_data.rs b/src/data/save_data.rs
index e69de29..7b2b24f 100644
--- a/src/data/save_data.rs
+++ b/src/data/save_data.rs
@@ -0,0 +1,53 @@
+use tch::Tensor;
+use crate::mcts::mcts_result::MCTSResult;
+
+pub fn save_game_data(file_path: &str, game_data: Vec<MCTSResult>) {
+    println!("üöÄ Saving game data to .pt files...");
+
+    let mut tensors = vec![];
+    let mut positions = vec![];
+    let mut subscores = vec![];
+
+    for result in game_data {
+        tensors.push(result.board_tensor.shallow_clone());
+        positions.push(result.best_position as i64);
+        subscores.push(result.subscore as f32);
+    }
+
+    // Cr√©ation des nouveaux tensors
+    let state_tensor = Tensor::stack(&tensors, 0);
+    let position_tensor = Tensor::from_slice(&positions).view([-1, 1]);
+    let subscore_tensor = Tensor::from_slice(&subscores).view([-1, 1]);
+
+    // üîÑ Append logic: charger les anciens tensors s'ils existent
+    let combined_states = if let Ok(prev) = Tensor::load(format!("{}_states.pt", file_path)) {
+        Tensor::cat(&[prev, state_tensor], 0)
+    } else {
+        state_tensor
+    };
+
+    let combined_positions = if let Ok(prev) = Tensor::load(format!("{}_positions.pt", file_path)) {
+        Tensor::cat(&[prev, position_tensor], 0)
+    } else {
+        position_tensor
+    };
+
+    let combined_subscores = if let Ok(prev) = Tensor::load(format!("{}_subscores.pt", file_path)) {
+        Tensor::cat(&[prev, subscore_tensor], 0)
+    } else {
+        subscore_tensor
+    };
+
+    // üîÑ Sauvegarde des tensors concat√©n√©s
+    if let Err(e) = combined_states.save(format!("{}_states.pt", file_path)) {
+        log::info!("‚ùå Error saving states: {:?}", e);
+    }
+    if let Err(e) = combined_positions.save(format!("{}_positions.pt", file_path)) {
+        log::info!("‚ùå Error saving positions: {:?}", e);
+    }
+    if let Err(e) = combined_subscores.save(format!("{}_subscores.pt", file_path)) {
+        log::info!("‚ùå Error saving subscores: {:?}", e);
+    }
+
+    log::info!("‚úÖ Save complete!");
+}
diff --git a/src/game/create_deck.rs b/src/game/create_deck.rs
index 5ea7b53..3a7c7f7 100644
--- a/src/game/create_deck.rs
+++ b/src/game/create_deck.rs
@@ -1,7 +1,7 @@
 use crate::game::deck::Deck;
 use crate::game::tile::Tile;
 
-pub fn create_shuffle_deck() -> Deck {
+pub fn create_deck() -> Deck {
     let tiles = vec![
         new_tiles(1, 2, 3),
         new_tiles(1, 6, 8),
@@ -40,13 +40,14 @@ pub(crate) fn new_tiles(x: i32, y: i32, z: i32) -> Tile {
 }
 #[cfg(test)]
 mod tests {
-    use crate::create_deck;
+    use crate::game::create_deck;
+    use crate::game::create_deck::create_deck;
     use crate::game::tile::Tile;
 
     #[test]
     fn test_create_shuffle_deck() {
         // Create the shuffle deck
-        let deck = create_shuffle_deck();
+        let deck = create_deck();
 
         // Check that the deck has exactly 27 tiles
         assert_eq!(
diff --git a/src/game/deck.rs b/src/game/deck.rs
index e69de29..2709b0d 100644
--- a/src/game/deck.rs
+++ b/src/game/deck.rs
@@ -0,0 +1,6 @@
+use crate::game::tile::Tile;
+
+#[derive(Debug, Clone, PartialEq)]
+pub struct Deck{
+    pub(crate) tiles: Vec<Tile>,
+}
\ No newline at end of file
diff --git a/src/game/get_legal_moves.rs b/src/game/get_legal_moves.rs
index e69de29..b8c40e3 100644
--- a/src/game/get_legal_moves.rs
+++ b/src/game/get_legal_moves.rs
@@ -0,0 +1,17 @@
+use crate::game::plateau::Plateau;
+use crate::game::tile::Tile;
+
+pub fn get_legal_moves(plateau: Plateau) -> Vec<usize> {
+    plateau
+        .tiles
+        .iter()
+        .enumerate()
+        .filter_map(|(i, tile)| {
+            if *tile == Tile(0, 0, 0) {
+                Some(i)
+            } else {
+                None
+            }
+        })
+        .collect()
+}
\ No newline at end of file
diff --git a/src/game/mod.rs b/src/game/mod.rs
index e69de29..97a038c 100644
--- a/src/game/mod.rs
+++ b/src/game/mod.rs
@@ -0,0 +1,9 @@
+pub mod deck;
+pub mod plateau;
+pub mod game_state;
+pub mod tile;
+pub mod create_deck;
+pub mod remove_tile_from_deck;
+pub mod plateau_is_full;
+pub mod get_legal_moves;
+pub mod simulate_game;
diff --git a/src/game/plateau.rs b/src/game/plateau.rs
index f6c4e1c..0502b7a 100644
--- a/src/game/plateau.rs
+++ b/src/game/plateau.rs
@@ -1,6 +1,13 @@
-use crate::tile::Tile;
+use crate::game::tile::Tile;
 
 #[derive(Debug, Clone, PartialEq)]
 pub(crate) struct Plateau{
     pub(crate) tiles: Vec<Tile>,
-}
\ No newline at end of file
+}
+
+
+pub(crate) fn create_plateau_empty() -> Plateau {
+    Plateau {
+        tiles: vec![Tile(0, 0, 0); 19],
+    }
+}
diff --git a/src/game/plateau_is_full.rs b/src/game/plateau_is_full.rs
index e69de29..050e566 100644
--- a/src/game/plateau_is_full.rs
+++ b/src/game/plateau_is_full.rs
@@ -0,0 +1,6 @@
+use crate::game::plateau::Plateau;
+use crate::game::tile::Tile;
+
+pub fn is_plateau_full(plateau: &Plateau) -> bool {
+    plateau.tiles.iter().all(|tile| *tile != Tile(0, 0, 0))
+}
\ No newline at end of file
diff --git a/src/game/simulate_game.rs b/src/game/simulate_game.rs
index e69de29..d370bf1 100644
--- a/src/game/simulate_game.rs
+++ b/src/game/simulate_game.rs
@@ -0,0 +1,41 @@
+use rand::Rng;
+use crate::game::deck::Deck;
+use crate::game::get_legal_moves::get_legal_moves;
+use crate::game::plateau::Plateau;
+use crate::game::plateau_is_full::is_plateau_full;
+use crate::game::tile::Tile;
+use crate::scoring::scoring::result;
+
+pub fn simulate_games(plateau: Plateau, deck: Deck) -> i32 {
+    let mut simulated_plateau = plateau.clone();
+    let simulated_deck = deck.clone();
+    let mut legal_moves = get_legal_moves(simulated_plateau.clone());
+
+    // Filter out invalid tiles (0, 0, 0)
+    let mut valid_tiles: Vec<Tile> = simulated_deck
+        .tiles
+        .iter()
+        .cloned()
+        .filter(|tile| *tile != Tile(0, 0, 0))
+        .collect();
+
+    let mut rng = rand::rng(); // Fixed: Use new API
+
+    while !is_plateau_full(&simulated_plateau) {
+        if legal_moves.is_empty() || valid_tiles.is_empty() {
+            break;
+        }
+
+        // Fixed: Use new rand API
+        let position_index = rng.random_range(0..legal_moves.len());
+        let position = legal_moves.swap_remove(position_index); // Swap-remove for O(1) removal
+
+        let tile_index = rng.random_range(0..valid_tiles.len());
+        let chosen_tile = valid_tiles.swap_remove(tile_index); // Swap-remove for O(1) removal
+
+        // Place the chosen tile
+        simulated_plateau.tiles[position] = chosen_tile;
+    }
+
+    result(&simulated_plateau) // Compute and return the result
+}
\ No newline at end of file
diff --git a/src/main.rs b/src/main.rs
index 390fb85..90a57dc 100644
--- a/src/main.rs
+++ b/src/main.rs
@@ -1,44 +1,61 @@
+use chrono::Utc;
+use clap::Parser;
+use futures_util::stream::SplitSink;
+use futures_util::{SinkExt, StreamExt};
+use rand::{rng, Rng};
+use serde_json;
 use std::collections::HashMap;
 use std::fs::OpenOptions;
 use std::io::{BufWriter, Write};
 use std::path::Path;
 use std::sync::Arc;
-use chrono::Utc;
-use clap::Parser;
-use futures_util::{SinkExt, StreamExt};
-use futures_util::stream::SplitSink;
-use rand::{Rng, rng};
-use serde_json;
-use tch::{Device, IndexOp, nn, Tensor};
 use tch::nn::{Optimizer, OptimizerConfig};
+use tch::{nn, Device, IndexOp, Tensor};
 use tokio::net::TcpListener;
-use tokio_tungstenite::{accept_async, WebSocketStream};
 use tokio_tungstenite::tungstenite::protocol::Message;
+use tokio_tungstenite::{accept_async, WebSocketStream};
 
-use create_plateau_empty::create_plateau_empty;
-use create_shuffle_deck::create_shuffle_deck;
-use result::result;
+use crate::game::deck::Deck;
+use crate::game::plateau::create_plateau_empty;
 use crate::logging::setup_logging;
+use crate::mcts::mcts_result::MCTSResult;
 use crate::mcts_vs_human::play_mcts_vs_human;
-use crate::policy_value_net::{PolicyNet, ValueNet};
-use crate::remove_tile_from_deck::replace_tile_in_deck;
-use crate::test::{Deck, Plateau, Tile};
+use game::create_deck::create_deck;
+use game::plateau::Plateau;
+use game::remove_tile_from_deck::replace_tile_in_deck;
+use game::tile::Tile;
+use neural::policy_value_net::{PolicyNet, ValueNet};
+use crate::data::append_result::append_to_results_file;
+use crate::data::load_data::load_game_data;
+use crate::data::save_data::save_game_data;
+use crate::game::get_legal_moves::get_legal_moves;
+use crate::game::plateau_is_full::is_plateau_full;
+use crate::game::simulate_game::simulate_games;
+use crate::mcts::algorithm::mcts_find_best_position_for_tile_with_nn;
+use crate::neural::tensor_conversion::convert_plateau_to_tensor;
+use crate::neural::training::gradient_clipping::enhanced_gradient_clipping;
+use crate::neural::training::normalization::robust_state_normalization;
+use crate::neural::training::trainer::train_network_with_game_data;
+use crate::scoring::scoring::result;
+use crate::strategy::position_evaluation::enhanced_position_evaluation;
+use crate::training::evaluator::evaluate_model;
+use crate::training::session::train_and_evaluate;
+use crate::training::websocket::reconnect_websocket;
+use crate::utils::image::generate_tile_image_names;
+use crate::utils::random_index::random_index;
 
 mod test;
-mod result;
-mod remove_tile_from_deck;
-mod create_plateau_empty;
-mod create_shuffle_deck;
 
-mod policy_value_net;
-mod mcts_vs_human;
+mod game;
 mod logging;
-
-fn generate_tile_image_names(tiles: &[Tile]) -> Vec<String> {
-    tiles.iter().map(|tile| {
-        format!("../image/{}{}{}.png", tile.0, tile.1, tile.2)
-    }).collect()
-}
+mod mcts;
+mod mcts_vs_human;
+mod neural;
+mod utils;
+mod strategy;
+mod scoring;
+mod data;
+mod training;
 
 #[derive(Parser, Debug)]
 #[command(name = "take_it_easy")]
@@ -52,71 +69,10 @@ struct Config {
     num_simulations: usize,
 
     /// Run MCTS vs Human instead of training
-    #[arg(long)]
+    #[arg(long, default_value_t = true)]
     mcts_vs_human: bool,
 }
-fn enhanced_gradient_clipping(vs_value: &nn::VarStore, vs_policy: &nn::VarStore) -> (f64, f64) {
-    let mut max_grad_value: f64 = 0.0;
-    let mut max_grad_policy: f64 = 0.0;
-
-    tch::no_grad(|| {
-        // Value network - clipping tr√®s agressif
-        for (_name, tensor) in vs_value.variables() {
-            if tensor.grad().defined() {
-                let grad_norm = tensor.grad().norm().double_value(&[]);
-                max_grad_value = max_grad_value.max(grad_norm);
 
-                // BEAUCOUP plus agressif que (-1.0, 1.0)
-                tensor.grad().clamp_(-0.5, 0.5);
-            }
-        }
-
-        // Policy network - clipping mod√©r√©
-        for (_name, tensor) in vs_policy.variables() {
-            if tensor.grad().defined() {
-                let grad_norm = tensor.grad().norm().double_value(&[]);
-                max_grad_policy = max_grad_policy.max(grad_norm);
-
-                tensor.grad().clamp_(-1.0, 1.0);
-            }
-        }
-
-        // Log seulement si vraiment √©lev√©
-        if max_grad_value > 1.0 {
-            log::warn!("üî• Value grad norm: {:.3}", max_grad_value);
-        }
-        if max_grad_policy > 2.0 {
-            log::warn!("üî• Policy grad norm: {:.3}", max_grad_policy);
-        }
-    });
-
-    (max_grad_value, max_grad_policy)
-}
-fn robust_state_normalization(state: &Tensor) -> Tensor {
-    // 1. Clamp les valeurs extr√™mes
-    let clamped = state.clamp(-10.0, 10.0);
-
-    // 2. Calcul de la m√©diane pour normalisation robuste
-    let flattened = clamped.view(-1);
-    let sorted = flattened.sort(0, false).0;
-    let median_idx = sorted.size()[0] / 2;
-    let median = sorted.i(median_idx).double_value(&[]);
-
-    // 3. MAD (Median Absolute Deviation) au lieu de std
-    let deviations = (flattened - median).abs();
-    let sorted_dev = deviations.sort(0, false).0;
-    let mad = sorted_dev.i(median_idx).double_value(&[]) * 1.4826;
-
-    // 4. Normalisation avec MAD
-    let normalized = if mad > 1e-6 {
-        (clamped - median) / mad.max(1e-6)
-    } else {
-        clamped - median
-    };
-
-    // 5. Clamp final pour √©viter les valeurs extr√™mes
-    normalized.clamp(-3.0, 3.0)
-}
 #[tokio::main]
 async fn main() {
     let config = Config::parse();
@@ -130,15 +86,12 @@ async fn main() {
     let mut policy_net = PolicyNet::new(&vs_policy, input_dim);
     let mut value_net = ValueNet::new(&mut vs_value, input_dim);
 
-
-
-
-
     // Load weights if the model directory exists
     if Path::new(model_path).exists() {
         log::info!("üîÑ Loading model weights from {}", model_path);
 
-        if let Err(e) = policy_net.load_model(&mut vs_policy, "model_weights/policy/policy.params") {
+        if let Err(e) = policy_net.load_model(&mut vs_policy, "model_weights/policy/policy.params")
+        {
             log::error!("‚ö†Ô∏è Error loading PolicyNet: {:?}", e);
             log::info!("‚û°Ô∏è  Initializing PolicyNet with random weights.");
         }
@@ -150,15 +103,14 @@ async fn main() {
     } else {
         log::info!("üì≠ No pre-trained model found. Initializing new models.");
     }
-    let mut optimizer_policy = nn::Adam ::default().build(&vs_policy, 1e-3).unwrap();
+    let mut optimizer_policy = nn::Adam::default().build(&vs_policy, 1e-3).unwrap();
     // Change your optimizer (around line 100):
     let mut optimizer_value = nn::Adam {
-        wd: 1e-6,  // Was 1e-5
+        wd: 1e-6, // Was 1e-5
         ..Default::default()
-    }.build(&vs_value, 2e-4).unwrap(); // Was 1e-3
-
-
-
+    }
+    .build(&vs_value, 2e-4)
+    .unwrap(); // Was 1e-3
 
     // ‚ûï Duel Mode: MCTS vs Human
     if config.mcts_vs_human {
@@ -178,7 +130,7 @@ async fn main() {
             &mut write,
             &mut read,
         )
-            .await;
+        .await;
 
         return; // Exit after duel game
     }
@@ -201,1210 +153,16 @@ async fn main() {
         50, // Evaluate every 50 games
         listener.into(),
     )
-        .await;
-}
-fn random_index(max: usize) -> usize {
-    use rand::Rng;
-    let mut rng = rand::thread_rng();
-    rng.gen_range(0..max)
-}
-
-// Version simplifi√©e qui se concentre sur les positions strat√©giques
-fn calculate_line_completion_bonus(plateau: &Plateau, position: usize, tile: &Tile) -> f64 {
-    let mut bonus = 0.0;
-
-    // Bonus bas√© sur les positions strat√©giques identifi√©es dans tes donn√©es
-    bonus += match position {
-        8 => 5.0,   // Position 8: 150.6 moyenne - excellente
-        14 => 4.0,  // Position 14: 147.7 moyenne - tr√®s bonne
-        2 => 4.0,   // Position 2: 147.1 moyenne - tr√®s bonne
-        5 => 3.0,   // Position 5: 143.6 moyenne - bonne
-        11 => 3.0,  // Position 11: 142.9 moyenne - bonne
-        10 => 2.0,  // Position 10: 140.8 moyenne - correcte
-        13 => 2.0,  // Position 13: 140.2 moyenne - correcte
-        1 | 4 | 6 | 9 | 0 => 1.0,  // Positions moyennes
-        12 | 15 | 16 => 0.5,  // Positions plus faibles
-        7 | 17 => 0.0,  // Positions les plus faibles
-        _ => 0.0,
-    };
-
-    // Bonus pour les valeurs de tuiles √©lev√©es (plus de points potentiels)
-    let tile_value_bonus = ((tile.0 + tile.1) as f64) * 0.1;
-    bonus += tile_value_bonus;
-
-    // Bonus pour la coh√©rence des couleurs/formes
-    if tile.0 == tile.1 {
-        bonus += 1.0;  // Tuiles avec m√™me couleur et forme
-    }
-
-    // Bonus central l√©g√®rement plus complexe
-    let row = position / 3;
-    let col = position % 3;
-    if row >= 1 && row <= 4 && col >= 1 && col <= 1 {
-        bonus += 2.0;  // Zone centrale du plateau
-    }
-
-    bonus
-}
-
-// ============================================================================
-// ALTERNATIVE PLUS SIMPLE (Si la version ci-dessus pose encore probl√®me)
-// ============================================================================
-
-// Si vous pr√©f√©rez une version plus simple, utilisez celle-ci:
-
-fn enhanced_position_evaluation(plateau: &Plateau, position: usize, tile: &Tile, current_turn: usize) -> f64 {
-    // Score de base alignement (votre fonction existante)
-    let alignment_score = compute_alignment_score(plateau, position, tile);
-
-    // Bonus pour positions centrales strat√©giques en d√©but de partie
-    let position_bonus = if current_turn < 8 {
-        match position {
-            7 | 8 | 9 | 10 | 11 => 5.0,    // Ligne centrale - critique
-            4 | 5 | 6 | 12 | 13 | 14 | 15 => 3.0, // Positions strat√©giques
-            _ => 0.0,
-        }
-    } else {
-        0.0 // En fin de partie, seul l'alignement compte
-    };
-
-    // Malus pour positions coins/bords si d√©but de partie
-    let position_malus = if current_turn < 5 {
-        match position {
-            0 | 2 | 16 | 18 => -2.0,  // Coins - √† √©viter en d√©but
-            1 | 17 => -1.0,           // Bords
-            _ => 0.0,
-        }
-    } else {
-        0.0
-    };
-
-    // Bonus pour compl√©tion de lignes
-    let completion_bonus = calculate_line_completion_bonus(plateau, position, tile);
-
-    alignment_score + position_bonus + position_malus + completion_bonus
-}
-
-fn mcts_find_best_position_for_tile_with_nn(
-    plateau: &mut Plateau,
-    deck: &mut Deck,
-    chosen_tile: Tile,
-    policy_net: &PolicyNet,
-    value_net: &ValueNet,
-    num_simulations: usize,
-    current_turn:usize,
-    total_turns:usize,
-) -> MCTSResult {
-    let legal_moves = get_legal_moves(plateau.clone());
-    if legal_moves.is_empty() {
-        return MCTSResult {
-            best_position: 0,
-            board_tensor: convert_plateau_to_tensor(plateau, &chosen_tile, deck,current_turn, total_turns),
-            subscore: 0.0,
-        };
-    }
-
-    let board_tensor = convert_plateau_to_tensor(plateau, &chosen_tile, deck,current_turn, total_turns);
-    let policy_logits = policy_net.forward(&board_tensor, false);
-    let policy = policy_logits.log_softmax(-1, tch::Kind::Float).exp(); // Log-softmax improves numerical stability
-
-    let mut visit_counts: HashMap<usize, usize> = HashMap::new();
-    let mut total_scores: HashMap<usize, f64> = HashMap::new();
-    let mut ucb_scores: HashMap<usize, f64> = HashMap::new();
-    let mut total_visits: i32 = 0;
-
-
-    for &position in &legal_moves {
-        visit_counts.insert(position, 0);
-        total_scores.insert(position, 0.0);
-        ucb_scores.insert(position, f64::NEG_INFINITY);
-    }
-
-    let c_puct = if current_turn < 5 {
-        4.2  // Plus d'exploitation en d√©but de partie (positions critiques)
-    } else if current_turn > 15 {
-        3.0  // Plus d'exploration en fin de partie (adaptation)
-    } else {
-        3.8  // √âquilibre pour le milieu de partie
-    };
-
-    // **Compute ValueNet scores for all legal moves**
-    let mut value_estimates = HashMap::new();
-    let mut min_value = f64::INFINITY;
-    let mut max_value = f64::NEG_INFINITY;
-
-    for &position in &legal_moves {
-        let mut temp_plateau = plateau.clone();
-        let mut temp_deck = deck.clone();
-
-        temp_plateau.tiles[position] = chosen_tile;
-        temp_deck = replace_tile_in_deck(&temp_deck, &chosen_tile);
-        let board_tensor_temp = convert_plateau_to_tensor(&temp_plateau, &chosen_tile, &temp_deck,current_turn, total_turns);
-
-        let pred_value = value_net.forward(&board_tensor_temp, false).double_value(&[]);
-        let pred_value = pred_value.clamp(-1.0, 1.0);
-
-        // Track min and max for dynamic pruning
-        min_value = min_value.min(pred_value);
-        max_value = max_value.max(pred_value);
-
-        value_estimates.insert(position, pred_value);
-    }
-
-    // **Dynamic Pruning Strategy**
-    let value_threshold = if current_turn < 8 {
-        min_value + (max_value - min_value) * 0.1  // Garder plus de candidats en d√©but
-    } else {
-        min_value + (max_value - min_value) * 0.15 // Pruning moins agressif
-    };
-
-    for _ in 0..num_simulations {
-        let mut moves_with_prior: Vec<_> = legal_moves
-            .iter()
-            .filter(|&&pos| value_estimates[&pos] >= value_threshold) // Prune weak moves
-            .map(|&pos| (pos, policy.i((0, pos as i64)).double_value(&[])))
-            .collect();
-
-        moves_with_prior.sort_by(|a, b| b.1.partial_cmp(&a.1).unwrap_or(std::cmp::Ordering::Equal));
-
-        let top_k = usize::min(
-            moves_with_prior.len(),
-            ((total_visits as f64).sqrt() as usize).max(5),
-        );
-
-        let subset_moves: Vec<usize> = moves_with_prior.iter().take(top_k).map(|&(pos, _)| pos).collect();
-
-        for &position in &subset_moves {
-            let mut temp_plateau = plateau.clone();
-            let mut temp_deck = deck.clone();
-
-            temp_plateau.tiles[position] = chosen_tile;
-            temp_deck = replace_tile_in_deck(&temp_deck, &chosen_tile);
-
-            let value_estimate = *value_estimates.get(&position).unwrap_or(&0.0);
-
-            // **Improved Adaptive Rollout Strategy**
-            let rollout_count = match value_estimate {
-                x if x > 8.0 => 2, // Very strong move -> minimal rollouts
-                x if x > 6.0 => 4, // Strong move -> fewer rollouts
-                x if x > 4.0 => 6, // Decent move -> moderate rollouts
-                _ => 8,           // Uncertain move -> more rollouts
-            };
-
-            let mut total_simulated_score = 0.0;
-
-            for _ in 0..rollout_count {
-                let mut lookahead_plateau = temp_plateau.clone();
-                let mut lookahead_deck = temp_deck.clone();
-
-                // üîÆ √âtape 1.1 ‚Äî Tirer une tuile hypoth√©tique (T2)
-                if lookahead_deck.tiles.is_empty() {
-                    continue;
-                }
-                let tile2_index = random_index(lookahead_deck.tiles.len());
-                let tile2 = lookahead_deck.tiles[tile2_index];
-
-                // üîç √âtape 1.2 ‚Äî Simuler tous les placements possibles de cette tuile
-                let second_moves = get_legal_moves(lookahead_plateau.clone());
-
-                let mut best_score_for_tile2: f64 = 0.0;
-
-                for &pos2 in &second_moves {
-                    let mut plateau2 = lookahead_plateau.clone();
-                    let mut deck2 = lookahead_deck.clone();
-
-                    plateau2.tiles[pos2] = tile2;
-                    deck2 = replace_tile_in_deck(&deck2, &tile2);
-
-                    let score = simulate_games(plateau2.clone(), deck2.clone()) as f64;
-                    best_score_for_tile2 = best_score_for_tile2.max(score);
-                }
-
-                total_simulated_score += best_score_for_tile2;
-            }
-
-            let simulated_score = total_simulated_score / rollout_count as f64;
-
-            let visits = visit_counts.entry(position).or_insert(0);
-            *visits += 1;
-            total_visits += 1;
-
-            let total_score = total_scores.entry(position).or_insert(0.0);
-            *total_score += simulated_score as f64;
-
-            let exploration_param = c_puct * (total_visits as f64).ln() / (1.0 + *visits as f64);
-            let prior_prob = policy.i((0, position as i64)).double_value(&[]);
-            let average_score = *total_score / (*visits as f64);
-            // üß™ Reduce weight of rollout average
-            let enhanced_eval = enhanced_position_evaluation(&temp_plateau, position, &chosen_tile, current_turn);
-
-            // Int√©grer dans le calcul UCB
-            let mut ucb_score = (average_score * 0.5)
-                + exploration_param * (prior_prob.sqrt())
-                + 0.25 * value_estimate.clamp(0.0, 2.0)
-                + 0.1 * enhanced_eval; // Nouveau facteur d'√©valuation
-
-            // üî• Explicit Priority Logic HERE üî•
-            // 1Ô∏è‚É£ Ajoute cette fonction en dehors de ta mcts_find_best_position_for_tile_with_nn
-
-
-            // 2Ô∏è‚É£ Int√®gre ceci dans ta boucle ucb_scores, juste apr√®s le boost fixe
-
-            if chosen_tile.0 == 9 && [7, 8, 9, 10, 11].contains(&position) {
-                ucb_score += 10000.0;  // double boost
-            } else if chosen_tile.0 == 5 && [3, 4, 5, 6, 12, 13, 14, 15].contains(&position) {
-                ucb_score += 8000.0;
-            } else if chosen_tile.0 == 1 && [0, 1, 2, 16, 17, 18].contains(&position) {
-                ucb_score += 6000.0;
-            }
-
-            // üî• Alignment Priority Logic üî•
-
-
-
-
-
-            ucb_scores.insert(position, ucb_score);
-        }
-    }
-
-    // Select the move with the highest UCB score
-    let best_position = legal_moves.into_iter()
-        .max_by(|&a, &b| {
-            ucb_scores.get(&a).unwrap_or(&f64::NEG_INFINITY)
-                .partial_cmp(ucb_scores.get(&b).unwrap_or(&f64::NEG_INFINITY))
-                .unwrap_or(std::cmp::Ordering::Equal)
-        }).unwrap_or(0);
-
-
-    // **NEW: Simulate the Rest of the Game to Get Final Score**
-    let mut final_plateau = plateau.clone();
-    let mut final_deck = deck.clone();
-    final_plateau.tiles[best_position] = chosen_tile;
-    final_deck = replace_tile_in_deck(&final_deck, &chosen_tile);
-
-    while !is_plateau_full(&final_plateau) {
-        let tile_index = random_index(final_deck.tiles.len());
-        let random_tile = final_deck.tiles[tile_index];
-
-        let available_moves = get_legal_moves(final_plateau.clone());
-        if available_moves.is_empty() {
-            break;
-        }
-
-        let random_position = available_moves[random_index(available_moves.len())];
-        final_plateau.tiles[random_position] = random_tile;
-        final_deck = replace_tile_in_deck(&final_deck, &random_tile);
-    }
-
-    let final_score = result(&final_plateau); // Get actual game score
-
-    log::info!("ü§ñ Pos:{} Score:{}", best_position, final_score as i32);
-
-    MCTSResult {
-        best_position,
-        board_tensor,
-        subscore: final_score as f64, // Store real final score, not UCB score
-    }
-}
-fn local_lookahead(mut plateau: Plateau, mut deck: Deck, depth: usize) -> i32 {
-    for _ in 0..depth {
-        if is_plateau_full(&plateau) || deck.tiles.is_empty() {
-            break;
-        }
-
-        let tile_index = random_index(deck.tiles.len());
-        let chosen_tile = deck.tiles[tile_index];
-
-        let legal_moves = get_legal_moves(plateau.clone());
-        if legal_moves.is_empty() {
-            break;
-        }
-
-        let best_pos = legal_moves
-            .into_iter()
-            .max_by_key(|&pos| compute_alignment_score(&plateau, pos, &chosen_tile) as i32)
-            .unwrap();
-
-        plateau.tiles[best_pos] = chosen_tile;
-        deck = replace_tile_in_deck(&deck, &chosen_tile);
-    }
-
-    result(&plateau)
-}
-fn compute_global_stats(game_data: &[MCTSResult]) -> (Tensor, Tensor) {
-    let stacked = Tensor::cat(
-        &game_data.iter().map(|gd| gd.board_tensor.shallow_clone()).collect::<Vec<_>>(),
-        0
-    );
-
-    let mean = stacked.mean_dim(&[0i64, 2, 3][..], true, tch::Kind::Float);
-    let std = stacked.std_dim(&[0i64, 2, 3][..], true, true).clamp_min(1e-8);
-
-
-    (mean, std)
+    .await;
 }
 
 
 
 
-fn normalize_input(tensor: &Tensor, global_mean: &Tensor, global_std: &Tensor) -> Tensor {
-    (tensor - global_mean) / (global_std + 1e-8)
-}
-
-
-// Enhanced training function with better value network stabilization
-fn train_network_with_game_data(
-    vs_policy: &nn::VarStore,
-    vs_value: &nn::VarStore,
-    game_data: &[MCTSResult],
-    discount_factor: f64,
-    policy_net: &PolicyNet,
-    value_net: &ValueNet,
-    optimizer_policy: &mut Optimizer,
-    optimizer_value: &mut Optimizer,
-) {
-    // Hyperparameters
-    let entropy_weight = 0.05;
-    let gamma = 0.99;
-    let epsilon = 1e-8;
-
-    // Initialize accumulators
-    let mut predictions = Vec::new();
-    let mut targets = Vec::new();
-    let mut total_policy_loss = Tensor::zeros(&[], tch::kind::FLOAT_CPU);
-    let mut total_value_loss = Tensor::zeros(&[], tch::kind::FLOAT_CPU);
-    let mut total_entropy_loss = Tensor::zeros(&[], tch::kind::FLOAT_CPU);
-
-    // Initialize trajectory rewards and discounted sum
-    let mut trajectory_rewards = Vec::new();
-    let mut discounted_sum = Tensor::zeros(&[], (tch::Kind::Float, tch::Device::Cpu));
-
-    // === Training Loop ===
-    for (step, result) in game_data.iter().rev().enumerate() {
-        // üõë No Normalization: Use raw tensor
-        let state = result.board_tensor.shallow_clone();
-        let normalized_state = robust_state_normalization(&state);
-
-        // Forward pass through networks with normalized state
-        let pred_policy = policy_net.forward(&normalized_state, true).clamp_min(1e-7);
-        let pred_value = value_net.forward(&normalized_state, true);
-
-        // Forward pass through networks with normalized state
-        // Normalize reward: divide by a constant max value (e.g., 100)
-        let reward = Tensor::from(result.subscore).to_kind(tch::Kind::Float) / 100.0;
-        let gamma_tensor = Tensor::from_slice(&[gamma]).to_kind(tch::Kind::Float);
-
-        // ‚úÖ NaN & Inf Check for reward
-        if reward.isnan().any().double_value(&[]) > 0.0 || reward.isinf().any().double_value(&[]) > 0.0 {
-            log::error!("‚ö†Ô∏è NaN or Inf detected in reward at step {}", step);
-            continue;
-        }
-
-        // Update discounted sum with normalized reward
-        discounted_sum = reward + gamma_tensor * discounted_sum;
-
-        // ‚úÖ NaN & Inf Check for discounted sum
-        if discounted_sum.isnan().any().double_value(&[]) > 0.0 || discounted_sum.isinf().any().double_value(&[]) > 0.0 {
-            log::error!("‚ö†Ô∏è NaN or Inf detected in discounted sum at step {}", step);
-            continue;
-        }
-
-        // Store the value for analysis
-        trajectory_rewards.push(discounted_sum.double_value(&[]));
-
-        // Generate target tensor directly from discounted sum
-        let discounted_reward = discounted_sum.shallow_clone();
 
-        // Append for later analysis
-        predictions.push(pred_value.double_value(&[]));
-        targets.push(discounted_reward.double_value(&[]));
 
-        // === Compute Losses ===
-        // Policy loss
-        let best_position = result.best_position as i64;
-        let target_policy = Tensor::zeros(&[1, pred_policy.size()[1]], tch::kind::FLOAT_CPU);
-        target_policy.i((0, best_position)).fill_(1.0);
-        let log_policy = pred_policy.log();
-        let policy_loss = -(target_policy * log_policy.shallow_clone()).sum(tch::Kind::Float);
-        total_policy_loss += policy_loss;
 
-        // Entropy loss
-        let entropy_loss = -(pred_policy * (log_policy + epsilon)).sum(tch::Kind::Float);
-        total_entropy_loss += entropy_loss;
 
-        // Value loss (Huber loss for better stability)
-        let diff = discounted_reward.shallow_clone() - pred_value.shallow_clone();
-        let abs_diff = diff.abs();
-        let delta = 1.0;
-        let value_loss = abs_diff.le(delta).to_kind(tch::Kind::Float) * 0.5 * &diff * &diff
-            + abs_diff.gt(delta).to_kind(tch::Kind::Float) * (delta * (&abs_diff - 0.5 * delta));
-        total_value_loss += value_loss.mean(tch::Kind::Float);
-    }
-
-    // Fix: Add explicit type annotation for total_loss
-    let total_loss: Tensor = total_policy_loss.shallow_clone()
-        + total_value_loss.shallow_clone()
-        + (entropy_weight * total_entropy_loss.shallow_clone());
-
-    // Log the loss before backpropagation
-    log::info!("üí° Total Loss before backward: {:.4}", total_loss.double_value(&[]));
-
-    // ‚úÖ Enhanced NaN and Inf check before backpropagation
-    if total_loss.isnan().any().double_value(&[]) > 0.0 {
-        log::error!("‚ö†Ô∏è NaN detected in total loss! Skipping backpropagation.");
-        return;
-    }
-    if total_loss.isinf().any().double_value(&[]) > 0.0 {
-        log::error!("‚ö†Ô∏è Inf detected in total loss! Skipping backpropagation.");
-        return;
-    }
-
-    // Check if total_loss requires gradients before calling backward
-    if !total_loss.requires_grad() {
-        log::error!("‚ö†Ô∏è Total loss does not require gradients! Skipping backpropagation.");
-        return;
-    }
-
-    total_loss.backward();
-
-    // Enhanced gradient clipping with fixed type annotation
-    let (_max_grad_value, _max_grad_policy) = enhanced_gradient_clipping(vs_value, vs_policy);
-
-
-    // === Optimizer Step ===
-    optimizer_policy.step();
-    optimizer_policy.zero_grad();
-    optimizer_value.step();
-    optimizer_value.zero_grad();
-
-    log::info!(
-        "üéØ Update Complete | Policy Loss: {:.4}, Value Loss: {:.4}, Entropy Loss: {:.4}",
-        total_policy_loss.double_value(&[]),
-        total_value_loss.double_value(&[]),
-        total_entropy_loss.double_value(&[])
-    );
-}
-
-// Fixed simulate_games function with updated rand API
-
-
-// N-step returns calculation for more stable targets
-fn calculate_n_step_returns(rewards: &[f32], gamma: f32, n: usize) -> Vec<f32> {
-    let mut returns = Vec::new();
-
-    for i in 0..rewards.len() {
-        let mut ret = 0.0;
-        let mut discount = 1.0;
-
-        // Calculate n-step return
-        for j in 0..n.min(rewards.len() - i) {
-            ret += discount * rewards[i + j];
-            discount *= gamma;
-        }
-
-        returns.push(ret / 100.0); // Normalize by dividing by max expected score
-    }
-
-    returns
-}
-
-// Huber loss for more stable value training
-fn huber_loss(predictions: &Tensor, targets: &Tensor, delta: f64) -> Tensor {
-    let diff = predictions - targets;
-    let abs_diff = diff.abs();
-    let quadratic = (abs_diff.le(delta)).to_kind(tch::Kind::Float) * 0.5 * &diff * &diff;
-    let linear = (abs_diff.gt(delta)).to_kind(tch::Kind::Float) *
-        (delta * (&abs_diff - 0.5 * delta));
-    (quadratic + linear).mean(tch::Kind::Float)
-}
-
-// Prediction accuracy calculation for monitoring
-fn calculate_prediction_accuracy(predictions: &[f64], targets: &[f64]) -> f64 {
-    if predictions.is_empty() || targets.is_empty() {
-        return 0.0;
-    }
-
-    let mse: f64 = predictions.iter()
-        .zip(targets.iter())
-        .map(|(p, t)| (p - t).powi(2))
-        .sum::<f64>() / predictions.len() as f64;
-
-    // Convert MSE to accuracy-like metric (higher is better)
-    1.0 / (1.0 + mse.sqrt())
-}
-
-
-
-
-
-
-
-fn convert_plateau_to_tensor(
-    plateau: &Plateau,
-    _tile: &Tile,        // Add underscore prefix
-    _deck: &Deck,        // Add underscore prefix
-    current_turn: usize,
-    total_turns: usize
-) -> Tensor {
-    let mut features = vec![0.0; 5 * 47]; // 5 channels
-
-    // Channel 1-3: Plateau (only use plateau data, not tile/deck)
-    for (i, t) in plateau.tiles.iter().enumerate() {
-        if i < 19 {
-            features[i] = (t.0 as f32 / 10.0).clamp(0.0, 1.0);
-            features[47 + i] = (t.1 as f32 / 10.0).clamp(0.0, 1.0);
-            features[2 * 47 + i] = (t.2 as f32 / 10.0).clamp(0.0, 1.0);
-        }
-    }
-
-    // Channel 4: Score Potential for each position
-    let potential_scores = compute_potential_scores(plateau);
-    for i in 0..19 {
-        features[3 * 47 + i] = potential_scores[i];
-    }
-
-    // Channel 5: Current Turn
-    let turn_normalized = current_turn as f32 / total_turns as f32;
-    for i in 0..19 {
-        features[4 * 47 + i] = turn_normalized;
-    }
-
-    Tensor::from_slice(&features).view([1, 5, 47, 1])
-}
-fn compute_potential_scores(plateau: &Plateau) -> Vec<f32> {
-    let mut scores = vec![0.0; 19]; // Potential score for each position
-
-    let patterns: Vec<(&[usize], i32, Box<dyn Fn(&Tile) -> i32>)> = vec![
-        (&[0, 1, 2], 3, Box::new(|tile: &Tile| tile.0)),
-        (&[3, 4, 5, 6], 4, Box::new(|tile: &Tile| tile.0)),
-        (&[7, 8, 9, 10, 11], 5, Box::new(|tile: &Tile| tile.0)),
-        (&[12, 13, 14, 15], 4, Box::new(|tile: &Tile| tile.0)),
-        (&[16, 17, 18], 3, Box::new(|tile: &Tile| tile.0)),
-        (&[0, 3, 7], 3, Box::new(|tile: &Tile| tile.1)),
-        (&[1, 4, 8, 12], 4, Box::new(|tile: &Tile| tile.1)),
-        (&[2, 5, 9, 13, 16], 5, Box::new(|tile: &Tile| tile.1)),
-        (&[6, 10, 14, 17], 4, Box::new(|tile: &Tile| tile.1)),
-        (&[11, 15, 18], 3, Box::new(|tile: &Tile| tile.1)),
-        (&[7, 12, 16], 3, Box::new(|tile: &Tile| tile.2)),
-        (&[3, 8, 13, 17], 4, Box::new(|tile: &Tile| tile.2)),
-        (&[0, 4, 9, 14, 18], 5, Box::new(|tile: &Tile| tile.2)),
-        (&[1, 5, 10, 15], 4, Box::new(|tile: &Tile| tile.2)),
-        (&[2, 6, 11], 3, Box::new(|tile: &Tile| tile.2)),
-    ];
-
-    for (indices, multiplier, selector) in &patterns {
-        let mut filled_values = Vec::new();
-        let mut empty_positions = Vec::new();
-
-        for &pos in *indices {
-            if plateau.tiles[pos] == Tile(0, 0, 0) {
-                empty_positions.push(pos);
-            } else {
-                filled_values.push(selector(&plateau.tiles[pos]) as f32);
-            }
-        }
-
-        // If at least one tile is placed in the pattern
-        if !filled_values.is_empty() {
-            let avg_filled_value = filled_values.iter().sum::<f32>() / filled_values.len() as f32;
-            let potential_score = avg_filled_value * (*multiplier as f32);
-
-            for &pos in empty_positions.iter() {
-                scores[pos] += potential_score / empty_positions.len() as f32; // Distribute potential score
-            }
-        }
-    }
-
-    scores
-}
-
-// Fix for alignment score function - add underscore prefix
-fn compute_alignment_score(plateau: &Plateau, position: usize, _tile: &Tile) -> f64 {
-    let patterns: Vec<(&[usize], Box<dyn Fn(&Tile) -> i32>)> = vec![
-        (&[0, 1, 2], Box::new(|t: &Tile| t.0)),
-        (&[3, 4, 5, 6], Box::new(|t: &Tile| t.0)),
-        (&[7, 8, 9, 10, 11], Box::new(|t: &Tile| t.0)),
-        (&[12, 13, 14, 15], Box::new(|t: &Tile| t.0)),
-        (&[16, 17, 18], Box::new(|t: &Tile| t.0)),
-        (&[0, 3, 7], Box::new(|t: &Tile| t.1)),
-        (&[1, 4, 8, 12], Box::new(|t: &Tile| t.1)),
-        (&[2, 5, 9, 13, 16], Box::new(|t: &Tile| t.1)),
-        (&[6, 10, 14, 17], Box::new(|t: &Tile| t.1)),
-        (&[11, 15, 18], Box::new(|t: &Tile| t.1)),
-        (&[7, 12, 16], Box::new(|t: &Tile| t.2)),
-        (&[3, 8, 13, 17], Box::new(|t: &Tile| t.2)),
-        (&[0, 4, 9, 14, 18], Box::new(|t: &Tile| t.2)),
-        (&[1, 5, 10, 15], Box::new(|t: &Tile| t.2)),
-        (&[2, 6, 11], Box::new(|t: &Tile| t.2)),
-    ];
-
-    let mut score = 0.0;
-
-    for (indices, selector) in patterns {
-        if indices.contains(&position) {
-            let values: Vec<i32> = indices
-                .iter()
-                .map(|&i| selector(&plateau.tiles[i]))
-                .filter(|&v| v != 0)
-                .collect();
-
-            if !values.is_empty() {
-                let sum = values.iter().sum::<i32>() as f64;
-                score += sum / values.len() as f64;
-            }
-        }
-    }
-
-    score
-}
-
-
-
-
-
-
-
-
-pub fn load_game_data(file_path: &str) -> Vec<MCTSResult> {
-    // Paths for the .pt files
-    let states_path = format!("{}_states.pt", file_path);
-    let positions_path = format!("{}_positions.pt", file_path);
-    let subscores_path = format!("{}_subscores.pt", file_path);
-
-    // Check if all files exist
-    if !Path::new(&states_path).exists() {
-        println!("‚ö†Ô∏è  Warning: '{}' not found. Returning empty dataset.", states_path);
-        return Vec::new();
-    }
-    if !Path::new(&positions_path).exists() {
-        println!("‚ö†Ô∏è  Warning: '{}' not found. Returning empty dataset.", positions_path);
-        return Vec::new();
-    }
-    if !Path::new(&subscores_path).exists() {
-        println!("‚ö†Ô∏è  Warning: '{}' not found. Returning empty dataset.", subscores_path);
-        return Vec::new();
-    }
-
-    println!("üöÄ Loading game data from .pt files...");
-
-    // Load the saved tensors
-    let state_tensor = Tensor::load(states_path).expect("Failed to load states");
-    let position_tensor = Tensor::load(positions_path).expect("Failed to load positions");
-    let subscore_tensor = Tensor::load(subscores_path).expect("Failed to load subscores");
-
-    // Convert them back into MCTSResult objects
-    let mut data = Vec::new();
-    for i in 0..state_tensor.size()[0] {
-        data.push(MCTSResult {
-            board_tensor: state_tensor.get(i),
-            best_position: position_tensor.get(i).int64_value(&[]) as usize,
-            subscore: subscore_tensor.get(i).double_value(&[]),
-        });
-    }
-    println!("‚úÖ Loaded {} game records.", data.len());
-    data
-}
-
-
-
-
-fn deserialize_game_data(line: &str) -> Option<MCTSResult> {
-    let parts: Vec<&str> = line.split(',').collect();
-
-    if parts.len() != 3 {
-        log::error!("Invalid data format: '{}'", line);
-        return None;
-    }
-
-    // Parse tensor
-    let state_values: Vec<f32> = parts[0]
-        .split_whitespace()
-        .map(|v| v.parse::<f32>())
-        .collect::<Result<Vec<f32>, _>>()
-        .unwrap_or_else(|_| {
-            log::error!("Failed to parse state tensor in line '{}'", line);
-            vec![]
-        });
-
-    if state_values.len() != 5 * 47 {
-        log::error!("ERROR: Parsed tensor has incorrect size {} (expected 235). Data: '{}'", state_values.len(), parts[0]);
-        return None;
-    }
-
-    let state_tensor = Tensor::from_slice(&state_values).view([1, 5, 47, 1]);
-
-    // Parse subscore
-    let subscore = parts[1].parse::<f64>().unwrap_or_else(|_| {
-        log::error!("Failed to parse subscore in line '{}'", line);
-        0.0
-    });
-
-    // Parse best position
-    let best_position = parts[2].parse::<usize>().unwrap_or_else(|_| {
-        log::error!("Failed to parse best_position in line '{}'", line);
-        0
-    });
-
-    Some(MCTSResult {
-        board_tensor: state_tensor,
-        subscore,
-        best_position,
-    })
-}
-
-
-
-
-fn save_game_data(file_path: &str, game_data: Vec<MCTSResult>) {
-    println!("üöÄ Saving game data to .pt files...");
-
-    let mut tensors = vec![];
-    let mut positions = vec![];
-    let mut subscores = vec![];
-
-    for result in game_data {
-        tensors.push(result.board_tensor.shallow_clone());
-        positions.push(result.best_position as i64);
-        subscores.push(result.subscore as f32);
-    }
-
-    // Cr√©ation des nouveaux tensors
-    let state_tensor = Tensor::stack(&tensors, 0);
-    let position_tensor = Tensor::from_slice(&positions).view([-1, 1]);
-    let subscore_tensor = Tensor::from_slice(&subscores).view([-1, 1]);
-
-    // üîÑ Append logic: charger les anciens tensors s'ils existent
-    let combined_states = if let Ok(prev) = Tensor::load(format!("{}_states.pt", file_path)) {
-        Tensor::cat(&[prev, state_tensor], 0)
-    } else {
-        state_tensor
-    };
-
-    let combined_positions = if let Ok(prev) = Tensor::load(format!("{}_positions.pt", file_path)) {
-        Tensor::cat(&[prev, position_tensor], 0)
-    } else {
-        position_tensor
-    };
-
-    let combined_subscores = if let Ok(prev) = Tensor::load(format!("{}_subscores.pt", file_path)) {
-        Tensor::cat(&[prev, subscore_tensor], 0)
-    } else {
-        subscore_tensor
-    };
-
-    // üîÑ Sauvegarde des tensors concat√©n√©s
-    if let Err(e) = combined_states.save(format!("{}_states.pt", file_path)) {
-        log::info!("‚ùå Error saving states: {:?}", e);
-    }
-    if let Err(e) = combined_positions.save(format!("{}_positions.pt", file_path)) {
-        log::info!("‚ùå Error saving positions: {:?}", e);
-    }
-    if let Err(e) = combined_subscores.save(format!("{}_subscores.pt", file_path)) {
-        log::info!("‚ùå Error saving subscores: {:?}", e);
-    }
-
-    log::info!("‚úÖ Save complete!");
-}
-
-
-fn tensor_to_vec(tensor: &Tensor) -> Vec<f32> {
-    // Flatten the tensor into a 1D array
-    let flattened = tensor.view(-1); // Reshape the tensor to a 1D view
-
-    // Convert each value in the flattened tensor to f32 and collect into a Vec
-    let mut vec = Vec::new();
-    for i in 0..flattened.size()[0] {
-        let value = flattened.i(i).double_value(&[]) as f32;
-        vec.push(value);
-    }
-
-    vec
-}
-
-
-fn serialize_tensor(tensor: Tensor) -> String {
-    let data: Vec<f32> = tensor_to_vec(&tensor); // Converts the slice to a Vec<f32>
-    data.iter()
-        .map(|v| v.to_string())
-        .collect::<Vec<_>>()
-        .join(" ")
-}
-#[derive(Debug)]
-pub struct MCTSResult {
-    pub board_tensor: Tensor,
-    pub best_position: usize,
-    pub subscore: f64,
-}
-
-impl Clone for MCTSResult {
-    fn clone(&self) -> Self {
-        MCTSResult {
-            board_tensor: self.board_tensor.shallow_clone(), // Manually clone the tensor
-            best_position: self.best_position,
-            subscore: self.subscore,
-        }
-    }
-}
-
-fn append_to_results_file(file_path: &str,  avg_score: f64) {
-    let timestamp = Utc::now().to_rfc3339();
-    let result_line = format!("{},{:.2}\n",timestamp,  avg_score );
-
-    let file = OpenOptions::new()
-        .create(true)
-        .append(true)
-        .open(file_path)
-        .expect("Unable to open results file");
-    let mut writer = BufWriter::new(file);
-    writer
-        .write_all(result_line.as_bytes())
-        .expect("Unable to write to results file");
-}
-async fn reconnect_websocket(
-    listener: &TcpListener,
-) -> Option<SplitSink<WebSocketStream<tokio::net::TcpStream>, Message>> {
-    match listener.accept().await {
-        Ok((stream, _)) => {
-            log::info!("Re-establishing WebSocket connection...");
-            let ws_stream = accept_async(stream).await.expect("Failed to accept WebSocket");
-            let (write, _) = ws_stream.split();
-            Some(write)
-        }
-        Err(e) => {
-            log::error!("Error while reconnecting WebSocket: {:?}", e);
-            None
-        }
-    }
-}
-async fn train_and_evaluate(
-    vs_policy: &nn::VarStore,
-    vs_value: &nn::VarStore,
-    policy_net: &mut PolicyNet,
-    value_net: &mut ValueNet,
-    optimizer_policy: &mut Optimizer,
-    optimizer_value: &mut Optimizer,
-    num_games: usize,
-    num_simulations: usize,
-    evaluation_interval: usize,
-    listener: Arc<TcpListener>,
-) {
-    let mut total_score = 0;
-    let mut games_played = 0;
-    let results_file = "results.csv";
-
-    while let Ok((stream, _)) = listener.accept().await {
-        let ws_stream = accept_async(stream).await.expect("Failed to accept WebSocket");
-        let (mut write, _) = ws_stream.split();
-        let mut scores_by_position: HashMap<usize, Vec<i32>> = HashMap::new();
-        let mut scores = Vec::new(); // Stocke les scores
-        let evaluation_interval_average = 10;
-
-        while games_played < num_games {
-
-            log::info!(
-                "Starting training iteration {}/{}...",
-                games_played + 1,
-                num_games
-            );
-            log::info!("\nüöÄ Starting Batch {}", games_played / evaluation_interval + 1);
-
-            let mut batch_games_played = 0; // Tracks games processed in this evaluation interval
-
-            let max_memory_size = 1000; // Store last 500 games
-
-            for game in 0..evaluation_interval {
-                let mut deck = create_shuffle_deck();
-                let mut plateau = create_plateau_empty();
-                let mut game_data = Vec::new();
-                let mut first_move: Option<(usize, Tile)> = None;
-                let total_turns = 19; // The number of moves in the game
-                let mut current_turn = 0;
-                while !is_plateau_full(&plateau) {
-                    let tile_index = rng().random_range(0..deck.tiles.len());
-                    let chosen_tile = deck.tiles[tile_index];
-                    // ‚úÖ **Send preview before placement**
-                    // ‚úÖ **INSERT YOUR NEW CODE HERE**
-                    let chosen_tile_image = format!("../image/{}{}{}.png", chosen_tile.0, chosen_tile.1, chosen_tile.2);
-                    let payload = serde_json::json!({
-        "next_tile": chosen_tile_image,
-        "plateau_tiles": generate_tile_image_names(&plateau.tiles)
-    });
-                    let serialized = serde_json::to_string(&payload).unwrap();
-                    write.send(Message::Text(serialized)).await.unwrap();
-
-                    let game_result = mcts_find_best_position_for_tile_with_nn(
-                        &mut plateau,
-                        &mut deck,
-                        chosen_tile,
-                        policy_net,
-                        value_net,
-                        num_simulations,
-                        current_turn,
-                        total_turns,
-                    );
-
-                    let best_position = game_result.best_position;
-                    if first_move.is_none() {
-                        first_move = Some((best_position, chosen_tile));
-                    }
-                    plateau.tiles[best_position] = chosen_tile;
-                    deck = replace_tile_in_deck(&deck, &chosen_tile);
-                    // ‚úÖ INSERT THIS TO SEND SCORE TO CLIENT
-                    let current_score = result(&plateau);
-                    let score_payload = serde_json::json!({
-    "type": "score_update",
-    "current_score": current_score,
-});
-                    let serialized_score = serde_json::to_string(&score_payload).unwrap();
-                    if let Err(e) = write.send(Message::Text(serialized_score)).await {
-                        log::error!("WebSocket error when sending score: {:?}", e);
-                        if let Some(new_write) = reconnect_websocket(&listener).await {
-                            write = new_write;
-                        } else {
-                            log::error!("Failed to reconnect WebSocket. Exiting...");
-                            break;
-                        }
-                    }
-
-                    game_data.push(game_result); // Store training data
-
-                    // ‚úÖ **INSERT YOUR NEW CODE HERE**
-                    let payload_after_placement = serde_json::json!({
-        "next_tile": null, // Clear preview
-        "plateau_tiles": generate_tile_image_names(&plateau.tiles) // new updated state
-    });
-                    let serialized = serde_json::to_string(&payload_after_placement).unwrap();
-
-                    // ‚úÖ Handle WebSocket disconnections
-                    if let Err(e) = write.send(Message::Text(serialized.clone())).await {
-                        log::error!("WebSocket error: {:?}. Attempting to reconnect...", e);
-
-                        // **Reconnect WebSocket**
-                        if let Some(new_write) = reconnect_websocket(&listener).await {
-                            write = new_write;
-                        } else {
-                            log::error!("Failed to reconnect WebSocket. Exiting...");
-                            break;
-                        }
-                    }
-                    current_turn += 1; // Increment turn counter each time a tile is placed
-
-                }
-
-                let final_score = result(&plateau);
-
-                if let Some((position, _)) = first_move {
-                    scores_by_position
-                        .entry(position)
-                        .or_insert_with(Vec::new)
-                        .push(final_score);
-                }
-
-                let mut batch_game_data = Vec::new();
-
-                // Prioritized historical data
-                let prioritized_data: Vec<MCTSResult> = load_game_data("game_data")
-                    .into_iter().filter(|r| r.subscore > 100.0) // Only select high-score games
-                    .take(50) // Limit to 50 samples to prevent overfitting
-                    .collect();
-
-                // Add historical data to batch
-                batch_game_data.extend(prioritized_data);
-
-                // Add current game's data to batch
-                batch_game_data.extend(game_data.iter().map(|result| MCTSResult {
-                    best_position: result.best_position,
-                    board_tensor: result.board_tensor.shallow_clone(),
-                    subscore: result.subscore,
-                }));
-
-                // Keep only last max_memory_size experiences
-                if batch_game_data.len() > max_memory_size {
-                    let to_remove = batch_game_data.len() - max_memory_size;
-                    batch_game_data.drain(0..to_remove); // Remove oldest data
-                }
-
-                // Train in batches
-                let batch_size = 10;
-                for batch in batch_game_data.chunks(batch_size) {
-                    train_network_with_game_data(
-                        &vs_policy,
-                        &vs_value,
-                        batch, // Use each batch directly
-                        final_score.into(),
-                        policy_net,
-                        value_net,
-                        optimizer_policy,
-                        optimizer_value,
-                    );
-                }
-
-                log::info!("Game {} finished with score: {}", game + 1, final_score);
-                scores.push(final_score);
-
-                // Update batch-specific counters
-                batch_games_played += 1;
-                total_score += final_score;
-
-                if game % evaluation_interval_average == 0 && game != 0 {
-                    let moyenne: f64 = scores.iter().sum::<i32>() as f64 / scores.len() as f64;
-                    log::info!("üìä [Batch {}] Avg Score: {:.2} | Games Played: {}", games_played / evaluation_interval, moyenne, games_played);
-                    log::info!("batch {} - Score moyen: {:.2}", game, moyenne);
-                    write.send(Message::Text(format!("GAME_RESULT:{}", moyenne))).await.unwrap();
-                }
-
-                // Save current game data for future training
-                save_game_data("game_data", game_data);
-            }
-
-
-            // Update main game counters
-            games_played += batch_games_played;
-
-            // Append results to the file
-            let avg_score = total_score as f64 / games_played as f64;
-            append_to_results_file(results_file,  avg_score);
-
-            // Calculate and display averages
-            let mut averages: Vec<(usize, f64)> = scores_by_position
-                .iter()
-                .map(|(position, scores)| {
-                    let average_score: f64 =
-                        scores.iter().sum::<i32>() as f64 / scores.len() as f64;
-                    (*position, average_score)
-                })
-                .collect();
-
-            averages.sort_by(|a, b| {
-                b.1.partial_cmp(&a.1)
-                    .unwrap_or(std::cmp::Ordering::Equal)
-            });
-
-            log::info!("\n--- Average Scores by First Position (Sorted) ---");
-            for (position, average_score) in averages {
-                log::info!("Position: {}, Average Score: {:.2}", position, average_score);
-            }
-
-            // Evaluate model after each interval
-            evaluate_model(policy_net, value_net,num_simulations).await;
-
-            log::info!(
-                "Games Played: {}, Total Score: {}, Avg Score: {:.2}",
-                games_played,
-                total_score,
-                total_score as f32 / games_played as f32
-            );
-            let model_path = "model_weights";
-            // Save model weights
-            log::info!("Saving models to {}", model_path);
-            log::info!("Saving model weights...");
-            if let Err(e) = policy_net.save_model(vs_policy,"model_weights/policy/policy.params") {
-                log::error!("Error saving PolicyNet weights: {:?}", e);
-            }
-            if let Err(e) = value_net.save_model(vs_value,"model_weights/value/value.params") {
-                log::error!("Error saving ValueNet weights: {:?}", e);
-            }
-        }
-        break; // Exit after handling one connection
-    }
-}
-
-async fn evaluate_model(
-    policy_net: &PolicyNet,
-    value_net: &ValueNet,
-    num_simulations: usize,
-) {
-    log::info!("Evaluating model...");
-    let mut scores = Vec::new();
-
-    for _ in 0..10 {
-        let mut deck = create_shuffle_deck();
-        let mut plateau = create_plateau_empty();
-        let total_turns = 19; // The number of moves in the game
-        let mut current_turn = 0;
-        while !is_plateau_full(&plateau) {
-            let tile_index = rng().random_range(0..deck.tiles.len());
-            let chosen_tile = deck.tiles[tile_index];
-            let game_result = mcts_find_best_position_for_tile_with_nn(
-                &mut plateau,
-                &mut deck,
-                chosen_tile,
-                policy_net,
-                value_net,
-                num_simulations,
-                current_turn,
-                total_turns,
-            );
-            let best_position = game_result.best_position;
-            plateau.tiles[best_position] = chosen_tile;
-            deck = replace_tile_in_deck(&deck, &chosen_tile);
-            current_turn += 1; // Increment turn counter each time a tile is placed
-
-        }
-
-        let game_score = result(&plateau);
-        scores.push(game_score);
-    }
-
-    let avg_score: f64 = scores.iter().copied().sum::<i32>() as f64 / scores.len() as f64;
-    log::info!("Model Evaluation Complete. Avg Score: {:.2}", avg_score);
-    // **Stop ping task**
-}
-
-
-/// Checks if the plateau is full
-fn is_plateau_full(plateau: &Plateau) -> bool {
-    plateau.tiles.iter().all(|tile| *tile != Tile(0, 0, 0))
-}
-
-/// Finds the best move using MCTS
-
-
-/// Simulates num_simulations games and returns the average score
-
-fn simulate_games(plateau: Plateau, deck: Deck) -> i32 {
-    let mut simulated_plateau = plateau.clone();
-    let simulated_deck = deck.clone();
-    let mut legal_moves = get_legal_moves(simulated_plateau.clone());
-
-    // Filter out invalid tiles (0, 0, 0)
-    let mut valid_tiles: Vec<Tile> = simulated_deck
-        .tiles
-        .iter()
-        .cloned()
-        .filter(|tile| *tile != Tile(0, 0, 0))
-        .collect();
-
-    let mut rng = rand::rng(); // Fixed: Use new API
-
-    while !is_plateau_full(&simulated_plateau) {
-        if legal_moves.is_empty() || valid_tiles.is_empty() {
-            break;
-        }
-
-        // Fixed: Use new rand API
-        let position_index = rng.random_range(0..legal_moves.len());
-        let position = legal_moves.swap_remove(position_index); // Swap-remove for O(1) removal
-
-        let tile_index = rng.random_range(0..valid_tiles.len());
-        let chosen_tile = valid_tiles.swap_remove(tile_index); // Swap-remove for O(1) removal
-
-        // Place the chosen tile
-        simulated_plateau.tiles[position] = chosen_tile;
-    }
-
-    result(&simulated_plateau) // Compute and return the result
-}
 
 
 
-/// Get all legal moves (empty positions) on the plateau
-fn get_legal_moves(plateau: Plateau) -> Vec<usize> {
-    plateau
-        .tiles
-        .iter()
-        .enumerate()
-        .filter_map(|(i, tile)| if *tile == Tile(0, 0, 0) { Some(i) } else { None })
-        .collect()
-}
\ No newline at end of file
diff --git a/src/mcts/algorithm.rs b/src/mcts/algorithm.rs
index e69de29..88c272f 100644
--- a/src/mcts/algorithm.rs
+++ b/src/mcts/algorithm.rs
@@ -0,0 +1,255 @@
+use std::collections::HashMap;
+use tch::IndexOp;
+use crate::game::deck::Deck;
+use crate::game::get_legal_moves::get_legal_moves;
+use crate::game::plateau::Plateau;
+use crate::game::plateau_is_full::is_plateau_full;
+use crate::game::remove_tile_from_deck::replace_tile_in_deck;
+use crate::game::simulate_game::simulate_games;
+use crate::game::tile::Tile;
+use crate::mcts::mcts_result::MCTSResult;
+use crate::neural::policy_value_net::{PolicyNet, ValueNet};
+use crate::neural::tensor_conversion::convert_plateau_to_tensor;
+use crate::scoring::scoring::result;
+use crate::strategy::position_evaluation::enhanced_position_evaluation;
+use crate::utils::random_index::random_index;
+
+pub fn mcts_find_best_position_for_tile_with_nn(
+    plateau: &mut Plateau,
+    deck: &mut Deck,
+    chosen_tile: Tile,
+    policy_net: &PolicyNet,
+    value_net: &ValueNet,
+    num_simulations: usize,
+    current_turn: usize,
+    total_turns: usize,
+) -> MCTSResult {
+    let legal_moves = get_legal_moves(plateau.clone());
+    if legal_moves.is_empty() {
+        return MCTSResult {
+            best_position: 0,
+            board_tensor: convert_plateau_to_tensor(
+                plateau,
+                &chosen_tile,
+                deck,
+                current_turn,
+                total_turns,
+            ),
+            subscore: 0.0,
+        };
+    }
+
+    let board_tensor =
+        convert_plateau_to_tensor(plateau, &chosen_tile, deck, current_turn, total_turns);
+    let policy_logits = policy_net.forward(&board_tensor, false);
+    let policy = policy_logits.log_softmax(-1, tch::Kind::Float).exp(); // Log-softmax improves numerical stability
+
+    let mut visit_counts: HashMap<usize, usize> = HashMap::new();
+    let mut total_scores: HashMap<usize, f64> = HashMap::new();
+    let mut ucb_scores: HashMap<usize, f64> = HashMap::new();
+    let mut total_visits: i32 = 0;
+
+    for &position in &legal_moves {
+        visit_counts.insert(position, 0);
+        total_scores.insert(position, 0.0);
+        ucb_scores.insert(position, f64::NEG_INFINITY);
+    }
+
+    let c_puct = if current_turn < 5 {
+        4.2 // Plus d'exploitation en d√©but de partie (positions critiques)
+    } else if current_turn > 15 {
+        3.0 // Plus d'exploration en fin de partie (adaptation)
+    } else {
+        3.8 // √âquilibre pour le milieu de partie
+    };
+
+    // **Compute ValueNet scores for all legal moves**
+    let mut value_estimates = HashMap::new();
+    let mut min_value = f64::INFINITY;
+    let mut max_value = f64::NEG_INFINITY;
+
+    for &position in &legal_moves {
+        let mut temp_plateau = plateau.clone();
+        let mut temp_deck = deck.clone();
+
+        temp_plateau.tiles[position] = chosen_tile;
+        temp_deck = replace_tile_in_deck(&temp_deck, &chosen_tile);
+        let board_tensor_temp = convert_plateau_to_tensor(
+            &temp_plateau,
+            &chosen_tile,
+            &temp_deck,
+            current_turn,
+            total_turns,
+        );
+
+        let pred_value = value_net
+            .forward(&board_tensor_temp, false)
+            .double_value(&[]);
+        let pred_value = pred_value.clamp(-1.0, 1.0);
+
+        // Track min and max for dynamic pruning
+        min_value = min_value.min(pred_value);
+        max_value = max_value.max(pred_value);
+
+        value_estimates.insert(position, pred_value);
+    }
+
+    // **Dynamic Pruning Strategy**
+    let value_threshold = if current_turn < 8 {
+        min_value + (max_value - min_value) * 0.1 // Garder plus de candidats en d√©but
+    } else {
+        min_value + (max_value - min_value) * 0.15 // Pruning moins agressif
+    };
+
+    for _ in 0..num_simulations {
+        let mut moves_with_prior: Vec<_> = legal_moves
+            .iter()
+            .filter(|&&pos| value_estimates[&pos] >= value_threshold) // Prune weak moves
+            .map(|&pos| (pos, policy.i((0, pos as i64)).double_value(&[])))
+            .collect();
+
+        moves_with_prior.sort_by(|a, b| b.1.partial_cmp(&a.1).unwrap_or(std::cmp::Ordering::Equal));
+
+        let top_k = usize::min(
+            moves_with_prior.len(),
+            ((total_visits as f64).sqrt() as usize).max(5),
+        );
+
+        let subset_moves: Vec<usize> = moves_with_prior
+            .iter()
+            .take(top_k)
+            .map(|&(pos, _)| pos)
+            .collect();
+
+        for &position in &subset_moves {
+            let mut temp_plateau = plateau.clone();
+            let mut temp_deck = deck.clone();
+
+            temp_plateau.tiles[position] = chosen_tile;
+            temp_deck = replace_tile_in_deck(&temp_deck, &chosen_tile);
+
+            let value_estimate = *value_estimates.get(&position).unwrap_or(&0.0);
+
+            // **Improved Adaptive Rollout Strategy**
+            let rollout_count = match value_estimate {
+                x if x > 8.0 => 2, // Very strong move -> minimal rollouts
+                x if x > 6.0 => 4, // Strong move -> fewer rollouts
+                x if x > 4.0 => 6, // Decent move -> moderate rollouts
+                _ => 8,            // Uncertain move -> more rollouts
+            };
+
+            let mut total_simulated_score = 0.0;
+
+            for _ in 0..rollout_count {
+                let mut lookahead_plateau = temp_plateau.clone();
+                let mut lookahead_deck = temp_deck.clone();
+
+                // üîÆ √âtape 1.1 ‚Äî Tirer une tuile hypoth√©tique (T2)
+                if lookahead_deck.tiles.is_empty() {
+                    continue;
+                }
+                let tile2_index = random_index(lookahead_deck.tiles.len());
+                let tile2 = lookahead_deck.tiles[tile2_index];
+
+                // üîç √âtape 1.2 ‚Äî Simuler tous les placements possibles de cette tuile
+                let second_moves = get_legal_moves(lookahead_plateau.clone());
+
+                let mut best_score_for_tile2: f64 = 0.0;
+
+                for &pos2 in &second_moves {
+                    let mut plateau2 = lookahead_plateau.clone();
+                    let mut deck2 = lookahead_deck.clone();
+
+                    plateau2.tiles[pos2] = tile2;
+                    deck2 = replace_tile_in_deck(&deck2, &tile2);
+
+                    let score = simulate_games(plateau2.clone(), deck2.clone()) as f64;
+                    best_score_for_tile2 = best_score_for_tile2.max(score);
+                }
+
+                total_simulated_score += best_score_for_tile2;
+            }
+
+            let simulated_score = total_simulated_score / rollout_count as f64;
+
+            let visits = visit_counts.entry(position).or_insert(0);
+            *visits += 1;
+            total_visits += 1;
+
+            let total_score = total_scores.entry(position).or_insert(0.0);
+            *total_score += simulated_score as f64;
+
+            let exploration_param = c_puct * (total_visits as f64).ln() / (1.0 + *visits as f64);
+            let prior_prob = policy.i((0, position as i64)).double_value(&[]);
+            let average_score = *total_score / (*visits as f64);
+            // üß™ Reduce weight of rollout average
+            let enhanced_eval =
+                enhanced_position_evaluation(&temp_plateau, position, &chosen_tile, current_turn);
+
+            // Int√©grer dans le calcul UCB
+            let mut ucb_score = (average_score * 0.5)
+                + exploration_param * (prior_prob.sqrt())
+                + 0.25 * value_estimate.clamp(0.0, 2.0)
+                + 0.1 * enhanced_eval; // Nouveau facteur d'√©valuation
+
+            // üî• Explicit Priority Logic HERE üî•
+            // 1Ô∏è‚É£ Ajoute cette fonction en dehors de ta mcts_find_best_position_for_tile_with_nn
+
+            // 2Ô∏è‚É£ Int√®gre ceci dans ta boucle ucb_scores, juste apr√®s le boost fixe
+
+            if chosen_tile.0 == 9 && [7, 8, 9, 10, 11].contains(&position) {
+                ucb_score += 10000.0; // double boost
+            } else if chosen_tile.0 == 5 && [3, 4, 5, 6, 12, 13, 14, 15].contains(&position) {
+                ucb_score += 8000.0;
+            } else if chosen_tile.0 == 1 && [0, 1, 2, 16, 17, 18].contains(&position) {
+                ucb_score += 6000.0;
+            }
+
+            // üî• Alignment Priority Logic üî•
+
+            ucb_scores.insert(position, ucb_score);
+        }
+    }
+
+    // Select the move with the highest UCB score
+    let best_position = legal_moves
+        .into_iter()
+        .max_by(|&a, &b| {
+            ucb_scores
+                .get(&a)
+                .unwrap_or(&f64::NEG_INFINITY)
+                .partial_cmp(ucb_scores.get(&b).unwrap_or(&f64::NEG_INFINITY))
+                .unwrap_or(std::cmp::Ordering::Equal)
+        })
+        .unwrap_or(0);
+
+    // **NEW: Simulate the Rest of the Game to Get Final Score**
+    let mut final_plateau = plateau.clone();
+    let mut final_deck = deck.clone();
+    final_plateau.tiles[best_position] = chosen_tile;
+    final_deck = replace_tile_in_deck(&final_deck, &chosen_tile);
+
+    while !is_plateau_full(&final_plateau) {
+        let tile_index = random_index(final_deck.tiles.len());
+        let random_tile = final_deck.tiles[tile_index];
+
+        let available_moves = get_legal_moves(final_plateau.clone());
+        if available_moves.is_empty() {
+            break;
+        }
+
+        let random_position = available_moves[random_index(available_moves.len())];
+        final_plateau.tiles[random_position] = random_tile;
+        final_deck = replace_tile_in_deck(&final_deck, &random_tile);
+    }
+
+    let final_score = result(&final_plateau); // Get actual game score
+
+    log::info!("ü§ñ Pos:{} Score:{}", best_position, final_score as i32);
+
+    MCTSResult {
+        best_position,
+        board_tensor,
+        subscore: final_score as f64, // Store real final score, not UCB score
+    }
+}
\ No newline at end of file
diff --git a/src/mcts/mod.rs b/src/mcts/mod.rs
index e69de29..9a9906a 100644
--- a/src/mcts/mod.rs
+++ b/src/mcts/mod.rs
@@ -0,0 +1,3 @@
+pub mod mcts_node;
+pub mod mcts_result;
+pub mod algorithm;
\ No newline at end of file
diff --git a/src/mcts_vs_human.rs b/src/mcts_vs_human.rs
index 95303c3..bec6730 100644
--- a/src/mcts_vs_human.rs
+++ b/src/mcts_vs_human.rs
@@ -1,19 +1,20 @@
-use crate::create_shuffle_deck::create_shuffle_deck;
-use crate::create_plateau_empty::create_plateau_empty;
-use crate::remove_tile_from_deck::replace_tile_in_deck;
+use crate::game::create_deck::create_deck;
+use crate::game::remove_tile_from_deck::replace_tile_in_deck;
 use crate::generate_tile_image_names;
-use crate::result::result;
-use crate::test::{Deck, Plateau, Tile};
-use crate::{mcts_find_best_position_for_tile_with_nn, is_plateau_full};
+use crate::{is_plateau_full};
 
 use serde_json::json;
 use tokio_tungstenite::tungstenite::protocol::Message;
 use tokio_tungstenite::WebSocketStream;
 use tokio::net::TcpStream;
-use futures_util::{StreamExt, SinkExt};
+use futures_util::{SinkExt, StreamExt};
 use futures_util::stream::SplitSink;
 use rand::Rng;
-use crate::policy_value_net::{PolicyNet, ValueNet};
+use crate::game::plateau::create_plateau_empty;
+use crate::neural::policy_value_net::{PolicyNet, ValueNet};
+use crate::game::tile::Tile;
+use crate::mcts::algorithm::mcts_find_best_position_for_tile_with_nn;
+use crate::scoring::scoring::result;
 
 pub async fn play_mcts_vs_human(
     policy_net: &PolicyNet,
@@ -22,7 +23,7 @@ pub async fn play_mcts_vs_human(
     write: &mut SplitSink<WebSocketStream<TcpStream>, Message>,
     read: &mut (impl StreamExt<Item = Result<Message, tokio_tungstenite::tungstenite::Error>> + Unpin),
 ) {
-    let mut deck = create_shuffle_deck();
+    let mut deck = create_deck();
     let mut plateau_human = create_plateau_empty();
     let mut plateau_mcts = create_plateau_empty();
 
diff --git a/src/mod.rs b/src/mod.rs
index e69de29..909b9ab 100644
--- a/src/mod.rs
+++ b/src/mod.rs
@@ -0,0 +1,11 @@
+pub mod game;
+pub mod mcts;
+
+pub mod utils;
+pub mod neural;
+pub mod strategy;
+pub mod scoring;
+
+pub mod data;
+
+pub mod training;
\ No newline at end of file
diff --git a/src/neural/mod.rs b/src/neural/mod.rs
index e69de29..9082c6e 100644
--- a/src/neural/mod.rs
+++ b/src/neural/mod.rs
@@ -0,0 +1,4 @@
+pub mod res_net_block;
+pub mod policy_value_net;
+pub mod tensor_conversion;
+pub mod training;
\ No newline at end of file
diff --git a/src/neural/res_net_block.rs b/src/neural/res_net_block.rs
index e9f3411..e40b131 100644
--- a/src/neural/res_net_block.rs
+++ b/src/neural/res_net_block.rs
@@ -1,6 +1,6 @@
 use tch::{nn, Tensor};
 
-use crate::policy_value_net::initialize_weights;
+use crate::neural::policy_value_net::initialize_weights;
 
 /// Residual Block
 pub struct ResNetBlock {
diff --git a/src/neural/tensor_conversion.rs b/src/neural/tensor_conversion.rs
index e69de29..95b6d19 100644
--- a/src/neural/tensor_conversion.rs
+++ b/src/neural/tensor_conversion.rs
@@ -0,0 +1,84 @@
+use tch::Tensor;
+use crate::game::deck::Deck;
+use crate::game::plateau::Plateau;
+use crate::game::tile::Tile;
+
+pub fn convert_plateau_to_tensor(
+    plateau: &Plateau,
+    _tile: &Tile, // Add underscore prefix
+    _deck: &Deck, // Add underscore prefix
+    current_turn: usize,
+    total_turns: usize,
+) -> Tensor {
+    let mut features = vec![0.0; 5 * 47]; // 5 channels
+
+    // Channel 1-3: Plateau (only use plateau data, not tile/deck)
+    for (i, t) in plateau.tiles.iter().enumerate() {
+        if i < 19 {
+            features[i] = (t.0 as f32 / 10.0).clamp(0.0, 1.0);
+            features[47 + i] = (t.1 as f32 / 10.0).clamp(0.0, 1.0);
+            features[2 * 47 + i] = (t.2 as f32 / 10.0).clamp(0.0, 1.0);
+        }
+    }
+
+    // Channel 4: Score Potential for each position
+    let potential_scores = compute_potential_scores(plateau);
+    for i in 0..19 {
+        features[3 * 47 + i] = potential_scores[i];
+    }
+
+    // Channel 5: Current Turn
+    let turn_normalized = current_turn as f32 / total_turns as f32;
+    for i in 0..19 {
+        features[4 * 47 + i] = turn_normalized;
+    }
+
+    Tensor::from_slice(&features).view([1, 5, 47, 1])
+}
+fn compute_potential_scores(plateau: &Plateau) -> Vec<f32> {
+    let mut scores = vec![0.0; 19]; // Potential score for each position
+
+    let patterns: Vec<(&[usize], i32, Box<dyn Fn(&Tile) -> i32>)> = vec![
+        (&[0, 1, 2], 3, Box::new(|tile: &Tile| tile.0)),
+        (&[3, 4, 5, 6], 4, Box::new(|tile: &Tile| tile.0)),
+        (&[7, 8, 9, 10, 11], 5, Box::new(|tile: &Tile| tile.0)),
+        (&[12, 13, 14, 15], 4, Box::new(|tile: &Tile| tile.0)),
+        (&[16, 17, 18], 3, Box::new(|tile: &Tile| tile.0)),
+        (&[0, 3, 7], 3, Box::new(|tile: &Tile| tile.1)),
+        (&[1, 4, 8, 12], 4, Box::new(|tile: &Tile| tile.1)),
+        (&[2, 5, 9, 13, 16], 5, Box::new(|tile: &Tile| tile.1)),
+        (&[6, 10, 14, 17], 4, Box::new(|tile: &Tile| tile.1)),
+        (&[11, 15, 18], 3, Box::new(|tile: &Tile| tile.1)),
+        (&[7, 12, 16], 3, Box::new(|tile: &Tile| tile.2)),
+        (&[3, 8, 13, 17], 4, Box::new(|tile: &Tile| tile.2)),
+        (&[0, 4, 9, 14, 18], 5, Box::new(|tile: &Tile| tile.2)),
+        (&[1, 5, 10, 15], 4, Box::new(|tile: &Tile| tile.2)),
+        (&[2, 6, 11], 3, Box::new(|tile: &Tile| tile.2)),
+    ];
+
+    for (indices, multiplier, selector) in &patterns {
+        let mut filled_values = Vec::new();
+        let mut empty_positions = Vec::new();
+
+        for &pos in *indices {
+            if plateau.tiles[pos] == Tile(0, 0, 0) {
+                empty_positions.push(pos);
+            } else {
+                filled_values.push(selector(&plateau.tiles[pos]) as f32);
+            }
+        }
+
+        // If at least one tile is placed in the pattern
+        if !filled_values.is_empty() {
+            let avg_filled_value = filled_values.iter().sum::<f32>() / filled_values.len() as f32;
+            let potential_score = avg_filled_value * (*multiplier as f32);
+
+            for &pos in empty_positions.iter() {
+                scores[pos] += potential_score / empty_positions.len() as f32; // Distribute potential score
+            }
+        }
+    }
+
+    scores
+}
+
diff --git a/src/neural/training/gradient_clipping.rs b/src/neural/training/gradient_clipping.rs
index e69de29..18052aa 100644
--- a/src/neural/training/gradient_clipping.rs
+++ b/src/neural/training/gradient_clipping.rs
@@ -0,0 +1,95 @@
+//! Gestion des gradients et clipping pour stabiliser l'entra√Ænement
+
+use tch::nn;
+
+/// R√©sultat du clipping des gradients
+#[derive(Debug)]
+pub struct GradientClippingResult {
+    pub max_grad_value: f64,
+    pub max_grad_policy: f64,
+}
+
+/// Applique un clipping am√©lior√© des gradients pour les r√©seaux value et policy
+pub fn enhanced_gradient_clipping(
+    vs_value: &nn::VarStore,
+    vs_policy: &nn::VarStore,
+) -> GradientClippingResult {
+    let max_grad_value = clip_value_network_gradients(vs_value);
+    let max_grad_policy = clip_policy_network_gradients(vs_policy);
+
+    log_gradient_norms(max_grad_value, max_grad_policy);
+
+    GradientClippingResult {
+        max_grad_value,
+        max_grad_policy,
+    }
+}
+
+/// Applique un clipping agressif pour le r√©seau de valeur
+fn clip_value_network_gradients(vs_value: &nn::VarStore) -> f64 {
+    let mut max_grad_value: f64 = 0.0;
+
+    tch::no_grad(|| {
+        for (_name, tensor) in vs_value.variables() {
+            if tensor.grad().defined() {
+                let grad_norm = tensor.grad().norm().double_value(&[]);
+                max_grad_value = max_grad_value.max(grad_norm);
+
+                // Clipping tr√®s agressif pour stabilit√©
+                tensor.grad().clamp_(-0.5, 0.5);
+            }
+        }
+    });
+
+    max_grad_value
+}
+
+/// Applique un clipping mod√©r√© pour le r√©seau de policy
+fn clip_policy_network_gradients(vs_policy: &nn::VarStore) -> f64 {
+    let mut max_grad_policy: f64 = 0.0;
+
+    tch::no_grad(|| {
+        for (_name, tensor) in vs_policy.variables() {
+            if tensor.grad().defined() {
+                let grad_norm = tensor.grad().norm().double_value(&[]);
+                max_grad_policy = max_grad_policy.max(grad_norm);
+
+                // Clipping mod√©r√©
+                tensor.grad().clamp_(-1.0, 1.0);
+            }
+        }
+    });
+
+    max_grad_policy
+}
+
+/// Log les normes de gradients si elles sont √©lev√©es
+fn log_gradient_norms(max_grad_value: f64, max_grad_policy: f64) {
+    if max_grad_value > 1.0 {
+        log::warn!("üî• Value grad norm: {:.3}", max_grad_value);
+    }
+    if max_grad_policy > 2.0 {
+        log::warn!("üî• Policy grad norm: {:.3}", max_grad_policy);
+    }
+}
+
+/// Version simple du clipping des gradients
+pub fn simple_gradient_clipping(
+    vs: &nn::VarStore,
+    max_norm: f64,
+) -> f64 {
+    let mut max_grad: f64 = 0.0;
+
+    tch::no_grad(|| {
+        for (_name, tensor) in vs.variables() {
+            if tensor.grad().defined() {
+                let grad_norm = tensor.grad().norm().double_value(&[]);
+                max_grad = max_grad.max(grad_norm);
+
+                tensor.grad().clamp_(-max_norm, max_norm);
+            }
+        }
+    });
+
+    max_grad
+}
\ No newline at end of file
diff --git a/src/neural/training/mod.rs b/src/neural/training/mod.rs
index e69de29..471498a 100644
--- a/src/neural/training/mod.rs
+++ b/src/neural/training/mod.rs
@@ -0,0 +1,5 @@
+//! Module d'entra√Ænement - Optimisation et stabilisation
+
+pub mod gradient_clipping;
+pub mod normalization;
+pub mod trainer;
diff --git a/src/neural/training/normalization.rs b/src/neural/training/normalization.rs
index e69de29..c2a44c6 100644
--- a/src/neural/training/normalization.rs
+++ b/src/neural/training/normalization.rs
@@ -0,0 +1,66 @@
+//! Normalisation robuste des donn√©es pour l'entra√Ænement
+
+use tch::{Tensor, IndexOp};
+
+/// Applique une normalisation robuste bas√©e sur la m√©diane (MAD)
+pub fn robust_state_normalization(state: &Tensor) -> Tensor {
+    let clamped = clamp_extreme_values(state);
+    let median = calculate_median(&clamped);
+    let mad = calculate_mad(&clamped, median);
+
+    normalize_with_mad(&clamped, median, mad)
+}
+
+/// Clamp les valeurs extr√™mes pour √©viter les outliers
+fn clamp_extreme_values(state: &Tensor) -> Tensor {
+    state.clamp(-10.0, 10.0)
+}
+
+/// Calcule la m√©diane d'un tensor
+fn calculate_median(tensor: &Tensor) -> f64 {
+    let flattened = tensor.view(-1);
+    let sorted = flattened.sort(0, false).0;
+    let median_idx = sorted.size()[0] / 2;
+    sorted.i(median_idx).double_value(&[])
+}
+
+/// Calcule la Median Absolute Deviation (MAD)
+fn calculate_mad(tensor: &Tensor, median: f64) -> f64 {
+    let flattened = tensor.view(-1);
+    let deviations = (flattened - median).abs();
+    let sorted_dev = deviations.sort(0, false).0;
+    let median_idx = sorted_dev.size()[0] / 2;
+    sorted_dev.i(median_idx).double_value(&[]) * 1.4826
+}
+
+/// Normalise avec MAD au lieu de l'√©cart-type standard
+fn normalize_with_mad(tensor: &Tensor, median: f64, mad: f64) -> Tensor {
+    let normalized = if mad > 1e-6 {
+        (tensor - median) / mad.max(1e-6)
+    } else {
+        tensor - median
+    };
+
+    // Clamp final pour √©viter les valeurs extr√™mes
+    normalized.clamp(-3.0, 3.0)
+}
+
+/// Version simple de normalisation z-score
+pub fn simple_normalization(tensor: &Tensor) -> Tensor {
+    let mean = tensor.mean(tch::Kind::Float);
+    let std = tensor.std(false).clamp_min(1e-8);
+    (tensor - mean) / std
+}
+
+/// Normalisation min-max
+pub fn min_max_normalization(tensor: &Tensor) -> Tensor {
+    let min_val = tensor.min();
+    let max_val = tensor.max();
+    let range = max_val - &min_val;
+
+    if range.double_value(&[]) > 1e-8 {
+        (tensor - min_val) / range
+    } else {
+        tensor.shallow_clone()
+    }
+}
\ No newline at end of file
diff --git a/src/neural/training/trainer.rs b/src/neural/training/trainer.rs
index e69de29..95a6d39 100644
--- a/src/neural/training/trainer.rs
+++ b/src/neural/training/trainer.rs
@@ -0,0 +1,146 @@
+use tch::{nn, IndexOp, Tensor};
+use tch::nn::Optimizer;
+use crate::mcts::mcts_result::MCTSResult;
+use crate::neural::policy_value_net::{PolicyNet, ValueNet};
+use crate::neural::training::gradient_clipping::enhanced_gradient_clipping;
+use crate::neural::training::normalization::robust_state_normalization;
+
+pub fn train_network_with_game_data(
+    vs_policy: &nn::VarStore,
+    vs_value: &nn::VarStore,
+    game_data: &[MCTSResult],
+    discount_factor: f64,
+    policy_net: &PolicyNet,
+    value_net: &ValueNet,
+    optimizer_policy: &mut Optimizer,
+    optimizer_value: &mut Optimizer,
+) {
+    // Hyperparameters
+    let entropy_weight = 0.05;
+    let gamma = 0.99;
+    let epsilon = 1e-8;
+
+    // Initialize accumulators
+    let mut predictions = Vec::new();
+    let mut targets = Vec::new();
+    let mut total_policy_loss = Tensor::zeros(&[], tch::kind::FLOAT_CPU);
+    let mut total_value_loss = Tensor::zeros(&[], tch::kind::FLOAT_CPU);
+    let mut total_entropy_loss = Tensor::zeros(&[], tch::kind::FLOAT_CPU);
+
+    // Initialize trajectory rewards and discounted sum
+    let mut trajectory_rewards = Vec::new();
+    let mut discounted_sum = Tensor::zeros(&[], (tch::Kind::Float, tch::Device::Cpu));
+
+    // === Training Loop ===
+    for (step, result) in game_data.iter().rev().enumerate() {
+        // üõë No Normalization: Use raw tensor
+        let state = result.board_tensor.shallow_clone();
+        let normalized_state = robust_state_normalization(&state);
+
+        // Forward pass through networks with normalized state
+        let pred_policy = policy_net.forward(&normalized_state, true).clamp_min(1e-7);
+        let pred_value = value_net.forward(&normalized_state, true);
+
+        // Forward pass through networks with normalized state
+        // Normalize reward: divide by a constant max value (e.g., 100)
+        let reward = Tensor::from(result.subscore).to_kind(tch::Kind::Float) / 100.0;
+        let gamma_tensor = Tensor::from_slice(&[gamma]).to_kind(tch::Kind::Float);
+
+        // ‚úÖ NaN & Inf Check for reward
+        if reward.isnan().any().double_value(&[]) > 0.0
+            || reward.isinf().any().double_value(&[]) > 0.0
+        {
+            log::error!("‚ö†Ô∏è NaN or Inf detected in reward at step {}", step);
+            continue;
+        }
+
+        // Update discounted sum with normalized reward
+        discounted_sum = reward + gamma_tensor * discounted_sum;
+
+        // ‚úÖ NaN & Inf Check for discounted sum
+        if discounted_sum.isnan().any().double_value(&[]) > 0.0
+            || discounted_sum.isinf().any().double_value(&[]) > 0.0
+        {
+            log::error!("‚ö†Ô∏è NaN or Inf detected in discounted sum at step {}", step);
+            continue;
+        }
+
+        // Store the value for analysis
+        trajectory_rewards.push(discounted_sum.double_value(&[]));
+
+        // Generate target tensor directly from discounted sum
+        let discounted_reward = discounted_sum.shallow_clone();
+
+        // Append for later analysis
+        predictions.push(pred_value.double_value(&[]));
+        targets.push(discounted_reward.double_value(&[]));
+
+        // === Compute Losses ===
+        // Policy loss
+        let best_position = result.best_position as i64;
+        let target_policy = Tensor::zeros(&[1, pred_policy.size()[1]], tch::kind::FLOAT_CPU);
+        target_policy.i((0, best_position)).fill_(1.0);
+        let log_policy = pred_policy.log();
+        let policy_loss = -(target_policy * log_policy.shallow_clone()).sum(tch::Kind::Float);
+        total_policy_loss += policy_loss;
+
+        // Entropy loss
+        let entropy_loss = -(pred_policy * (log_policy + epsilon)).sum(tch::Kind::Float);
+        total_entropy_loss += entropy_loss;
+
+        // Value loss (Huber loss for better stability)
+        let diff = discounted_reward.shallow_clone() - pred_value.shallow_clone();
+        let abs_diff = diff.abs();
+        let delta = 1.0;
+        let value_loss = abs_diff.le(delta).to_kind(tch::Kind::Float) * 0.5 * &diff * &diff
+            + abs_diff.gt(delta).to_kind(tch::Kind::Float) * (delta * (&abs_diff - 0.5 * delta));
+        total_value_loss += value_loss.mean(tch::Kind::Float);
+    }
+
+    // Fix: Add explicit type annotation for total_loss
+    let total_loss: Tensor = total_policy_loss.shallow_clone()
+        + total_value_loss.shallow_clone()
+        + (entropy_weight * total_entropy_loss.shallow_clone());
+
+    // Log the loss before backpropagation
+    log::info!(
+        "üí° Total Loss before backward: {:.4}",
+        total_loss.double_value(&[])
+    );
+
+    // ‚úÖ Enhanced NaN and Inf check before backpropagation
+    if total_loss.isnan().any().double_value(&[]) > 0.0 {
+        log::error!("‚ö†Ô∏è NaN detected in total loss! Skipping backpropagation.");
+        return;
+    }
+    if total_loss.isinf().any().double_value(&[]) > 0.0 {
+        log::error!("‚ö†Ô∏è Inf detected in total loss! Skipping backpropagation.");
+        return;
+    }
+
+    // Check if total_loss requires gradients before calling backward
+    if !total_loss.requires_grad() {
+        log::error!("‚ö†Ô∏è Total loss does not require gradients! Skipping backpropagation.");
+        return;
+    }
+
+    total_loss.backward();
+
+    // Utilisez :
+    let gradient_result = enhanced_gradient_clipping(vs_value, vs_policy);
+    let _max_grad_value = gradient_result.max_grad_value;
+    let _max_grad_policy = gradient_result.max_grad_policy;
+
+    // === Optimizer Step ===
+    optimizer_policy.step();
+    optimizer_policy.zero_grad();
+    optimizer_value.step();
+    optimizer_value.zero_grad();
+
+    log::info!(
+        "üéØ Update Complete | Policy Loss: {:.4}, Value Loss: {:.4}, Entropy Loss: {:.4}",
+        total_policy_loss.double_value(&[]),
+        total_value_loss.double_value(&[]),
+        total_entropy_loss.double_value(&[])
+    );
+}
\ No newline at end of file
diff --git a/src/scoring/mod.rs b/src/scoring/mod.rs
index 4c07743..80d0f3d 100644
--- a/src/scoring/mod.rs
+++ b/src/scoring/mod.rs
@@ -1,2 +1 @@
 pub mod scoring;
-pub mod results;
diff --git a/src/strategy/mod.rs b/src/strategy/mod.rs
index e69de29..e775ccb 100644
--- a/src/strategy/mod.rs
+++ b/src/strategy/mod.rs
@@ -0,0 +1 @@
+pub mod position_evaluation;
\ No newline at end of file
diff --git a/src/strategy/position_evaluation.rs b/src/strategy/position_evaluation.rs
index e69de29..69e2403 100644
--- a/src/strategy/position_evaluation.rs
+++ b/src/strategy/position_evaluation.rs
@@ -0,0 +1,84 @@
+use crate::game::plateau::Plateau;
+use crate::game::tile::Tile;
+use crate::scoring::scoring::compute_alignment_score;
+
+// Version simplifi√©e qui se concentre sur les positions strat√©giques
+pub fn calculate_line_completion_bonus(plateau: &Plateau, position: usize, tile: &Tile) -> f64 {
+    let mut bonus = 0.0;
+
+    // Bonus bas√© sur les positions strat√©giques identifi√©es dans tes donn√©es
+    bonus += match position {
+        8 => 5.0,                 // Position 8: 150.6 moyenne - excellente
+        14 => 4.0,                // Position 14: 147.7 moyenne - tr√®s bonne
+        2 => 4.0,                 // Position 2: 147.1 moyenne - tr√®s bonne
+        5 => 3.0,                 // Position 5: 143.6 moyenne - bonne
+        11 => 3.0,                // Position 11: 142.9 moyenne - bonne
+        10 => 2.0,                // Position 10: 140.8 moyenne - correcte
+        13 => 2.0,                // Position 13: 140.2 moyenne - correcte
+        1 | 4 | 6 | 9 | 0 => 1.0, // Positions moyennes
+        12 | 15 | 16 => 0.5,      // Positions plus faibles
+        7 | 17 => 0.0,            // Positions les plus faibles
+        _ => 0.0,
+    };
+
+    // Bonus pour les valeurs de tuiles √©lev√©es (plus de points potentiels)
+    let tile_value_bonus = ((tile.0 + tile.1) as f64) * 0.1;
+    bonus += tile_value_bonus;
+
+    // Bonus pour la coh√©rence des couleurs/formes
+    if tile.0 == tile.1 {
+        bonus += 1.0; // Tuiles avec m√™me couleur et forme
+    }
+
+    // Bonus central l√©g√®rement plus complexe
+    let row = position / 3;
+    let col = position % 3;
+    if row >= 1 && row <= 4 && col >= 1 && col <= 1 {
+        bonus += 2.0; // Zone centrale du plateau
+    }
+
+    bonus
+}
+
+// ============================================================================
+// ALTERNATIVE PLUS SIMPLE (Si la version ci-dessus pose encore probl√®me)
+// ============================================================================
+
+// Si vous pr√©f√©rez une version plus simple, utilisez celle-ci:
+
+pub fn enhanced_position_evaluation(
+    plateau: &Plateau,
+    position: usize,
+    tile: &Tile,
+    current_turn: usize,
+) -> f64 {
+    // Score de base alignement (votre fonction existante)
+    let alignment_score = compute_alignment_score(plateau, position, tile);
+
+    // Bonus pour positions centrales strat√©giques en d√©but de partie
+    let position_bonus = if current_turn < 8 {
+        match position {
+            7 | 8 | 9 | 10 | 11 => 5.0,           // Ligne centrale - critique
+            4 | 5 | 6 | 12 | 13 | 14 | 15 => 3.0, // Positions strat√©giques
+            _ => 0.0,
+        }
+    } else {
+        0.0 // En fin de partie, seul l'alignement compte
+    };
+
+    // Malus pour positions coins/bords si d√©but de partie
+    let position_malus = if current_turn < 5 {
+        match position {
+            0 | 2 | 16 | 18 => -2.0, // Coins - √† √©viter en d√©but
+            1 | 17 => -1.0,          // Bords
+            _ => 0.0,
+        }
+    } else {
+        0.0
+    };
+
+    // Bonus pour compl√©tion de lignes
+    let completion_bonus = calculate_line_completion_bonus(plateau, position, tile);
+
+    alignment_score + position_bonus + position_malus + completion_bonus
+}
diff --git a/src/test.rs b/src/test.rs
index 2bb8cd8..896f3e0 100644
--- a/src/test.rs
+++ b/src/test.rs
@@ -1,31 +1,5 @@
-
-#[derive(Debug, Clone, PartialEq)]
-pub struct MCTSNode {
-    pub state: GameState,             // Current game state
-    pub visits: usize,                // Number of visits
-    pub value: f64,                   // Total value of the node
-    pub children: Vec<MCTSNode>,      // Child nodes
-    pub parent: Option<*mut MCTSNode>, // Pointer to the parent node (raw pointer to allow mutation)
-}
-
-
-#[derive(Debug, Clone, PartialEq, Copy,Hash,Eq)]
-pub(crate) struct Tile(pub i32, pub i32, pub i32);
-#[derive(Debug, Clone, PartialEq)]
-pub(crate) struct Plateau{
-    pub(crate) tiles: Vec<Tile>,
-}
-
-#[derive(Debug, Clone, PartialEq)]
-pub struct GameState {
-    pub plateau: Plateau,
-    pub deck: Deck,
-}
-
-#[derive(Debug, Clone, PartialEq)]
-pub struct Deck{
-    pub(crate) tiles: Vec<Tile>,
-}
+use crate::game::plateau::Plateau;
+use crate::game::tile::Tile;
 
 #[cfg(test)]
 pub(crate) mod tests {
@@ -75,7 +49,7 @@ pub(crate) mod tests {
 
     pub fn create_game_state() -> GameState {
         let plateau = create_plateau_empty();
-        let deck = create_shuffle_deck();
+        let deck = create_deck();
         GameState { plateau, deck }
     }
 
@@ -155,21 +129,25 @@ pub(crate) mod tests {
             (*current).value += score;
         }
     }
-    use crate::remove_tile_from_deck::remove_tile_from_deck;
+    use crate::game::remove_tile_from_deck::remove_tile_from_deck;
 use super::*;
 
 
     use rand::Rng;
-    use crate::create_plateau_empty::create_plateau_empty;
-    use crate::create_shuffle_deck::create_shuffle_deck;
+    use crate::game::create_deck::create_deck;
     use crate::{convert_plateau_to_tensor, get_legal_moves, is_plateau_full, simulate_games};
-    use crate::result::result;
-    use crate::test::{  Deck, GameState,  MCTSNode, Plateau, Tile};
+    use crate::game::deck::Deck;
+    use crate::game::game_state::GameState;
+    use crate::mcts::mcts_node::MCTSNode;
+    use crate::game::plateau::{create_plateau_empty, Plateau};
+    use crate::game::tile::Tile;
+    use crate::scoring::scoring::result;
+
     #[test]
     fn test_placement_tuile_valide_take_it_easy() {
         let mut plateau:Plateau=create_plateau_empty();
-        let deckSfuffle:Deck= create_shuffle_deck();
-        let tuile = deckSfuffle.tiles[5].clone();
+        let shuffled_deck:Deck= create_deck();
+        let tuile = shuffled_deck.tiles[5].clone();
         assert!(placer_tile(&mut plateau, tuile.clone(), 1));
         assert_eq!(plateau.tiles[1], tuile);
     }
@@ -203,7 +181,7 @@ use super::*;
     #[test]
     fn test_simulate_games() {
         let plateau = create_plateau_empty();
-        let deck = create_shuffle_deck();
+        let deck = create_deck();
         let num_simulations = 10;
 
         let avg_score = simulate_games(plateau, deck);
@@ -213,7 +191,7 @@ use super::*;
     fn test_convert_plateau_to_tensor() {
         let plateau = create_plateau_empty();
         let tile = Tile(1, 2, 3);
-        let deck = create_shuffle_deck();
+        let deck = create_deck();
 
         let tensor = convert_plateau_to_tensor(&plateau, &tile, &deck, /* usize */0, 19/* usize */);
         assert_eq!(tensor.size(), vec![1, 3, 5, 5]); // Ensure the tensor has the correct shape
@@ -224,7 +202,7 @@ use super::*;
     #[test]
     fn test_placement_tuile_not_valide_take_it_easy() {
         let mut plateau:Plateau=create_plateau_empty();
-        let deckSfuffle:Deck= create_shuffle_deck();
+        let deckSfuffle:Deck= create_deck();
         let tile = deckSfuffle.tiles[5].clone();
         assert!(placer_tile(&mut plateau, tile.clone(), 1));
         assert_eq!(plateau.tiles[1], tile);
@@ -234,7 +212,7 @@ use super::*;
     #[test]
     fn test_choir_aleatorytile() {
         // Cr√©e un deck
-        let deck_shuffle: Deck = create_shuffle_deck();
+        let deck_shuffle: Deck = create_deck();
 
         // G√©n√®re un index al√©atoire
         let mut rng = rand::thread_rng();
@@ -258,7 +236,7 @@ use super::*;
         use rand::Rng; // Pour g√©n√©rer un indice al√©atoire
 
         // Cr√©e un deck initial
-        let deck_shuffle: Deck = create_shuffle_deck();
+        let deck_shuffle: Deck = create_deck();
 
         // G√©n√®re un indice al√©atoire
         let mut rng = rand::thread_rng();
@@ -282,7 +260,7 @@ use super::*;
     }
     #[test]
     fn test_remplir_plateau_take_it_easy() {
-        let mut deck = create_shuffle_deck();
+        let mut deck = create_deck();
         let mut plateau = create_plateau_empty();
 
         // Remplir le plateau
@@ -300,7 +278,7 @@ use super::*;
     }
     #[test]
     fn test_remplir_plateau_take_it_easy_count_first_3_plateau_3_1() {
-        let mut deck = create_shuffle_deck();
+        let mut deck = create_deck();
         let mut plateau = create_plateau_empty();
         placer_tile(&mut plateau, deck.tiles[0].clone(), 0);
         placer_tile(&mut plateau, deck.tiles[1].clone(), 1);
@@ -310,7 +288,7 @@ use super::*;
     }
     #[test]
     fn test_remplir_plateau_take_it_easy_count_first_3_plateau_3_2() {
-        let mut deck = create_shuffle_deck();
+        let mut deck = create_deck();
         let mut plateau = create_plateau_empty();
         placer_tile(&mut plateau, deck.tiles[9].clone(), 0);
         placer_tile(&mut plateau, deck.tiles[10].clone(), 1);
@@ -320,7 +298,7 @@ use super::*;
     }
     #[test]
     fn test_remplir_plateau_take_it_easy_count_2_column_plateau_4_2() {
-        let mut deck = create_shuffle_deck();
+        let mut deck = create_deck();
         let mut plateau = create_plateau_empty();
         placer_tile(&mut plateau, deck.tiles[9].clone(), 3);
         placer_tile(&mut plateau, deck.tiles[10].clone(), 4);
@@ -334,7 +312,7 @@ use super::*;
     }
     #[test]
     fn test_remplir_plateau_take_it_easy_count_column_center_plateau_5_2() {
-        let mut deck = create_shuffle_deck();
+        let mut deck = create_deck();
         let mut plateau = create_plateau_empty();
         placer_tile(&mut plateau, deck.tiles[9].clone(), 7);
         placer_tile(&mut plateau, deck.tiles[10].clone(), 8);
@@ -349,7 +327,7 @@ use super::*;
     }
     #[test]
     fn test_remplir_plateau_take_it_easy_count_column_4_plateau_4_2() {
-        let mut deck = create_shuffle_deck();
+        let mut deck = create_deck();
         let mut plateau = create_plateau_empty();
         placer_tile(&mut plateau, deck.tiles[9].clone(), 12);
         placer_tile(&mut plateau, deck.tiles[10].clone(), 13);
@@ -363,7 +341,7 @@ use super::*;
     }
     #[test]
     fn test_remplir_plateau_take_it_easy_count_last_column_3_plateau_3_1() {
-        let mut deck = create_shuffle_deck();
+        let mut deck = create_deck();
         let mut plateau = create_plateau_empty();
         placer_tile(&mut plateau, deck.tiles[0].clone(), 16);
         placer_tile(&mut plateau, deck.tiles[1].clone(),17);
@@ -373,7 +351,7 @@ use super::*;
     }
     #[test]
     fn test_remplir_plateau_take_it_easy_count_first_diag_plateau_0_3_7() {
-        let mut deck = create_shuffle_deck();
+        let mut deck = create_deck();
         let mut plateau = create_plateau_empty();
         placer_tile(&mut plateau, deck.tiles[0].clone(), 0);
         placer_tile(&mut plateau, deck.tiles[4].clone(),3);
@@ -383,7 +361,7 @@ use super::*;
     }
     #[test]
     fn test_remplir_plateau_take_it_easy_count_second_diag_plateau_1_4_8_12() {
-        let mut deck = create_shuffle_deck();
+        let mut deck = create_deck();
         let mut plateau = create_plateau_empty();
         placer_tile(&mut plateau, deck.tiles[0].clone(), 1);
         placer_tile(&mut plateau, deck.tiles[4].clone(),4);
@@ -394,7 +372,7 @@ use super::*;
     }
     #[test]
     fn test_remplir_plateau_take_it_easy_count_third_diag_plateau_2_5_9_13_16() {
-        let mut deck = create_shuffle_deck();
+        let mut deck = create_deck();
         let mut plateau = create_plateau_empty();
         placer_tile(&mut plateau, deck.tiles[0].clone(), 2);
         placer_tile(&mut plateau, deck.tiles[4].clone(),5);
@@ -406,7 +384,7 @@ use super::*;
     }
     #[test]
     fn test_remplir_plateau_take_it_easy_count_fourth_diag_plateau_6_10_14_17() {
-        let mut deck = create_shuffle_deck();
+        let mut deck = create_deck();
         let mut plateau = create_plateau_empty();
         placer_tile(&mut plateau, deck.tiles[0].clone(), 6);
         placer_tile(&mut plateau, deck.tiles[4].clone(),10);
@@ -418,7 +396,7 @@ use super::*;
     }
     #[test]
     fn test_remplir_plateau_take_it_easy_count_last_diag_plateau_11_15_18() {
-        let mut deck = create_shuffle_deck();
+        let mut deck = create_deck();
         let mut plateau = create_plateau_empty();
         placer_tile(&mut plateau, deck.tiles[0].clone(), 11);
         placer_tile(&mut plateau, deck.tiles[4].clone(),15);
@@ -430,7 +408,7 @@ use super::*;
     }
     #[test]
     fn test_remplir_plateau_take_it_easy_count_firdt_diag_left_plateau_7_12_16() {
-        let mut deck = create_shuffle_deck();
+        let mut deck = create_deck();
         let mut plateau = create_plateau_empty();
         placer_tile(&mut plateau, deck.tiles[0].clone(), 7);
         placer_tile(&mut plateau, deck.tiles[2].clone(),12);
@@ -442,7 +420,7 @@ use super::*;
     }
     #[test]
     fn test_remplir_plateau_take_it_easy_count_second_diag_left_plateau_3_8_13_17() {
-        let mut deck = create_shuffle_deck();
+        let mut deck = create_deck();
         let mut plateau = create_plateau_empty();
         placer_tile(&mut plateau, deck.tiles[0].clone(), 3);
         placer_tile(&mut plateau, deck.tiles[2].clone(),8);
@@ -455,7 +433,7 @@ use super::*;
     }
     #[test]
     fn test_remplir_plateau_take_it_easy_count_third_diag_left_plateau_0_4_9_14_18() {
-        let mut deck = create_shuffle_deck();
+        let mut deck = create_deck();
         let mut plateau = create_plateau_empty();
         placer_tile(&mut plateau, deck.tiles[0].clone(), 0);
         placer_tile(&mut plateau, deck.tiles[2].clone(),4);
@@ -470,7 +448,7 @@ use super::*;
 
     #[test]
     fn test_remplir_plateau_take_it_easy_count_fourth_diag_left_plateau_1_5_10_15() {
-        let mut deck = create_shuffle_deck();
+        let mut deck = create_deck();
         let mut plateau = create_plateau_empty();
         placer_tile(&mut plateau, deck.tiles[0].clone(), 1);
         placer_tile(&mut plateau, deck.tiles[2].clone(),5);
@@ -483,7 +461,7 @@ use super::*;
     }
     #[test]
     fn test_remplir_plateau_take_it_easy_count_last_diag_left_plateau_2_6_11() {
-        let mut deck = create_shuffle_deck();
+        let mut deck = create_deck();
         let mut plateau = create_plateau_empty();
         placer_tile(&mut plateau, deck.tiles[0].clone(), 2);
         placer_tile(&mut plateau, deck.tiles[2].clone(),6);
diff --git a/src/training/evaluator.rs b/src/training/evaluator.rs
index e69de29..052208b 100644
--- a/src/training/evaluator.rs
+++ b/src/training/evaluator.rs
@@ -0,0 +1,45 @@
+use rand::{rng, Rng};
+use crate::game::create_deck::create_deck;
+use crate::game::plateau::create_plateau_empty;
+use crate::game::plateau_is_full::is_plateau_full;
+use crate::game::remove_tile_from_deck::replace_tile_in_deck;
+use crate::mcts::algorithm::mcts_find_best_position_for_tile_with_nn;
+use crate::neural::policy_value_net::{PolicyNet, ValueNet};
+use crate::scoring::scoring::result;
+
+pub async fn evaluate_model(policy_net: &PolicyNet, value_net: &ValueNet, num_simulations: usize) {
+    log::info!("Evaluating model...");
+    let mut scores = Vec::new();
+
+    for _ in 0..10 {
+        let mut deck = create_deck();
+        let mut plateau = create_plateau_empty();
+        let total_turns = 19; // The number of moves in the game
+        let mut current_turn = 0;
+        while !is_plateau_full(&plateau) {
+            let tile_index = rng().random_range(0..deck.tiles.len());
+            let chosen_tile = deck.tiles[tile_index];
+            let game_result = mcts_find_best_position_for_tile_with_nn(
+                &mut plateau,
+                &mut deck,
+                chosen_tile,
+                policy_net,
+                value_net,
+                num_simulations,
+                current_turn,
+                total_turns,
+            );
+            let best_position = game_result.best_position;
+            plateau.tiles[best_position] = chosen_tile;
+            deck = replace_tile_in_deck(&deck, &chosen_tile);
+            current_turn += 1; // Increment turn counter each time a tile is placed
+        }
+
+        let game_score = result(&plateau);
+        scores.push(game_score);
+    }
+
+    let avg_score: f64 = scores.iter().copied().sum::<i32>() as f64 / scores.len() as f64;
+    log::info!("Model Evaluation Complete. Avg Score: {:.2}", avg_score);
+    // **Stop ping task**
+}
\ No newline at end of file
diff --git a/src/training/mod.rs b/src/training/mod.rs
index e69de29..0fe0e0c 100644
--- a/src/training/mod.rs
+++ b/src/training/mod.rs
@@ -0,0 +1,3 @@
+pub mod websocket;
+pub mod evaluator;
+pub mod session;
diff --git a/src/training/session.rs b/src/training/session.rs
index e69de29..7b0fbaf 100644
--- a/src/training/session.rs
+++ b/src/training/session.rs
@@ -0,0 +1,276 @@
+use crate::data::append_result::append_to_results_file;
+use crate::data::load_data::load_game_data;
+use crate::data::save_data::save_game_data;
+use crate::game::create_deck::create_deck;
+use crate::game::plateau::create_plateau_empty;
+use crate::game::plateau_is_full::is_plateau_full;
+use crate::game::remove_tile_from_deck::replace_tile_in_deck;
+use crate::game::tile::Tile;
+use crate::mcts::algorithm::mcts_find_best_position_for_tile_with_nn;
+use crate::mcts::mcts_result::MCTSResult;
+use crate::mcts_vs_human::play_mcts_vs_human;
+use crate::neural::policy_value_net::{PolicyNet, ValueNet};
+use crate::neural::training::trainer::train_network_with_game_data;
+use crate::scoring::scoring::result;
+use crate::training::evaluator::evaluate_model;
+use crate::training::websocket::reconnect_websocket;
+use crate::utils::image::generate_tile_image_names;
+use crate::Config;
+use futures_util::{SinkExt, StreamExt};
+use rand::{rng, Rng};
+use std::collections::HashMap;
+use std::sync::Arc;
+use tch::nn;
+use tch::nn::Optimizer;
+use tokio::net::TcpListener;
+use tokio_tungstenite::accept_async;
+use tokio_tungstenite::tungstenite::Message;
+/// Lance une session MCTS vs Humain
+
+pub async fn train_and_evaluate(
+    vs_policy: &nn::VarStore,
+    vs_value: &nn::VarStore,
+    policy_net: &mut PolicyNet,
+    value_net: &mut ValueNet,
+    optimizer_policy: &mut Optimizer,
+    optimizer_value: &mut Optimizer,
+    num_games: usize,
+    num_simulations: usize,
+    evaluation_interval: usize,
+    listener: Arc<TcpListener>,
+) {
+    let mut total_score = 0;
+    let mut games_played = 0;
+    let results_file = "results.csv";
+
+    while let Ok((stream, _)) = listener.accept().await {
+        let ws_stream = accept_async(stream)
+            .await
+            .expect("Failed to accept WebSocket");
+        let (mut write, _) = ws_stream.split();
+        let mut scores_by_position: HashMap<usize, Vec<i32>> = HashMap::new();
+        let mut scores = Vec::new(); // Stocke les scores
+        let evaluation_interval_average = 10;
+
+        while games_played < num_games {
+            log::info!(
+                "Starting training iteration {}/{}...",
+                games_played + 1,
+                num_games
+            );
+            log::info!(
+                "\nüöÄ Starting Batch {}",
+                games_played / evaluation_interval + 1
+            );
+
+            let mut batch_games_played = 0; // Tracks games processed in this evaluation interval
+
+            let max_memory_size = 1000; // Store last 500 games
+
+            for game in 0..evaluation_interval {
+                let mut deck = create_deck();
+                let mut plateau = create_plateau_empty();
+                let mut game_data = Vec::new();
+                let mut first_move: Option<(usize, Tile)> = None;
+                let total_turns = 19; // The number of moves in the game
+                let mut current_turn = 0;
+                while !is_plateau_full(&plateau) {
+                    let tile_index = rng().random_range(0..deck.tiles.len());
+                    let chosen_tile = deck.tiles[tile_index];
+                    // ‚úÖ **Send preview before placement**
+                    // ‚úÖ **INSERT YOUR NEW CODE HERE**
+                    let chosen_tile_image = format!(
+                        "../image/{}{}{}.png",
+                        chosen_tile.0, chosen_tile.1, chosen_tile.2
+                    );
+                    let payload = serde_json::json!({
+                        "next_tile": chosen_tile_image,
+                        "plateau_tiles": generate_tile_image_names(&plateau.tiles)
+                    });
+                    let serialized = serde_json::to_string(&payload).unwrap();
+                    write.send(Message::Text(serialized)).await.unwrap();
+
+                    let game_result = mcts_find_best_position_for_tile_with_nn(
+                        &mut plateau,
+                        &mut deck,
+                        chosen_tile,
+                        policy_net,
+                        value_net,
+                        num_simulations,
+                        current_turn,
+                        total_turns,
+                    );
+
+                    let best_position = game_result.best_position;
+                    if first_move.is_none() {
+                        first_move = Some((best_position, chosen_tile));
+                    }
+                    plateau.tiles[best_position] = chosen_tile;
+                    deck = replace_tile_in_deck(&deck, &chosen_tile);
+                    // ‚úÖ INSERT THIS TO SEND SCORE TO CLIENT
+                    let current_score = result(&plateau);
+                    let score_payload = serde_json::json!({
+                        "type": "score_update",
+                        "current_score": current_score,
+                    });
+                    let serialized_score = serde_json::to_string(&score_payload).unwrap();
+                    if let Err(e) = write.send(Message::Text(serialized_score)).await {
+                        log::error!("WebSocket error when sending score: {:?}", e);
+                        if let Some(new_write) = reconnect_websocket(&listener).await {
+                            write = new_write;
+                        } else {
+                            log::error!("Failed to reconnect WebSocket. Exiting...");
+                            break;
+                        }
+                    }
+
+                    game_data.push(game_result); // Store training data
+
+                    // ‚úÖ **INSERT YOUR NEW CODE HERE**
+                    let payload_after_placement = serde_json::json!({
+                        "next_tile": null, // Clear preview
+                        "plateau_tiles": generate_tile_image_names(&plateau.tiles) // new updated state
+                    });
+                    let serialized = serde_json::to_string(&payload_after_placement).unwrap();
+
+                    // ‚úÖ Handle WebSocket disconnections
+                    if let Err(e) = write.send(Message::Text(serialized.clone())).await {
+                        log::error!("WebSocket error: {:?}. Attempting to reconnect...", e);
+
+                        // **Reconnect WebSocket**
+                        if let Some(new_write) = reconnect_websocket(&listener).await {
+                            write = new_write;
+                        } else {
+                            log::error!("Failed to reconnect WebSocket. Exiting...");
+                            break;
+                        }
+                    }
+                    current_turn += 1; // Increment turn counter each time a tile is placed
+                }
+
+                let final_score = result(&plateau);
+
+                if let Some((position, _)) = first_move {
+                    scores_by_position
+                        .entry(position)
+                        .or_insert_with(Vec::new)
+                        .push(final_score);
+                }
+
+                let mut batch_game_data = Vec::new();
+
+                // Prioritized historical data
+                let prioritized_data: Vec<MCTSResult> = load_game_data("game_data")
+                    .into_iter()
+                    .filter(|r| r.subscore > 100.0) // Only select high-score games
+                    .take(50) // Limit to 50 samples to prevent overfitting
+                    .collect();
+
+                // Add historical data to batch
+                batch_game_data.extend(prioritized_data);
+
+                // Add current game's data to batch
+                batch_game_data.extend(game_data.iter().map(|result| MCTSResult {
+                    best_position: result.best_position,
+                    board_tensor: result.board_tensor.shallow_clone(),
+                    subscore: result.subscore,
+                }));
+
+                // Keep only last max_memory_size experiences
+                if batch_game_data.len() > max_memory_size {
+                    let to_remove = batch_game_data.len() - max_memory_size;
+                    batch_game_data.drain(0..to_remove); // Remove oldest data
+                }
+
+                // Train in batches
+                let batch_size = 10;
+                for batch in batch_game_data.chunks(batch_size) {
+                    train_network_with_game_data(
+                        &vs_policy,
+                        &vs_value,
+                        batch, // Use each batch directly
+                        final_score.into(),
+                        policy_net,
+                        value_net,
+                        optimizer_policy,
+                        optimizer_value,
+                    );
+                }
+
+                log::info!("Game {} finished with score: {}", game + 1, final_score);
+                scores.push(final_score);
+
+                // Update batch-specific counters
+                batch_games_played += 1;
+                total_score += final_score;
+
+                if game % evaluation_interval_average == 0 && game != 0 {
+                    let moyenne: f64 = scores.iter().sum::<i32>() as f64 / scores.len() as f64;
+                    log::info!(
+                        "üìä [Batch {}] Avg Score: {:.2} | Games Played: {}",
+                        games_played / evaluation_interval,
+                        moyenne,
+                        games_played
+                    );
+                    log::info!("batch {} - Score moyen: {:.2}", game, moyenne);
+                    write
+                        .send(Message::Text(format!("GAME_RESULT:{}", moyenne)))
+                        .await
+                        .unwrap();
+                }
+
+                // Save current game data for future training
+                save_game_data("game_data", game_data);
+            }
+
+            // Update main game counters
+            games_played += batch_games_played;
+
+            // Append results to the file
+            let avg_score = total_score as f64 / games_played as f64;
+            append_to_results_file(results_file, avg_score);
+
+            // Calculate and display averages
+            let mut averages: Vec<(usize, f64)> = scores_by_position
+                .iter()
+                .map(|(position, scores)| {
+                    let average_score: f64 =
+                        scores.iter().sum::<i32>() as f64 / scores.len() as f64;
+                    (*position, average_score)
+                })
+                .collect();
+
+            averages.sort_by(|a, b| b.1.partial_cmp(&a.1).unwrap_or(std::cmp::Ordering::Equal));
+
+            log::info!("\n--- Average Scores by First Position (Sorted) ---");
+            for (position, average_score) in averages {
+                log::info!(
+                    "Position: {}, Average Score: {:.2}",
+                    position,
+                    average_score
+                );
+            }
+
+            // Evaluate model after each interval
+            evaluate_model(policy_net, value_net, num_simulations).await;
+
+            log::info!(
+                "Games Played: {}, Total Score: {}, Avg Score: {:.2}",
+                games_played,
+                total_score,
+                total_score as f32 / games_played as f32
+            );
+            let model_path = "model_weights";
+            // Save model weights
+            log::info!("Saving models to {}", model_path);
+            log::info!("Saving model weights...");
+            if let Err(e) = policy_net.save_model(vs_policy, "model_weights/policy/policy.params") {
+                log::error!("Error saving PolicyNet weights: {:?}", e);
+            }
+            if let Err(e) = value_net.save_model(vs_value, "model_weights/value/value.params") {
+                log::error!("Error saving ValueNet weights: {:?}", e);
+            }
+        }
+        break; // Exit after handling one connection
+    }
+}
diff --git a/src/training/websocket.rs b/src/training/websocket.rs
index e69de29..3ca151d 100644
--- a/src/training/websocket.rs
+++ b/src/training/websocket.rs
@@ -0,0 +1,42 @@
+use futures_util::stream::SplitSink;
+use futures_util::{SinkExt, StreamExt};
+use tokio::net::TcpListener;
+use tokio_tungstenite::tungstenite::Message;
+use tokio_tungstenite::{accept_async, WebSocketStream};
+
+pub async fn reconnect_websocket(
+    listener: &TcpListener,
+) -> Option<SplitSink<WebSocketStream<tokio::net::TcpStream>, Message>> {
+    match listener.accept().await {
+        Ok((stream, _)) => {
+            log::info!("Re-establishing WebSocket connection...");
+            let ws_stream = accept_async(stream)
+                .await
+                .expect("Failed to accept WebSocket");
+            let (write, _) = ws_stream.split();
+            Some(write)
+        }
+        Err(e) => {
+            log::error!("Error while reconnecting WebSocket: {:?}", e);
+            None
+        }
+    }
+}
+/// Envoie un message via WebSocket avec gestion d'erreur
+pub async fn send_websocket_message(
+    write: &mut SplitSink<WebSocketStream<tokio::net::TcpStream>, Message>,
+    message: String,
+    listener: &TcpListener,
+) -> Result<(), Box<dyn std::error::Error>> {
+    if let Err(e) = write.send(Message::Text(message.clone())).await {
+        log::error!("WebSocket error: {:?}. Attempting to reconnect...", e);
+
+        if let Some(new_write) = reconnect_websocket(listener).await {
+            *write = new_write;
+            write.send(Message::Text(message)).await?;
+        } else {
+            return Err("Failed to reconnect WebSocket".into());
+        }
+    }
+    Ok(())
+}
\ No newline at end of file
diff --git a/src/utils/image.rs b/src/utils/image.rs
index e69de29..8e5c04a 100644
--- a/src/utils/image.rs
+++ b/src/utils/image.rs
@@ -0,0 +1,8 @@
+use crate::game::tile::Tile;
+
+pub fn generate_tile_image_names(tiles: &[Tile]) -> Vec<String> {
+    tiles
+        .iter()
+        .map(|tile| format!("../image/{}{}{}.png", tile.0, tile.1, tile.2))
+        .collect()
+}
\ No newline at end of file
diff --git a/src/utils/mod.rs b/src/utils/mod.rs
index e69de29..8a824bf 100644
--- a/src/utils/mod.rs
+++ b/src/utils/mod.rs
@@ -0,0 +1,2 @@
+pub mod random_index;
+pub mod image;
\ No newline at end of file
diff --git a/src/utils/random_index.rs b/src/utils/random_index.rs
index e69de29..fe25ce6 100644
--- a/src/utils/random_index.rs
+++ b/src/utils/random_index.rs
@@ -0,0 +1,5 @@
+pub fn random_index(max: usize) -> usize {
+    use rand::Rng;
+    let mut rng = rand::thread_rng();
+    rng.gen_range(0..max)
+}
\ No newline at end of file
